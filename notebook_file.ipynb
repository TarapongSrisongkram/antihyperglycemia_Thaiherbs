 1/1: print("Hello world")
 1/2: import os
 1/3:
import os
import tarfile
import urllib
 1/4:
import os
import tarfile
import urllib
 1/5:
import os
import tarfile
import urllib
 1/6:
import os
import tarfile
import urllib
 1/7:
import os
import tarfile
import urllib
 1/8:
import os
import tarfile
import urllib
 1/9: import panda as pd
1/10: import pandas as pd
1/11: import pandas as pd
1/12: import pandas as pd
1/13: import pandas as pd
1/14: import pandas as pd
1/15: import pandas as pd
1/16: import pandas as pd
1/17:
import os
import tarfile
import urllib
1/18:
import os
import tarfile
import urllib
1/19:
import os
import tarfile
import urllib
1/20: import pandas as pd
1/21: import pandas as pd
1/22: import pandas as pd
1/23:
import os
import tarfile
import urllib

Download_root = "http://raw.githubusercintent.com/ageron/handson-ml2/master"
1/24:
import os
import tarfile
from six.moves import urllib
DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml/master"
HOUSING_PATH = "datasets/housing"
HOUSING_URL = DOWNLOAD_ROOT + HOUSING_PATH + "/housing.tgz"

def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
    if not os.path.isdir(housing_path):
        os.makedirs(housing_path)
    tgz_path=os.path.join(housing_path,"housing.tgz")
    urllib.request.urlretrieve(housing_url,tgz_path)
    housing_tgz=tarfile.open(tgz_path)
    housing_tgz.extractall(path=housing_path)
    housing_tgz.close()
1/25:
import os
import tarfile
from six.moves import urllib
DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml/master"
HOUSING_PATH = "datasets/housing"
HOUSING_URL = DOWNLOAD_ROOT + HOUSING_PATH + "/housing.tgz"

def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
    if not os.path.isdir(housing_path):
        os.makedirs(housing_path)
    tgz_path=os.path.join(housing_path,"housing.tgz")
    urllib.request.urlretrieve(housing_url,tgz_path)
    housing_tgz=tarfile.open(tgz_path)
    housing_tgz.extractall(path=housing_path)
    housing_tgz.close()
fetch_housing_data()
1/26:
import os
import tarfile
from six.moves import urllib
DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml/master/"
HOUSING_PATH = "datasets/housing"
HOUSING_URL = DOWNLOAD_ROOT + HOUSING_PATH + "/housing.tgz"

def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
    if not os.path.isdir(housing_path):
        os.makedirs(housing_path)
    tgz_path=os.path.join(housing_path,"housing.tgz")
    urllib.request.urlretrieve(housing_url,tgz_path)
    housing_tgz=tarfile.open(tgz_path)
    housing_tgz.extractall(path=housing_path)
    housing_tgz.close()
fetch_housing_data()
1/27:
import os
import tarfile
import urllib
DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml2/master/"
HOUSING_PATH = os.path.join("datasets","housing")
HOUSING_URL = DOWNLOAD_ROOT + "datasets/housing/housing.tgz"

def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
    os.makedirs(housing_path, exist_ok=True)
    tgz_path = os.path.join(housing_path, "housing.tgz")
    urllib.request.urlretrieve(housing_url, tgz_path)
    housing_tgz = tarfile.open(tgz_path)
    housing_tgz.extractall(path=housing_path)
    housing_tgz.close()
fetch_housing_data()
1/28:
import pandas as pd
def load_housing_data(housing_path=HOUSING_PATH):
    cvs_path = os.path.join(housing_path, "housing.csv")
    return pd.read_csv(csv_path)
1/29:
housing = load_housing_data()
housing.head()
 2/1:
import pandas as pd
def load_housing_data(housing_path=HOUSING_PATH):
    cvs_path = os.path.join(housing_path, "housing.csv")
    return pd.read_csv(csv_path)
 2/2:
import os
import tarfile
import urllib
DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml2/master/"
HOUSING_PATH = os.path.join("datasets","housing")
HOUSING_URL = DOWNLOAD_ROOT + "datasets/housing/housing.tgz"

def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
    os.makedirs(housing_path, exist_ok=True)
    tgz_path = os.path.join(housing_path, "housing.tgz")
    urllib.request.urlretrieve(housing_url, tgz_path)
    housing_tgz = tarfile.open(tgz_path)
    housing_tgz.extractall(path=housing_path)
    housing_tgz.close()
fetch_housing_data()
 2/3:
import os
import tarfile
import urllib
DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml2/master/"
HOUSING_PATH = os.path.join("datasets","housing")
HOUSING_URL = DOWNLOAD_ROOT + "datasets/housing/housing.tgz"

def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
    os.makedirs(housing_path, exist_ok=True)
    tgz_path = os.path.join(housing_path, "housing.tgz")
    urllib.request.urlretrieve(housing_url, tgz_path)
    housing_tgz = tarfile.open(tgz_path)
    housing_tgz.extractall(path=housing_path)
    housing_tgz.close()
 2/4:
import os
import tarfile
import urllib
DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml2/master/"
HOUSING_PATH = os.path.join("datasets","housing")
HOUSING_URL = DOWNLOAD_ROOT + "datasets/housing/housing.tgz"

def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
    os.makedirs(housing_path, exist_ok=True)
    tgz_path = os.path.join(housing_path, "housing.tgz")
    urllib.request.urlretrieve(housing_url, tgz_path)
    housing_tgz = tarfile.open(tgz_path)
    housing_tgz.extractall(path=housing_path)
    housing_tgz.close()
 2/5:
import os
import tarfile
import urllib
DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml2/master/"
HOUSING_PATH = os.path.join("datasets","housing")
HOUSING_URL = DOWNLOAD_ROOT + "datasets/housing/housing.tgz"

def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
    os.makedirs(housing_path, exist_ok=True)
    tgz_path = os.path.join(housing_path, "housing.tgz")
    urllib.request.urlretrieve(housing_url, tgz_path)
    housing_tgz = tarfile.open(tgz_path)
    housing_tgz.extractall(path=housing_path)
    housing_tgz.close()
 2/6: fetch_housing_data()
 2/7:
import pandas as pd
def load_housing_data(housing_path=download):
    cvs_path = os.path.join(housing_path, "housing.csv")
    return pd.read_csv(csv_path)
 2/8:
import pandas as pd
def load_housing_data(housing_path=HOUSING_PATH):
    cvs_path = os.path.join(housing_path, "housing.csv")
    return pd.read_csv(csv_path)
 2/9:
import pandas as pd
def load_housing_data(housing_path=HOUSING_PATH):
    cvs_path = os.path.join(housing_path, "housing.csv")
    return pd.read_csv(csv_path)
2/10:
housing = load_housing_data()
housing.head()
2/11:
import pandas as pd
def load_housing_data(housing_path=HOUSING_PATH):
    cvs_path = os.path.join(housing_path, "housing.csv")
    return pd.read_csv(csv_path)
2/12:
housing = load_housing_data()
housing.head()
2/13:
housing = load_housing_data()
housing.head()
2/14:
import pandas as pd
def load_housing_data(housing_path=HOUSING_PATH):
    csv_path = os.path.join(housing_path, "housing.csv")
    return pd.read_csv(csv_path)
2/15:
housing = load_housing_data()
housing.head()
2/16:
import os
import tarfile
import urllib
DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml2/master/"
HOUSING_PATH = os.path.join("datasets","housing")
HOUSING_URL = DOWNLOAD_ROOT + "datasets/housing/housing.tgz"

def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
    os.makedirs(housing_path, exist_ok=True)
    tgz_path = os.path.join(housing_path, "housing.tgz")
    urllib.request.urlretrieve(housing_url, tgz_path)
    housing_tgz = tarfile.open(tgz_path)
    housing_tgz.extractall(path=housing_path)
    housing_tgz.close()
2/17: fetch_housing_data()
2/18:
import pandas as pd
def load_housing_data(housing_path=HOUSING_PATH):
    csv_path = os.path.join(housing_path, "housing.csv")
    return pd.read_csv(csv_path)
2/19:
housing = load_housing_data()
housing.head()
2/20:
# To support both python 2 and python 3
from __future__ import division, print_function, unicode_literals

# Common imports
import numpy as np
import os

# to make this notebook's output stable across runs
np.random.seed(42)

# To plot pretty figures
%matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt
mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

# Where to save the figures
PROJECT_ROOT_DIR = "."
CHAPTER_ID = "end_to_end_project"
IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, "images", CHAPTER_ID)
os.makedirs(IMAGES_PATH, exist_ok=True)

def save_fig(fig_id, tight_layout=True, fig_extension="png", resolution=300):
    path = os.path.join(IMAGES_PATH, fig_id + "." + fig_extension)
    print("Saving figure", fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format=fig_extension, dpi=resolution)
2/21:
import os
import tarfile
import urllib
DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml2/master/"
HOUSING_PATH = os.path.join("datasets","housing")
HOUSING_URL = DOWNLOAD_ROOT + "datasets/housing/housing.tgz"

def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
    os.makedirs(housing_path, exist_ok=True)
    tgz_path = os.path.join(housing_path, "housing.tgz")
    urllib.request.urlretrieve(housing_url, tgz_path)
    housing_tgz = tarfile.open(tgz_path)
    housing_tgz.extractall(path=housing_path)
    housing_tgz.close()
2/22: fetch_housing_data()
2/23:
# Python ≥3.5 is required
import sys
assert sys.version_info >= (3, 5)

# Scikit-Learn ≥0.20 is required
import sklearn
assert sklearn.__version__ >= "0.20"

# Common imports
import numpy as np
import os

# To plot pretty figures
%matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt
mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

# Where to save the figures
PROJECT_ROOT_DIR = "."
CHAPTER_ID = "end_to_end_project"
IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, "images", CHAPTER_ID)
os.makedirs(IMAGES_PATH, exist_ok=True)

def save_fig(fig_id, tight_layout=True, fig_extension="png", resolution=300):
    path = os.path.join(IMAGES_PATH, fig_id + "." + fig_extension)
    print("Saving figure", fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format=fig_extension, dpi=resolution)
2/24:
import os
import tarfile
import urllib.request

DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml2/master/"
HOUSING_PATH = os.path.join("datasets", "housing")
HOUSING_URL = DOWNLOAD_ROOT + "datasets/housing/housing.tgz"

def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
    if not os.path.isdir(housing_path):
        os.makedirs(housing_path)
    tgz_path = os.path.join(housing_path, "housing.tgz")
    urllib.request.urlretrieve(housing_url, tgz_path)
    housing_tgz = tarfile.open(tgz_path)
    housing_tgz.extractall(path=housing_path)
    housing_tgz.close()
2/25: fetch_housing_data()
2/26:
import pandas as pd

def load_housing_data(housing_path=HOUSING_PATH):
    csv_path = os.path.join(housing_path, "housing.csv")
    return pd.read_csv(csv_path)
2/27:
housing = load_housing_data()
housing.head()
2/28:
housing = load_housing_data()
housing.head()
 3/1: import sys
 3/2:
import sys 
assert sys.version_info >= (3, 5)
 3/3:
import sys 
assert sys.version_info >= (3, 5)

import sklearn
assert sklearn.__version__>= "0.20"
 4/1: print("hello world!")
 4/2: print("hello world!")
 4/3:
import os
import tarfile
import urllib

DOWNLOAD_ROOT = “https://raw.githubusercontent.com/ageron/handson-ml2/master/”
HOUSING_PATH = os.path.join("datasets", "housing")
HOUSING_URL = DOWNLOAD_ROOT + "datasets/housing/housing.tagz"

def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
    os.makedirs(housing_path, exist_ok = True)
    tgz_path = os.path.join(housing_path, "housing.tgz")
    urllib.request.urlretrieve(housing_url,tgz_path)
    housing_tgz = tarfile.open(tgz_path)
    housing_tgz.extractcall(path=housing_path)
    housing_tgz.close()
 4/4:
import os
import tarfile
import urllib

DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml2/master/"
HOUSING_PATH = os.path.join("datasets", "housing")
HOUSING_URL = DOWNLOAD_ROOT + "datasets/housing/housing.tagz"

def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
    os.makedirs(housing_path, exist_ok = True)
    tgz_path = os.path.join(housing_path, "housing.tgz")
    urllib.request.urlretrieve(housing_url,tgz_path)
    housing_tgz = tarfile.open(tgz_path)
    housing_tgz.extractcall(path=housing_path)
    housing_tgz.close()
 4/5:
import pandas as pd

def load_housing_data(housing_path=HOUSING_PATH):csv_path = os.path.join(housing_path, "housing.csv")
    return pdf.read_csv(csv_path)
 4/6:
import pandas as pd

def load_housing_data(housing_path=HOUSING_PATH):csv_path = os.path.join(housing_path, "housing.csv")
return pdf.read_csv(csv_path)
 4/7:
import pandas as pd

def load_housing_data(housing_path=HOUSING_PATH):csv_path = os.path.join(housing_path, "housing.csv")
    return pdf.read_csv(csv_path)
 4/8:
import pandas as pd

def load_housing_data(housing_path=HOUSING_PATH):csv_path = os.path.join(housing_path, "housing.csv")
   return pdf.read_csv(csv_path)
 4/9:
import pandas as pd

def load_housing_data(housing_path=HOUSING_PATH):csv_path = os.path.join(housing_path, "housing.csv")
   return pdf.read_csv(csv_path)
4/10:
import pandas as pd

def load_housing_data(housing_path=HOUSING_PATH):csv_path = os.path.join(housing_path, "housing.csv") return pdf.read_csv(csv_path)
4/11:
import pandas as pd

def load_housing_data(housing_path=HOUSING_PATH):csv_path = os.path.join(housing_path, "housing.csv")
    return pdf.read_csv(csv_path)
4/12:
import pandas as pd

def load_housing_data(housing_path=HOUSING_PATH):csv_path = os.path.join(housing_path, "housing.csv")
    return pdf.read_csv(csv_path)
4/13:
import pandas as pd

def load_housing_data(housing_path=HOUSING_PATH):csv_path = os.path.join(housing_path, "housing.csv")
    return pd.read_csv(csv_path)
4/14:
import pandas as pd

def load_housing_data(housing_path=HOUSING_PATH):csv_path = os.path.join(housing_path, "housing.csv")
return pd.read_csv(csv_path)
4/15:
import pandas as pd

def load_housing_data(housing_path=HOUSING_PATH):csv_path = os.path.join(housing_path, "housing.csv")
 return pd.read_csv(csv_path)
4/16:
import pandas as pd

def load_housing_data(housing_path=HOUSING_PATH):csv_path = os.path.join(housing_path, "housing.csv")
  return pd.read_csv(csv_path)
4/17:
import pandas as pd

def load_housing_data(housing_path=HOUSING_PATH):csv_path = os.path.join(housing_path, "housing.csv")
return pd.read_csv(csv_path)
4/18:
import pandas as pd

def load_housing_data(housing_path=HOUSING_PATH):csv_path = os.path.join(housing_path, "housing.csv")
    return pd.read_csv(csv_path)
4/19:
import pandas as pd

def load_housing_data(housing_path=HOUSING_PATH):csv_path = os.path.join(housing_path, 
                                                                         "housing.csv")
    return pd.read_csv(csv_path)
4/20:
import pandas as pd

def load_housing_data(housing_path=HOUSING_PATH):csv_path = os.path.join(housing_path, "housing.csv")
    return pd.read_csv(csv_path)
4/21:
import pandas as pd

def load_housing_data(housing_path=HOUSING_PATH):
    csv_path = os.path.join(housing_path,
                            "housing.csv")
    return pd.read_csv(csv_path)
4/22: head()
4/23:
housing = load_housing_data()
housing.head()
4/24:
import pandas as pd

def load_housing_data(housing_path=HOUSING_PATH):
    csv_path = os.path.join(housing_path,
                            "housing.csv")
    return pd.read_csv(csv_path)
4/25: housing.head()
4/26:
housing = load_housing_data
housing.head()
4/27:
housing = load_housing_data()
housing.head()
4/28:
housing = load_housing_data()
housing.head()
4/29:
import pandas as pd

def load_housing_data(housing_path=HOUSING_PATH):
    csv_path = os.path.join(housing_path, "housing.csv")
    return pd.read_csv(csv_path)
4/30:
import os
import tarfile
import urllib.request

DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml2/master/"
HOUSING_PATH = os.path.join("datasets", "housing")
HOUSING_URL = DOWNLOAD_ROOT + "datasets/housing/housing.tagz"

def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
    os.makedirs(housing_path, exist_ok = True)
    tgz_path = os.path.join(housing_path, "housing.tgz")
    urllib.request.urlretrieve(housing_url,tgz_path)
    housing_tgz = tarfile.open(tgz_path)
    housing_tgz.extractcall(path=housing_path)
    housing_tgz.close()
4/31:
import pandas as pd

def load_housing_data(housing_path=HOUSING_PATH):
    csv_path = os.path.join(housing_path, "housing.csv")
    return pd.read_csv(csv_path)
4/32:
housing = load_housing_data()
housing.head()
4/33:
import os
import tarfile
import urllib.request

DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml2/master/"
HOUSING_PATH = os.path.join("datasets", "housing")
HOUSING_URL = DOWNLOAD_ROOT + "datasets/housing/housing.tgz"

def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
    os.makedirs(housing_path, exist_ok = True)
    tgz_path = os.path.join(housing_path, "housing.tgz")
    urllib.request.urlretrieve(housing_url,tgz_path)
    housing_tgz = tarfile.open(tgz_path)
    housing_tgz.extractcall(path=housing_path)
    housing_tgz.close()
4/34:
import pandas as pd

def load_housing_data(housing_path=HOUSING_PATH):
    csv_path = os.path.join(housing_path, "housing.csv")
    return pd.read_csv(csv_path)
4/35:
housing = load_housing_data()
housing.head()
4/36: fetch_housing_data()
4/37: fetch_housing_data()
4/38:
import pandas as pd

def load_housing_data(housing_path=HOUSING_PATH):
    csv_path = os.path.join(housing_path, "housing.csv")
    return pd.read_csv(csv_path)
4/39:
import os
import tarfile
import urllib

DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml2/master/"
HOUSING_PATH = os.path.join("datasets", "housing")
HOUSING_URL = DOWNLOAD_ROOT + "datasets/housing/housing.tgz"

def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
    os.makedirs(housing_path)
    tgz_path = os.path.join(housing_path, "housing.tgz")
    urllib.request.urlretrieve(housing_url, tgz_path)
    housing_tgz = tarfile.open(tgz_path)
    housing_tgz.extractall(path=housing_path)
    housing_tgz.close()
4/40: fetch_housing_data()
4/41:
import pandas as pd

def load_housing_data(housing_path=HOUSING_PATH):
    csv_path = os.path.join(housing_path, "housing.csv")
    return pd.read_csv(csv_path)
4/42:
housing = load_housing_data()
housing.head()
4/43:
import os
import tarfile
import urllib

DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml2/master/"
HOUSING_PATH = os.path.join("datasets", "housing")
HOUSING_URL = DOWNLOAD_ROOT + "datasets/housing/housing.tgz"

def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
    os.makedirs(housing_path, exist_ok=True)
    tgz_path = os.path.join(housing_path, "housing.tgz")
    urllib.request.urlretrieve(housing_url, tgz_path)
    housing_tgz = tarfile.open(tgz_path)
    housing_tgz.extractall(path=housing_path)
    housing_tgz.close()
4/44: fetch_housing_data()
4/45:
import os
import tarfile
import urllib.request

DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml2/master/"
HOUSING_PATH = os.path.join("datasets", "housing")
HOUSING_URL = DOWNLOAD_ROOT + "datasets/housing/housing.tgz"

def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
    os.makedirs(housing_path, exist_ok=True)
    tgz_path = os.path.join(housing_path, "housing.tgz")
    urllib.request.urlretrieve(housing_url, tgz_path)
    housing_tgz = tarfile.open(tgz_path)
    housing_tgz.extractall(path=housing_path)
    housing_tgz.close()
4/46: fetch_housing_data()
4/47:
import pandas as pd

def load_housing_data(housing_path=HOUSING_PATH):
    csv_path = os.path.join(housing_path, "housing.csv")
    return pd.read_csv(csv_path)
4/48:
import os
import tarfile
import urllib.request

DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml2/master/"
HOUSING_PATH = os.path.join("datasets", "housing")
HOUSING_URL = DOWNLOAD_ROOT + "datasets/housing/housing.tgz"

def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
    if not os.path.isdir(housing_path):
        os.makedirs(housing_path)
    tgz_path = os.path.join(housing_path, "housing.tgz")
    urllib.request.urlretrieve(housing_url, tgz_path)
    housing_tgz = tarfile.open(tgz_path)
    housing_tgz.extractall(path=housing_path)
    housing_tgz.close()
4/49: fetch_housing_data()
4/50:


fetch_housing_data()
4/51: fetch_housing_data()
4/52: print("hello world!")
4/53:
import pandas as pd

def load_housing_data(housing_path=HOUSING_PATH):
    csv_path = os.path.join(housing_path, "housing.csv")
    return pd.read_csv(csv_path)
4/54:
housing = load_housing_data()
housing.head()
4/55:
housing = load_housing_data()
housing.head()
4/56:
housing = load_housing_data()
housing.head()
4/57: housing.info
4/58: housing.info()
4/59: housing["ocean_proximity"].value_counts()
4/60: housing.describe()
4/61: %matplotlib inline
4/62:
%matplotlib inline
import matplotlib.pyplot as plt
housing.hist(bins=50, figsize=(20,15))
plt.show()
 5/1:
%matplotlib inline
import matplotlib.pyplot as plt
housing.hist(bins=100, figsize=(20,15))
plt.show()
 5/2:
%matplotlib inline
import matplotlib.pyplot as plt
housing.hist(bins=50, figsize=(20,15))
plt.show()
 5/3: print("hello world!")
 5/4:
import os
import tarfile
import urllib.request

DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml2/master/"
HOUSING_PATH = os.path.join("datasets", "housing")
HOUSING_URL = DOWNLOAD_ROOT + "datasets/housing/housing.tgz"

def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
    if not os.path.isdir(housing_path):
        os.makedirs(housing_path)
    tgz_path = os.path.join(housing_path, "housing.tgz")
    urllib.request.urlretrieve(housing_url, tgz_path)
    housing_tgz = tarfile.open(tgz_path)
    housing_tgz.extractall(path=housing_path)
    housing_tgz.close()
 5/5: fetch_housing_data()
 5/6:
import pandas as pd

def load_housing_data(housing_path=HOUSING_PATH):
    csv_path = os.path.join(housing_path, "housing.csv")
    return pd.read_csv(csv_path)
 5/7:
housing = load_housing_data()
housing.head()
 5/8: housing.info()
 5/9: housing["ocean_proximity"].value_counts()
5/10: housing.describe()
5/11:
%matplotlib inline
import matplotlib.pyplot as plt
housing.hist(bins=50, figsize=(20,15))
plt.show()
5/12:
%matplotlib inline
import matplotlib.pyplot as plt
housing.hist(bins=100, figsize=(20,15))
plt.show()
5/13:
import numpy as np
def split_train_test(data, test_ratio):
    shuffled_indices = np.random.permutation(len(data))
    test_set_size = int(len(data) * test_ratio)
    test_indices = shuffled_indices[:test_set_size]
    train_indices = shuffled_indices[test_set_size:]
    return data.iloc[train_indices], data.iloc[test_indices]
5/14:
train_set, test_set = split_train_test(housing, 0.2)
len(train_set)
5/15: len(test_set)
5/16:
from zlib import crc32

def test_set_check(identifier, test_ratio):
    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32

def split_train_test_by_id(data, test_ratio, id_column):
    ids = data[id_column]
    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))
    return data.loc[~in_test_set], data.loc[in_test_set]
5/17:
import hashlib

def test_set_check(identifier, test_ratio, hash=hashlib.md5):
    return hash(np.int64(identifier)).digest()[-1] < 256 * test_ratio
5/18:
def test_set_check(identifier, test_ratio, hash=hashlib.md5):
    return bytearray(hash(np.int64(identifier)).digest())[-1] < 256 * test_ratio
5/19:
housing_with_id = housing.reset_index()   # adds an `index` column
train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, "index")
5/20:
housing_with_id["id"] = housing["longitude"] * 1000 + housing["latitude"]
train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, "id")
5/21: test_set.head()
5/22: print(housing_with_id["id"])
5/23:
from sklearn.model_selection import train_test_split
train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)
5/24:
from sklearn.model_selection import train_test_split
train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)
5/25: test_set.head()
5/26: housing["median_income"].hist()
5/27: housing["total_bedrooms"].hist()
5/28: from rdkit import Chem
5/29: pip install squarify
5/30: import squarify
5/31:
size = [40, 30, 5, 25]
squarify.plot(sizes)
plt.show()
5/32:
sizes = [40, 30, 5, 25]
squarify.plot(sizes)
plt.show()
5/33: cellculture = pd.read_csv('cellculture_isi.csv', encoding = "utf-8")
5/34: cellculture = pd.read_csv('cellculture_isi', encoding = "utf-8")
5/35: cellculture = pd.read_csv('/Users/tarapongsrisongkram/OneDrive - Khon Kaen University/Biotech/cellculture_isi.csv')
5/36: print(cellculture)
5/37: sizes = cellculture.records
5/38:
sizes = cellculture.records
label = cellculture.Web of Science Categories
5/39:
sizes = cellculture.records
label = cellculture.Web-of-Science-Categories
5/40:
sizes = cellculture.records
label = cellculture.WebofScienceCategories
5/41:
sizes = cellculture.records
label = cellculture.Web_of_Science_Categories
5/42: cellculture = pd.read_csv('/Users/tarapongsrisongkram/OneDrive - Khon Kaen University/Biotech/cellculture_isi.csv')
5/43: print(cellculture)
5/44: print(cellculture)
5/45: cellculture = pd.read_csv('/Users/tarapongsrisongkram/OneDrive - Khon Kaen University/Biotech/cellculture_isi.csv')
5/46: print(cellculture)
5/47:
sizes = cellculture.records
label = cellculture.Categories
5/48:
squarify.plot(sizes=sizes, labe=label)
plt.show()
5/49:
squarify.plot(sizes=sizes, labe;=label)
plt.show()
5/50:
squarify.plot(sizes=sizes, label=label)
plt.show()
5/51: print(sizes)
5/52:
data = int(sizes)
print(data)
5/53:
data = int(sizes)
print(sizes)
5/54:
squarify.plot(sizes=sizes, label=label)
plt.axis('off')
plt.show()
5/55:
squarify.plot(sizes=sizes, label=label, alpha=.8))
plt.axis('off')
plt.show()
5/56:
squarify.plot(sizes=sizes, label=label, alpha=.8)
plt.axis('off')
plt.show()
5/57:
squarify.plot(sizes=sizes)
plt.axis('off')
plt.show()
5/58:
squarify.plot(sizes=sizes)
plt.axis('off')
plt.show()
5/59:
squarify.plot(sizes=sizes)
plt.axis('off')
plt.show()
5/60:
squarify.plot(sizes=sizes)
plt.axis('off')
plt.show()
5/61: sizes
5/62:
sizes = cellculture.records(0-24)
label = cellculture.Categories
5/63:
sizes = cellculture.records
label = cellculture.Categories
5/64: cellculture = pd.read_csv('/Users/tarapongsrisongkram/OneDrive - Khon Kaen University/Biotech/cellculture_isi.csv')
5/65: print(cellculture)
5/66:
sizes = cellculture.records
label = cellculture.Categories
5/67: sizes
5/68:
squarify.plot(sizes=sizes)
plt.axis('off')
plt.show()
5/69:
squarify.plot(sizes=sizes label=label)
plt.axis('off')
plt.show()
5/70:
squarify.plot(sizes=sizes, label=label)
plt.axis('off')
plt.show()
5/71:
squarify.plot(sizes=sizes, label=label, alpha=0.6)
plt.axis('off')
plt.show()
5/72:
squarify.plot(sizes=sizes, label=label, alpha=0.6)
plt.axis('off')
SMALL_SIZE = 8
matplotlib.rc('font', size=SMALL_SIZE)
matplotlib.rc('axes', titlesize=SMALL_SIZE)
plt.show()
5/73:
squarify.plot(sizes=sizes, label=label, alpha=0.6)
plt.axis('off')
plt.show()
5/74:
SMALL_SIZE = 8
matplotlib.rc('font', size=SMALL_SIZE)
matplotlib.rc('axes', titlesize=SMALL_SIZE)
5/75:
sizes = cellculture.records.iloc[0:9]
label = cellculture.Categories
5/76: sizes
5/77:
squarify.plot(sizes=sizes, label=label, alpha=0.6)
plt.axis('off')
plt.show()
5/78:
import matplotlib.pyplot as plt
SMALL_SIZE = 8
matplotlib.rc('font', size=SMALL_SIZE)
matplotlib.rc('axes', titlesize=SMALL_SIZE)
5/79:
import matplotlib as plt
SMALL_SIZE = 8
matplotlib.rc('font', size=SMALL_SIZE)
matplotlib.rc('axes', titlesize=SMALL_SIZE)
5/80:
import matplotlib as plt
SMALL_SIZE = 8
matplotlib.rc('font', size=SMALL_SIZE)
matplotlib.rc('axes', titlesize=SMALL_SIZE)
5/81:
squarify.plot(sizes=sizes, label=label, alpha=0.6, text_kwargs={'fontsize':8})
plt.axis('off')
plt.show()
5/82:
squarify.plot(sizes=sizes, label=label, alpha=0.6)
plt.axis('off')
plt.show()
5/83:
squarify.plot(sizes=sizes, label=label, alpha=0.6)
plt.axis('off')
plt.show()
5/84:
sizes = cellculture.records.iloc[0:9]
label = cellculture.Categories
5/85: sizes
5/86:
squarify.plot(sizes=sizes, label=label, alpha=0.6)
plt.axis('off')
plt.show()
5/87:
sizes = cellculture.records
label = cellculture.Categories
5/88:
squarify.plot(sizes=sizes, label=label, alpha=0.6)
plt.axis('off')
plt.show()
5/89:
squarify.plot(sizes=sizes, label=label, alpha=0.6 )
plt.axis('off')
plt.show()
5/90:
squarify.plot(sizes=sizes, label=label, alpha=0.6 )

plt.show()
5/91: squarify.plot(sizes=sizes, label=label, alpha=0.6 )
5/92:
%matplotlib inline
import matplotlib.pyplot as plt
housing.hist(bins=100, figsize=(20,15))
plt.show()
5/93:
%matplotlib inline
import matplotlib.pyplot as plt
housing.hist(bins=100, figsize=(20,15))
plt.show()
5/94:
squarify.plot(sizes=sizes, label=label, alpha=0.6 )
plt.show()
5/95:
squarify.plot(sizes=sizes, label=label, alpha=0.6)
plt.show()
5/96: sizes
5/97:
sizes = cellculture.records
label = cellculture.Categories
5/98: sizes
5/99:
squarify.plot(sizes=sizes, label=label, alpha=0.6)
plt.show()
5/100:
squarify.plot(sizes=sizes, label=label, alpha=0.6)
plt.axis('off')
plt.show()
5/101:
sizes = cellculture.records.loc[0:9]
label = cellculture.Categories
5/102: sizes
5/103:
squarify.plot(sizes=sizes, label=label, alpha=0.6)
plt.axis('off')
plt.show()
5/104:
squarify.plot(sizes=sizes, alpha=0.6)
plt.axis('off')
plt.show()
5/105:
squarify.plot(sizes=sizes, alpha=0.8)
plt.axis('off')
plt.show()
5/106:
squarify.plot(sizes=sizes, alpha=1)
plt.axis('off')
plt.show()
5/107:
squarify.plot(sizes=sizes, alpha=1)
plt.axis('off')
plt.show()
5/108:
squarify.plot(sizes=sizes, alpha=1)
plt.axis('off')
plt.show()
5/109:
squarify.plot(sizes=sizes, alpha=1)
plt.axis('off')
plt.show()
5/110:
squarify.plot(sizes=sizes, alpha=1)
plt.axis('off')
plt.show()
5/111:
squarify.plot(sizes=sizes, alpha=1)
plt.axis('off')
plt.show()
5/112:
squarify.plot(sizes=sizes, alpha=1)
plt.axis('off')
plt.show()
5/113:
squarify.plot(sizes=sizes, alpha=1)
plt.axis('off')
plt.show()
5/114:
squarify.plot(sizes=sizes, alpha=1)
plt.axis('off')
plt.show()
5/115:
squarify.plot(sizes=sizes, alpha=0.8)
plt.axis('off')
plt.show()
5/116:
squarify.plot(sizes=sizes, alpha=0.8)
plt.axis('off')
plt.show()
5/117:
squarify.plot(sizes=sizes, alpha=0.8)
plt.axis('off')
plt.show()
5/118:
squarify.plot(sizes=sizes, alpha=1)
plt.axis('off')
plt.show()
5/119:
squarify.plot(sizes=sizes, alpha=1)
plt.axis('off')
plt.show()
5/120:
squarify.plot(sizes=sizes, alpha=1)
plt.axis('off')
plt.show()
5/121:
squarify.plot(sizes=sizes, alpha=1)
plt.axis('off')
plt.show()
5/122:
squarify.plot(sizes=sizes, alpha=1)
plt.axis('off')
plt.show()
f.savefig("three.png", bbox_inches='tight', dpi=600)
5/123:
squarify.plot(sizes=sizes, alpha=1)
plt.axis('off')
plt.show()
5/124:
squarify.plot(sizes=sizes, alpha=1)
plt.axis('off')
plt.show()
5/125:
f = plt.figure()
f.savefig("three.png", bbox_inches='tight', dpi=600)
5/126:
f = plt.figure()
squarify.plot(sizes=sizes, alpha=1)
plt.axis('off')
plt.show()
f.savefig("three.png", bbox_inches='tight', dpi=600)
5/127:
f = plt.figure()
squarify.plot(sizes=sizes, alpha=1)
plt.axis('off')
plt.show()
f.savefig("three.png", bbox_inches='tight', dpi=600)
5/128:
f = plt.figure()
squarify.plot(sizes=sizes, alpha=0.8)
plt.axis('off')
plt.show()
f.savefig("three0.8.png", bbox_inches='tight', dpi=600)
5/129:
f = plt.figure()
squarify.plot(sizes=sizes, alpha=0.8)
plt.axis('off')
plt.show()
f.savefig("three0.8.png", bbox_inches='tight', dpi=600)
5/130:
f = plt.figure()
squarify.plot(sizes=sizes, alpha=0.8)
plt.axis('off')
plt.show()
f.savefig("three0.8.png", bbox_inches='tight', dpi=600)
5/131:
f = plt.figure()
squarify.plot(sizes=sizes, alpha=0.8)
plt.axis('off')
plt.show()
f.savefig("three0.8.png", bbox_inches='tight', dpi=600)
5/132:
f = plt.figure()
squarify.plot(sizes=sizes, alpha=0.8)
plt.axis('off')
plt.show()
f.savefig("three0.8.png", bbox_inches='tight', dpi=600)
5/133:
f = plt.figure()
squarify.plot(sizes=sizes, alpha=0.8)
plt.axis('off')
plt.show()
f.savefig("three0.8.png", bbox_inches='tight', dpi=600)
5/134:
f = plt.figure()
squarify.plot(sizes=sizes, alpha=0.8)
plt.axis('off')
plt.show()
f.savefig("three0.8.png", bbox_inches='tight', dpi=600)
5/135:
f = plt.figure()
squarify.plot(sizes=sizes, alpha=0.8)
plt.axis('off')
plt.show()
f.savefig("three0.8.png", bbox_inches='tight', dpi=600)
5/136:
f = plt.figure()
squarify.plot(sizes=sizes, alpha=0.8)
plt.axis('off')
plt.show()
f.savefig("three0.8.png", bbox_inches='tight', dpi=600)
5/137:
f = plt.figure()
squarify.plot(sizes=sizes, alpha=0.8)
plt.axis('off')
plt.show()
f.savefig("three0.8.png", bbox_inches='tight', dpi=600)
5/138:
f = plt.figure()
squarify.plot(sizes=sizes, alpha=0.8)
plt.axis('off')
plt.show()
f.savefig("three0.8.png", bbox_inches='tight', dpi=600)
5/139:
f = plt.figure()
squarify.plot(sizes=sizes, alpha=0.8)
plt.axis('off')
plt.show()
f.savefig("three0.8.png", bbox_inches='tight', dpi=600)
5/140:
f = plt.figure()
squarify.plot(sizes=sizes, alpha=0.8)
plt.axis('off')
plt.show()
f.savefig("three0.8.png", bbox_inches='tight', dpi=600)
5/141:
f = plt.figure()
squarify.plot(sizes=sizes, alpha=0.8)
plt.axis('off')
plt.show()
f.savefig("three0.8.png", bbox_inches='tight', dpi=600)
5/142:
f = plt.figure()
squarify.plot(sizes=sizes, alpha=0.8)
plt.axis('off')
plt.show()
f.savefig("three0.8.png", bbox_inches='tight', dpi=600)
5/143:
f = plt.figure()
squarify.plot(sizes=sizes, alpha=0.8)
plt.axis('off')
plt.show()
f.savefig("three0.8.png", bbox_inches='tight', dpi=600)
5/144:
f = plt.figure()
squarify.plot(sizes=sizes, alpha=0.8)
plt.axis('off')
plt.show()
f.savefig("three0.8.png", bbox_inches='tight', dpi=600)
5/145:
f = plt.figure()
squarify.plot(sizes=sizes, alpha=0.8)
plt.axis('off')
plt.show()
f.savefig("three0.8.png", bbox_inches='tight', dpi=600)
5/146:
f = plt.figure()
squarify.plot(sizes=sizes, alpha=0.8)
plt.axis('off')
plt.show()
f.savefig("three0.8.png", bbox_inches='tight', dpi=600)
5/147:
f = plt.figure()
squarify.plot(sizes=sizes, alpha=0.8)
plt.axis('off')
plt.show()
f.savefig("three0.8.png", bbox_inches='tight', dpi=600)
5/148:
f = plt.figure()
squarify.plot(sizes=sizes, alpha=0.8)
plt.axis('off')
plt.show()
f.savefig("three0.8.png", bbox_inches='tight', dpi=600)
5/149:
f = plt.figure()
squarify.plot(sizes=sizes, alpha=0.8)
plt.axis('off')
plt.show()
f.savefig("three0.8.png", bbox_inches='tight', dpi=600)
5/150:
f = plt.figure()
squarify.plot(sizes=sizes, alpha=0.8)
plt.axis('off')
plt.show()
f.savefig("three0.8.png", bbox_inches='tight', dpi=600)
5/151:
f = plt.figure()
squarify.plot(sizes=sizes, alpha=0.8)
plt.axis('off')
plt.show()
f.savefig("three0.8.png", bbox_inches='tight', dpi=600)
5/152:
f = plt.figure()
squarify.plot(sizes=sizes, alpha=0.8)
plt.axis('off')
plt.show()
f.savefig("three0.8.png", bbox_inches='tight', dpi=600)
5/153:
f = plt.figure()
squarify.plot(sizes=sizes, alpha=0.8)
plt.axis('off')
plt.show()
f.savefig("three0.8.png", bbox_inches='tight', dpi=600)
5/154:
f = plt.figure()
squarify.plot(sizes=sizes, alpha=0.8)
plt.axis('off')
plt.show()
f.savefig("three0.8.png", bbox_inches='tight', dpi=600)
5/155:
f = plt.figure()
squarify.plot(sizes=sizes, alpha=0.8)
plt.axis('off')
plt.show()
f.savefig("three0.8.png", bbox_inches='tight', dpi=600)
5/156:
f = plt.figure()
squarify.plot(sizes=sizes, alpha=0.8)
plt.axis('off')
plt.show()
f.savefig("three0.8.png", bbox_inches='tight', dpi=600)
 8/1: result = c(99, 101, 100, 98,  95, 90)
 8/2: result = (99, 101, 100, 98,  95, 90)
 8/3:
result = (99, 101, 100, 98,  95, 90)
time = (0, 3, 6, 12, 24, 48, 96)
 8/4:
result = (99, 101, 100, 98,  95, 90)
time = (0, 3, 6, 12, 24, 48, 96)
print(result)
 8/5:
result = (99, 101, 100, 98,  95, 90)
time = (0, 3, 6, 12, 24, 48, 96)
test = data.fra,e(time,result)
 8/6:
result = (99, 101, 100, 98,  95, 90)
time = (0, 3, 6, 12, 24, 48, 96)
test = data.frame(time,result)
 8/7:
result = (99, 101, 100, 98,  95, 90)
time = (0, 3, 6, 12, 24, 48, 96)
test = as.dataframe(time,result)
 8/8:
result = (99, 101, 100, 98,  95, 90)
time = (0, 3, 6, 12, 24, 48, 96)
test = dataframe(time,result)
 8/9:
result = (99, 101, 100, 98,  95, 90)
time = (0, 3, 6, 12, 24, 48, 96)
test = matrix(time,result)
8/10:
result = (99, 101, 100, 98,  95, 90)
time = (0, 3, 6, 12, 24, 48, 96)
plot(result, time)
8/11:
result = (99, 101, 100, 98,  95, 90)
time = (0, 3, 6, 12, 24, 48, 96)
plt.plot(result, time)
8/12:
%matplotlib inline
import matplotlib.pyplot as plt
housing.hist(bins=100, figsize=(20,15))
plt.show()
8/13: test_set.head()
8/14:
from sklearn.model_selection import train_test_split
train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)
8/15:
result = (99, 101, 100, 98,  95, 90)
time = (0, 3, 6, 12, 24, 48, 96)
plt.plot(result, time)
8/16:
result = (99, 101, 100, 98,  95, 90)
time = (0, 3, 6, 12, 24, 48)
plt.plot(result, time)
8/17:
result = (99, 101, 100, 98,  95, 90)
time = (0, 3, 6, 12, 24, 48)
plt.plot(result, time)
8/18:
result = (99, 101, 100, 98,  95, 90)
time = (0, 3, 6, 12, 24, 48)
plt.plot(time, result)
10/1: ### THis is a new notebook for LAT1 analysis ###
10/2:
# Connect to database 
pip install chembl_webresource_client
10/3:
# Connect to database 
pip install chembl_webresource_client
10/4:
# Connect to Chembl database 
pip install chembl_webresource_client
10/5:
# Connect to Chembl database 
pipinstall chembl_webresource_client
10/6:
# Connect to Chembl database 
pipinstall chembl_webresource_client
13/1:
# Connect to Chembl database 
pip install chembl_webresource_client
14/1:
# Start searching Chembl database
from chembl_webresource_clint.new_client import new_client
target = new_client.target
target_query = target.search('LAT1')
14/2: pip install chembl_webresource_clint
14/3: pip install chembl_webresource_clint
14/4:
# Start searching Chembl database
from chembl_webresource_clint.new_client import new_client
target = new_client.target
target_query = target.search('LAT1')
14/5: pip install chembl_webresource_client
14/6:
# Start searching Chembl database
from chembl_webresource_clint.new_client import new_client
target = new_client.target
target_query = target.search('LAT1')
14/7:
# Start searching Chembl database
from chembl_webresource_clint.new_client import new_client
target = new_client.target
target_query = target.search('LAT1')
14/8:
# Start searching Chembl database
from chembl_webresource_client.new_client import new_client
target = new_client.target
target_query = target.search('LAT1')
14/9:
# Start searching Chembl database
from chembl_webresource_client.new_client import new_client
target = new_client.target
target_query = target.search('LAT1')
14/10:
# Start searching Chembl database
import pandas as pd
from chembl_webresource_client.new_client import new_client
target = new_client.target
target_query = target.search('LAT1')
16/1:
# Start searching Chembl database
import pandas as pd
from chembl_webresource_client.new_client import new_client
molecule = new_client.molecule
res = molecule.search('viagra')
16/2: pip install chembl_webresource_client
16/3:
# Start searching Chembl database
import pandas as pd
from chembl_webresource_client.new_client import new_client
molecule = new_client.molecule
res = molecule.search('viagra')
16/4:
# Start searching Chembl database
import pandas as pd
from chembl_webresource_client.new_client import new_client
16/5: pip -update
16/6: pip install --upgrade pip
17/1: ### THis is a new notebook for LAT1 analysis ###
17/2: pip install chembl_webresource_client
17/3:
# Start searching Chembl database
import pandas as pd
from chembl_webresource_client.new_client import new_client
18/1: pip install chembl_webresource_client
18/2:
# Start searching Chembl database
import pandas as pd
from chembl_webresource_client.new_client import new_client
18/3: pip install --upgrade pip
18/4:
#import library
import pandas as pd
18/5: from chembl_webresource_client.new_client import new_cleint
18/6: from chembl_webresource_client.new_client import new_client
18/7: from chembl_webresource_client import new_client
18/8: pip install Flask-Caching
18/9: from chembl_webresource_client import new_client
18/10: from chembl_webresource_client import new_client
18/11: from chembl_webresource_client import new_client
18/12: from chembl_webresource_client import new_client
18/13: x
18/14: 5+3
18/15: x = input(what is your name?)
18/16: x = input('what is your name?')
18/17: x
18/18: print(x)
18/19: import tensorflow as tf
18/20: import matplot as mp
18/21:
usr/bin/ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"
export PATH="/usr/local/opt/python/libexec/bin:$PATH"
# if you are on macOS 10.12 (Sierra) use `export PATH="/usr/local/bin:/usr/local/sbin:$PATH"`
brew update
brew install python  # Python 3
18/22:
usr/bin/ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"
export PATH="/usr/local/opt/python/libexec/bin:$PATH"
# if you are on macOS 10.12 (Sierra) use `export PATH="/usr/local/bin:/usr/local/sbin:$PATH"`
brew update
brew install python  # Python 3
18/23: pip install chembl_webresource_new.client
18/24: pip install chembl_webresource
18/25: pip -version
18/26: pip -update
18/27: pip update
18/28: pip --version
19/1: ### THis is a new notebook for LAT1 analysis ###
19/2:
#import library
import pandas as pd
19/3: 5+3
19/4: x = input('what is your name?')
19/5: x
19/6: print(x)
19/7: pip --version
19/8: pip install chembl_webresource_client
19/9:
from chembl_webresource_client.new_client import new_client
target = new_client.target
gene_name = 'BRD4'
res = target.search(gene_name)
19/10:
from chembl_webresource_client.new_client import new_client
target = new_client.target
gene_name = 'BRD4'
res = target.search(gene_name)
19/11:
import logging
from operator import itemgetter
from IPython.display import display, SVG
19/12:
# Python modules used for API access...
from chembl_webresource_client.new_client import new_client
19/13:
# Python modules used for API access...
from chembl_webresource_client import *
from chembl_webresource_client.new_client import new_client
19/14:
# Python modules used for API access...
from chembl_webresource_client.new_client import new_client
21/1: pip install request_cache-=0.7.0
21/2: pip install request_cache==0.7.0
21/3: pip install request_cache==0.6.4
21/4: pip install requests_cache==0.6.4
21/5: pip install chembl_webresource_client
21/6:
# Python modules used for API access...
from chembl_webresource_client.new_client import new_client
21/7:
#import library
import pandas as pd
21/8:
# Python modules used for API access...
from chembl_webresource_client.new_client import new_client
target = new_client.target
target_query = target.search('LAT1')
targets = pd.DataFrame.from_dict(target_query)
targets
21/9:
#Input yourname for this lab notebook
labname = input('what is your name?')
21/10: print(labname)
21/11:
#As we can see, we will select H.sapiens LAT1 as atarget
selected_target = targets.target_chembl_id[2]
selected_target
21/12:
#Here we will access the activity of compounds in this target, first we will try IC50
activity = new_client.activity
res = activity.filter(target_chembl_id=selected_target).filter(standard_type="IC50")
21/13:
#Here we will access the activity of compounds in this target, first we will try IC50
activity = new_client.activity
result = activity.filter(target_chembl_id=selected_target).filter(standard_type="IC50")
21/14: df = pd.DataFrame.from_dict(result)
21/15: df
21/16:
#Here we will access the activity of compounds in this target, first we will try IC50
activity = new_client.activity
result = activity.filter(target_chembl_id=selected_target).filter(standard_type="IC50")
21/17: df = pd.DataFrame.from_dict(result)
21/18: df
21/19: result
21/20:
#Here we will access the activity of compounds in this target, first we will try IC50
activity = new_client.activity
res = activity.filter(target_chembl_id=selected_target).filter(standard_type="IC50")
21/21: res
21/22:
#As we can see, we will select H.sapiens LAT1 as atarget
selected_target = targets.target_chembl_id[1]
selected_target
21/23:
#Here we will access the activity of compounds in this target, first we will try IC50
activity = new_client.activity
res = activity.filter(target_chembl_id=selected_target).filter(standard_type="IC50")
21/24: res
21/25: df = pd.DataFrame.from_dict(res)
21/26:
df = pd.DataFrame.from_dict(res)
df
21/27:
#if any compound is missing the IC50 value, then it should drop out
df2 = df[df.standard_value.notna()]
21/28:
#if any compound is missing the IC50 value, then it should drop out
df2 = df[df.standard_value.notna()]
df2
21/29: print(df2.columns)
21/30:
#we want to see the molecule structure via canonical_smiles
df2.canonical_smiles
21/31:
#We want to know the structure and activity
selection = ['molecule_chembl_id','canonical_smiles','standard_units', 'standard_value']
21/32:
#We want to know the structure and activity
selection = ['molecule_chembl_id','canonical_smiles','standard_units', 'standard_value']
df3 = df2[selection]
21/33:
#We want to know the structure and activity
selection = ['molecule_chembl_id','canonical_smiles','standard_units', 'standard_value']
df3 = df2[selection]
df3
21/34:
#We want to know how many of them is high potency agaisnt LAT1 protein
#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution
#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()
# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 10−9
# Take the molar value and apply -log10
# Delete the standard_value column and create a new pIC50 column

#before we do that ,we need to install nescessery package

import numpy as np
21/35:
#We want to know how many of them is high potency agaisnt LAT1 protein
#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution
#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()
# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 10−9
# Take the molar value and apply -log10
# Delete the standard_value column and create a new pIC50 column

#before we do that ,we need to install nescessery package

import numpy as np
def pIC50(input):
    pIC50 = []

    for i in input['standard_value_norm']:
        molar = i*(10**-9) # Converts nM to M
        pIC50.append(-np.log10(molar))

    input['pIC50'] = pIC50
    x = input.drop('standard_value_norm', 1)
        
    return x
21/36:
#Test
def norm_value(input):
    norm = []

    for i in input['standard_value']:
        if i > 100000000:
          i = 100000000
        norm.append(i)

    input['standard_value_norm'] = norm
    x = input.drop('standard_value', 1)
        
    return x
21/37: -np.log10(1000)
21/38: standard_value_norm
21/39: df3.standard_value_norm
21/40: df3.standard_value
21/41:
df_norm = norm_value(df_combined)
df_norm
21/42:
df_norm = norm_value(df3)
df_norm
21/43:
#We want to know how many of them is high potency agaisnt LAT1 protein
#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution
#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()
# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 10−9
# Take the molar value and apply -log10
# Delete the standard_value column and create a new pIC50 column

#before we do that ,we need to install nescessery package

import numpy as np
def pIC50(input):
    pIC50 = []

    for i in input['standard_value_norm']:
        molar = i*(10**-9) # Converts nM to M
        pIC50.append(-np.log10(molar))

    input['pIC50'] = pIC50
    x = input.drop('standard_value_norm', 1)
        
    return x
21/44:
#Test
def norm_value(input):
    norm = []

    for i in input['standard_value']:
        if i > 100000000:
          i = 100000000
        norm.append(i)

    input['standard_value_norm'] = norm
    x = input.drop('standard_value', 1)
        
    return x
21/45:
df_norm = norm_value(df3)
df_norm
21/46:
#We want to know how many of them is high potency agaisnt LAT1 protein
#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution
#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()
# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 10−9
# Take the molar value and apply -log10
# Delete the standard_value column and create a new pIC50 column

#before we do that ,we need to install nescessery package

import numpy as np
def pIC50(input):
    pIC50 = []

    for i in input['standard_value']:
        molar = i*(10**-9) # Converts nM to M
        pIC50.append(-np.log10(molar))

    input['pIC50'] = pIC50
    x = input.drop('standard_value', 1)
        
    return x
21/47:
#Test
def norm_value(input):
    norm = []

    for i in input['standard_value']:
        if i > 100000000:
          i = 100000000
        norm.append(i)

    input['standard_value_norm'] = norm
    x = input.drop('standard_value', 1)
        
    return x
21/48:
df_norm = norm_value(df3)
df_norm
21/49:
#Test
molar = d3['standard_value']*(10**-9)
21/50:
#We want to know the structure and activity
selection = ['molecule_chembl_id','canonical_smiles','standard_units', 'standard_value']
df3 = df2[selection]
df3
21/51:
#Test
molar = df3['standard_value']*(10**-9)
21/52:
#Test
'standard_value'
molar = df3['standard_value']*(10**-9)
21/53:
#Test
df3['standard_value']
molar = df3['standard_value']*(10**-9)
21/54:
#Test
df3['standard_value']
21/55:
#Test
df3['standard_value']
molar = int(df3['standard_value'])*(10**-9)
21/56:
#Test
df3['standard_value']
# Calculate logarithm to 
# base 10 on 'Salary' column
data['molar'] = df3['Salary']*10  
# Show the dataframe
data
21/57:
#Test
df3['standard_value']
# Calculate logarithm to 
# base 10 on 'Salary' column
data['molar'] = df3['standard_value']*10  
# Show the dataframe
data
21/58:
#Test
df3['standard_value']
# Calculate logarithm to 
# base 10 on 'Salary' column
data['molar'] = df3['standard_value']*10  
# Show the dataframe
data
21/59:
#Test
df3['standard_value']

df3['molar'] = df3['standard_value']*10  
# Show the dataframe
df3
21/60:
#Test
df3['standard_value']

df3['molar'] = df3['standard_value']*(10**-9)
# Show the dataframe
df3
21/61:
#Test
df3['standard_value']

df3['molar'] = np.df3['standard_value']*(10**-9)
# Show the dataframe
df3
21/62:
#We want to know how many of them is high potency agaisnt LAT1 protein
#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution
#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()
# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 10−9
# Take the molar value and apply -log10
# Delete the standard_value column and create a new pIC50 column

#before we do that ,we need to install nescessery package

import numpy as np

def pIC50(input):
    pIC50 = []

    for i in input['standard_value_norm']:
        molar = i*(10**-9) # Converts nM to M
        pIC50.append(-np.log10(molar))

    input['pIC50'] = pIC50
    x = input.drop('standard_value_norm', 1)
        
    return x
21/63: df3.standard_value.describe()
21/64: df4 = pIC50(df3)
21/65:
#We want to know how many of them is high potency agaisnt LAT1 protein
#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution
#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()
# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 10−9
# Take the molar value and apply -log10
# Delete the standard_value column and create a new pIC50 column

#before we do that ,we need to install nescessery package

import numpy as np

def pIC50(input):
    pIC50 = []

    for i in input['standard_value']:
        molar = i*(10**-9) # Converts nM to M
        pIC50.append(-np.log10(molar))

    input['pIC50'] = pIC50
    x = input.drop('standard_value', 1)
        
    return x
21/66: df3.standard_value.describe()
21/67: df4 = pIC50(df3)
21/68:
#We want to know how many of them is high potency agaisnt LAT1 protein
#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution
#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()
# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 10−9
# Take the molar value and apply -log10
# Delete the standard_value column and create a new pIC50 column

#before we do that ,we need to install nescessery package

import numpy as np

def pIC50(input):
    pIC50 = []

    for i in input['standard_value']:
        molar = i/1000000000 # Converts nM to M
        pIC50.append(-np.log10(molar))

    input['pIC50'] = pIC50
    x = input.drop('standard_value', 1)
        
    return x
21/69: df3.standard_value.describe()
21/70: df4 = pIC50(df3)
21/71:
#We want to know how many of them is high potency agaisnt LAT1 protein
#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution
#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()
# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 10−9
# Take the molar value and apply -log10
# Delete the standard_value column and create a new pIC50 column

#before we do that ,we need to install nescessery package

import numpy as np

def pIC50(input):
    pIC50 = []

    for ic in input['standard_value']:
        molar = ic/1000000000 # Converts nM to M
        pIC50.append(-np.log10(molar))

    input['pIC50'] = pIC50
    x = input.drop('standard_value', 1)
        
    return x
21/72:
#We want to know how many of them is high potency agaisnt LAT1 protein
#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution
#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()
# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 10−9
# Take the molar value and apply -log10
# Delete the standard_value column and create a new pIC50 column

#before we do that ,we need to install nescessery package

import numpy as np

def pIC50(input):
    pIC50 = []

    for ic in input['standard_value']:
        molar = ic / 1000000000 # Converts nM to M
        pIC50.append(-np.log10(molar))

    input['pIC50'] = pIC50
    x = input.drop('standard_value', 1)
        
    return x
21/73: df4 = pIC50(df3)
21/74: df3.standard_value.describe()
21/75:
#We want to know how many of them is high potency agaisnt LAT1 protein
#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution
#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()
# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 10−9
# Take the molar value and apply -log10
# Delete the standard_value column and create a new pIC50 column

#before we do that ,we need to install nescessery package

import numpy as np

def pIC50(input):
    pIC50 = []

    for i in input['standard_value']:
        molar = i /1000000000 # Converts nM to M
        pIC50.append(-np.log10(molar))

    input['pIC50'] = pIC50
    x = input.drop('standard_value', 1)
        
    return x
21/76: df3.standard_value.describe()
21/77:
#We want to know how many of them is high potency agaisnt LAT1 protein
#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution
#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()
# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 10−9
# Take the molar value and apply -log10
# Delete the standard_value column and create a new pIC50 column

#before we do that ,we need to install nescessery package

import numpy as np

def pIC50(input):
    pIC50 = []

    for i in input['standard_value']:
        molar = int(i) /1000000000 # Converts nM to M
        pIC50.append(-np.log10(molar))

    input['pIC50'] = pIC50
    x = input.drop('standard_value', 1)
        
    return x
21/78: df3.standard_value.describe()
21/79: df4 = pIC50(df3)
21/80:
#We want to know how many of them is high potency agaisnt LAT1 protein
#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution
#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()
# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 10−9
# Take the molar value and apply -log10
# Delete the standard_value column and create a new pIC50 column

#before we do that ,we need to install nescessery package

import numpy as np

def pIC50(input):
    pIC50 = []

    for i in input['standard_value']:
        molar = int(i)/1000000000 # Converts nM to M
        pIC50.append(-np.log10(molar))

    input['pIC50'] = pIC50
    x = input.drop('standard_value', 1)
        
    return x
21/81: df4 = pIC50(df3)
21/82: int(i)
21/83:
#We want to know how many of them is high potency agaisnt LAT1 protein
#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution
#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()
# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 10−9
# Take the molar value and apply -log10
# Delete the standard_value column and create a new pIC50 column

#before we do that ,we need to install nescessery package

import numpy as np

def pIC50(input):
    pIC50 = []

    for i in input['standard_value']:
        molar = int(i)/1000000000 # Converts nM to M
        pIC50.append(-np.log10(molar))

    input['pIC50'] = pIC50
    x = input.drop('standard_value', 1)
        
    return x
21/84:
#We want to know how many of them is high potency agaisnt LAT1 protein
#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution
#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()
# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 10−9
# Take the molar value and apply -log10
# Delete the standard_value column and create a new pIC50 column

#before we do that ,we need to install nescessery package

import numpy as np

def pIC50(input):
    pIC50 = []

    for i in input['standard_value']:
        molar = int(i/1000000000) # Converts nM to M
        pIC50.append(-np.log10(molar))

    input['pIC50'] = pIC50
    x = input.drop('standard_value', 1)
        
    return x
21/85: df4 = pIC50(df3)
21/86:
#We want to know how many of them is high potency agaisnt LAT1 protein
#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution
#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()
# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 10−9
# Take the molar value and apply -log10
# Delete the standard_value column and create a new pIC50 column

#before we do that ,we need to install nescessery package

import numpy as np

def pIC50(input):
    pIC50 = []

    for i in input['standard_value']:
        molar = int(i)/1000000000 # Converts nM to M
        pIC50.append(-np.log10(molar))

    input['pIC50'] = pIC50
    x = input.drop('standard_value', 1)
        
    return x
21/87: df4 = pIC50(df3)
21/88:
#create lipinski rule of five
import numpy as np
from rdkit import Chem
from rdkit.Chem import Descriptors, Lipinski
21/89:
#create lipinski rule of five
import numpy as np
import rdkit  
from rdkit import Chem
from rdkit.Chem import Descriptors, Lipinski
21/90:
#create lipinski rule of five
import numpy as np
import rdkit
21/91:
#create lipinski rule of five
import numpy as np
import rdKit
21/92:
#create lipinski rule of five
import numpy as np
import rdkit
21/93:
#create lipinski rule of five
import numpy as np
import rdkit 
from rdkit import Chem
from rdkit.Chem import Descriptors, Lipinski
21/94:
#create lipinski rule of five
import numpy as np
import sys
from rdkit import Chem
from rdkit.Chem import Descriptors, Lipinski
21/95:
#create lipinski rule of five
import numpy as np
import sys
import rdkit
from rdkit import Chem
from rdkit.Chem import Descriptors, Lipinski
21/96:
#create lipinski rule of five
pip install rdkit-pypi
21/97: pip install rdkit-pypi
21/98:
import numpy as np
import rdkit
21/99:
#We are going to calculate Lipinski rule of five
from rdkit import chem
from rdkit.Chem import Descriptors, Lipinski
21/100:
#We are going to calculate Lipinski rule of five
from rdkit import chem
21/101:
#We are going to calculate Lipinski rule of five
from rdkit import Chem
21/102:
#We are going to calculate Lipinski rule of five
from rdkit import Chem
from rdkit.Chem import Descriptors, Lipinski
21/103: pip install -U notebook-as-pdf
21/104: pyppeteer-install
21/105: jupyter nbconvert --to html notebook.ipynb
21/106: pyppeteer-install
21/107:
#install pdf file
pip install -U notebook-as-pdf
pyppeteer-install
21/108:
#install pdf file
pip install -U notebook-as-pdf
pip install pyppeteer
21/109:
#install pdf file
pip install -U notebook-as-pdf
pip install pyppeteer
21/110:
pip install -U notebook-as-pdf
pip install pyppeteer
21/111:
pip install -U notebook-as-pdf
pip install pyppeteer
21/112:
pip install -U notebook-as-pdf
pip install pyppeteer
21/113:
import numpy as np
import rdkit
from time import time

from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix
import seaborn as sns
%matplotlib inline
21/114:
import numpy as np
import rdkit
from time import time

from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
21/115:
def lipinski(smiles, verbose=False):

    moldata= []
    for elem in smiles:
        mol=Chem.MolFromSmiles(elem) 
        moldata.append(mol)
       
    baseData= np.arange(1,1)
    i=0  
    for mol in moldata:        
       
        desc_MolWt = Descriptors.MolWt(mol)
        desc_MolLogP = Descriptors.MolLogP(mol)
        desc_NumHDonors = Lipinski.NumHDonors(mol)
        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)
           
        row = np.array([desc_MolWt,
                        desc_MolLogP,
                        desc_NumHDonors,
                        desc_NumHAcceptors])   
    
        if(i==0):
            baseData=row
        else:
            baseData=np.vstack([baseData, row])
        i=i+1      
    
    columnNames=["MW","LogP","NumHDonors","NumHAcceptors"]   
    descriptors = pd.DataFrame(data=baseData,columns=columnNames)
    
    return descriptor
21/116: df_lipinski = lipinski(df3.canonical_smiles)
21/117:
def lipinski(smiles, verbose=False):

    moldata= []
    for elem in smiles:
        mol=Chem.MolFromSmiles(elem) 
        moldata.append(mol)
       
    baseData= np.arange(1,1)
    i=0  
    for mol in moldata:        
       
        desc_MolWt = Descriptors.MolWt(mol)
        desc_MolLogP = Descriptors.MolLogP(mol)
        desc_NumHDonors = Lipinski.NumHDonors(mol)
        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)
           
        row = np.array([desc_MolWt,
                        desc_MolLogP,
                        desc_NumHDonors,
                        desc_NumHAcceptors])   
    
        if(i==0):
            baseData=row
        else:
            baseData=np.vstack([baseData, row])
        i=i+1      
    
    columnNames=["MW","LogP","NumHDonors","NumHAcceptors"]   
    descriptors = pd.DataFrame(data=baseData,columns=columnNames)
    
    return descriptor
21/118:
def lipinski(smiles, verbose=False):

    moldata= []
    for elem in smiles:
        mol=Chem.MolFromSmiles(elem) 
        moldata.append(mol)
       
    baseData= np.arange(1,1)
    i=0  
    for mol in moldata:        
       
        desc_MolWt = Descriptors.MolWt(mol)
        desc_MolLogP = Descriptors.MolLogP(mol)
        desc_NumHDonors = Lipinski.NumHDonors(mol)
        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)
           
        row = np.array([desc_MolWt,
                        desc_MolLogP,
                        desc_NumHDonors,
                        desc_NumHAcceptors])   
    
        if(i==0):
            baseData=row
        else:
            baseData=np.vstack([baseData, row])
        i=i+1      
    
    columnNames=["MW","LogP","NumHDonors","NumHAcceptors"]   
    descriptors = pd.DataFrame(data=baseData,columns=columnNames)
    
    return descriptors
21/119: df_lipinski = lipinski(df3.canonical_smiles)
21/120:
df_lipinski = lipinski(df3.canonical_smiles)
df_lipinski
21/121:
def lipinski(smiles, verbose=False):

    moldata= []
    for elem in smiles:
        mol=Chem.MolFromSmiles(elem) 
        moldata.append(mol)
       
    baseData= np.arange(1,1)
    i=0  
    for mol in moldata:        
       
        desc_MolWt = Descriptors.MolWt(mol)
        desc_MolLogP = Descriptors.MolLogP(mol)
        desc_NumHDonors = Lipinski.NumHDonors(mol)
        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)
        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)
           
        row = np.array([desc_MolWt,
                        desc_MolLogP,
                        desc_NumHDonors,
                        desc_NumHAcceptors,
                        desc_NumRotatableBonds])   
    
        if(i==0):
            baseData=row
        else:
            baseData=np.vstack([baseData, row])
        i=i+1      
    
    columnNames=["MW","LogP","NumHDonors","NumHAcceptors", "NumRotatableBonds"]   
    descriptors = pd.DataFrame(data=baseData,columns=columnNames)
    
    return descriptors
21/122:
df_lipinski = lipinski(df3.canonical_smiles)
df_lipinski
21/123:
#combine results
df_combined = pd.concat([df3,df_lipinski], axis=1)
df_combined
21/124:
#We want to know the structure and activity
selection = ['molecule_chembl_id','canonical_smiles','standard_units', 'standard_value']
df3 = df2[selection]
df3
21/125:
#combine results
df_combined = pd.concat([df3,df_lipinski], axis=1)
df_combined
21/126:
df_lipinski = lipinski(df3.canonical_smiles)
df_lipinski
21/127:
#combine results
df_combined = pd.concat([df3,df_lipinski], axis=1)
df_combined
21/128:
df_lipinski = lipinski(df3.canonical_smiles)
df_lipinski
df_lipinski,tail(3)
21/129:
df_lipinski = lipinski(df3.canonical_smiles)
df_lipinski
df_lipinski.tail(3)
21/130:
df_lipinski = lipinski(df3.canonical_smiles)
df_lipinski
df_lipinski.head(3)
21/131:
df_lipinski = lipinski(df3.canonical_smiles)
df_lipinski
df_lipinski
21/132:
#We want to know the structure and activity
selection = ['molecule_chembl_id','canonical_smiles','standard_units', 'standard_value']
df3 = df2[selection]
df3
21/133:
def lipinski(smiles, verbose=False):

    moldata= []
    for elem in smiles:
        mol=Chem.MolFromSmiles(elem) 
        moldata.append(mol)
       
    baseData= np.arange(1,1)
    i=0  
    for mol in moldata:        
       
        desc_MolWt = Descriptors.MolWt(mol)
        desc_MolLogP = Descriptors.MolLogP(mol)
        desc_NumHDonors = Lipinski.NumHDonors(mol)
        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)
        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)
           
        row = np.array([desc_MolWt,
                        desc_MolLogP,
                        desc_NumHDonors,
                        desc_NumHAcceptors,
                        desc_NumRotatableBonds])   
    
        if(i==0):
            baseData=row
        else:
            baseData=np.vstack([baseData, row])
        i=i+1      
    
    columnNames=["MW","LogP","NumHDonors","NumHAcceptors", "NumRotatableBonds"]   
    descriptors = pd.DataFrame(data=baseData,columns=columnNames)
    
    return descriptors
21/134:
df4 = df3[df.canonical_smiles.notna()
df4
21/135:
df4 = df3[df.canonical_smiles.notna()
df4
21/136: df4 = df3[df.canonical_smiles.notna()
21/137: df4 = df3[df.canonical_smiles.notna()]
21/138:
df4 = df3[df.canonical_smiles.notna()]
df4
21/139:
df_lipinski = lipinski(df4.canonical_smiles)
df_lipinski
df_lipinski
21/140:
import pandas as pd
def lipinski(smiles, verbose=False):

    moldata= []
    for elem in smiles:
        mol=Chem.MolFromSmiles(elem) 
        moldata.append(mol)
       
    baseData= np.arange(1,1)
    i=0  
    for mol in moldata:        
       
        desc_MolWt = Descriptors.MolWt(mol)
        desc_MolLogP = Descriptors.MolLogP(mol)
        desc_NumHDonors = Lipinski.NumHDonors(mol)
        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)
        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)
           
        row = np.array([desc_MolWt,
                        desc_MolLogP,
                        desc_NumHDonors,
                        desc_NumHAcceptors,
                        desc_NumRotatableBonds])   
    
        if(i==0):
            baseData=row
        else:
            baseData=np.vstack([baseData, row])
        i=i+1      
    
    columnNames=["MW","LogP","NumHDonors","NumHAcceptors", "NumRotatableBonds"]   
    descriptors = pd.DataFrame(data=baseData,columns=columnNames)
    
    return descriptors
21/141:
df_lipinski = lipinski(df4.canonical_smiles)
df_lipinski
df_lipinski
21/142:
import pandas as pd
def lipinski(smiles, verbose=False):

    moldata= []
    for elem in smiles:
        mol=Chem.MolFromSmiles(elem) 
        moldata.append(mol)
       
    baseData= np.arange(1,1)
    i=0  
    for mol in moldata:        
       
        desc_MolWt = Descriptors.MolWt(mol)
        desc_MolLogP = Descriptors.MolLogP(mol)
        desc_NumHDonors = Lipinski.NumHDonors(mol)
        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)
        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)
           
        row = np.array([desc_MolWt,
                        desc_MolLogP,
                        desc_NumHDonors,
                        desc_NumHAcceptors,
                        desc_NumRotatableBonds])   
    
        if(i==0):
            baseData=row
        else:
            baseData=np.vstack([baseData, row])
        i=i      
    
    columnNames=["MW","LogP","NumHDonors","NumHAcceptors", "NumRotatableBonds"]   
    descriptors = pd.DataFrame(data=baseData,columns=columnNames)
    
    return descriptors
21/143:
df_lipinski = lipinski(df4.canonical_smiles)
df_lipinski
df_lipinski
21/144:
import pandas as pd
def lipinski(smiles, verbose=False):

    moldata= []
    for elem in smiles:
        mol=Chem.MolFromSmiles(elem) 
        moldata.append(mol)
       
    baseData= np.arange(1,1)
    i=0  
    for mol in moldata:        
       
        desc_MolWt = Descriptors.MolWt(mol)
        desc_MolLogP = Descriptors.MolLogP(mol)
        desc_NumHDonors = Lipinski.NumHDonors(mol)
        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)
        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)
           
        row = np.array([desc_MolWt,
                        desc_MolLogP,
                        desc_NumHDonors,
                        desc_NumHAcceptors,
                        desc_NumRotatableBonds])   
    
        if(i==0):
            baseData=row
        else:
            baseData=np.vstack([baseData, row])
        i=i+1      
    
    columnNames=["MW","LogP","NumHDonors","NumHAcceptors", "NumRotatableBonds"]   
    descriptors = pd.DataFrame(data=baseData,columns=columnNames)
    
    return descriptors
21/145:
df_lipinski = lipinski(df4.canonical_smiles)
df_lipinski
df_lipinski
21/146:
import pandas as pd
def lipinski(smiles, verbose=False):

    moldata= []
    for elem in smiles:
        mol=Chem.MolFromSmiles(elem) 
        moldata.append(mol)
       
    baseData= np.arange(0,1)
    i=0  
    for mol in moldata:        
       
        desc_MolWt = Descriptors.MolWt(mol)
        desc_MolLogP = Descriptors.MolLogP(mol)
        desc_NumHDonors = Lipinski.NumHDonors(mol)
        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)
        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)
           
        row = np.array([desc_MolWt,
                        desc_MolLogP,
                        desc_NumHDonors,
                        desc_NumHAcceptors,
                        desc_NumRotatableBonds])   
    
        if(i==0):
            baseData=row
        else:
            baseData=np.vstack([baseData, row])
        i=i+1      
    
    columnNames=["MW","LogP","NumHDonors","NumHAcceptors", "NumRotatableBonds"]   
    descriptors = pd.DataFrame(data=baseData,columns=columnNames)
    
    return descriptors
21/147:
df_lipinski = lipinski(df4.canonical_smiles)
df_lipinski
df_lipinski
21/148:
import pandas as pd
def lipinski(smiles, verbose=False):

    moldata= []
    for elem in smiles:
        mol=Chem.MolFromSmiles(elem) 
        moldata.append(mol)
       
    baseData= np.arange(1,1)
    i=0  
    for mol in moldata:        
       
        desc_MolWt = Descriptors.MolWt(mol)
        desc_MolLogP = Descriptors.MolLogP(mol)
        desc_NumHDonors = Lipinski.NumHDonors(mol)
        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)
        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)
           
        row = np.array([desc_MolWt,
                        desc_MolLogP,
                        desc_NumHDonors,
                        desc_NumHAcceptors,
                        desc_NumRotatableBonds])   
    
        if(i==0):
            baseData=row
        else:
            baseData=np.vstack([baseData, row])
        i=i+1      
    
    columnNames=["MW","LogP","NumHDonors","NumHAcceptors", "NumRotatableBonds"]   
    descriptors = pd.DataFrame(data=baseData,columns=columnNames)
    
    return descriptors
21/149:
df_lipinski = lipinski(df4.canonical_smiles)
df_lipinski
df_lipinski
21/150: print(df4."[canonical_smiles]".unique()
21/151: print(df4.'canonical_smiles]'.unique()
21/152: print(df4.canonical_smiles.unique())
21/153: print(df4.molecule_chembl_id .unique())
21/154: print(df4.canonical_smiles.unique())
21/155:
import pandas as pd
def lipinski(smiles, verbose=False):

    moldata= []
    for elem in smiles:
        mol=Chem.MolFromSmiles(elem) 
        moldata.append(mol)
       
    baseData= np.arange(1,1)
    i=0  
    for mol in moldata:        
       
        desc_MolWt = Descriptors.MolWt(mol)
        desc_MolLogP = Descriptors.MolLogP(mol)
        desc_NumHDonors = Lipinski.NumHDonors(mol)
        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)
        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)
           
        row = np.array([desc_MolWt,
                        desc_MolLogP,
                        desc_NumHDonors,
                        desc_NumHAcceptors,
                        desc_NumRotatableBonds])   
    
        if(i==0):
            baseData=row
        else:
            baseData=np.vstack([baseData, row])
        i=i+1      
    
    columnNames=["MW","LogP","NumHDonors","NumHAcceptors", "NumRotatableBonds"]   
    descriptors = pd.DataFrame(data=baseData,columns=columnNames)
    
    return descriptors
21/156:
df_lipinski = lipinski(df4.canonical_smiles)
df_lipinski
df_lipinski
21/157:
#combine results
df_combined = pd.concat([df3,df_lipinski])
df_combined
21/158:
#combine results
df_combined = pd.concat([df3,df_lipinski], axis=1)
df_combined
21/159:
#combine results
df_combined = pd.concat([df3,df_lipinski], axis=1)
df_combined
21/160:
#remove na
final_df_combined = final_df_combined.replace(['None','NONE','none'],np.nan)
final_df_combined = final_df_combined.dropna()
#print('Unique value in MW column',final_df_combined['MW'].unique())
len(final_df_combined)
21/161:
#remove na
final_df_combined = df_combined.replace(['None','NONE','none'],np.nan)
final_df_combined = final_df_combined.dropna()
#print('Unique value in MW column',final_df_combined['MW'].unique())
len(final_df_combined)
21/162:
#remove na
final_df_combined = df_combined.replace(['None','NONE','none'],np.nan)
final_df_combined = final_df_combined.dropna()
print('Unique value in MW column',final_df_combined['MW'].unique())
len(final_df_combined)
21/163: print(df4['canonical_smiles'].unique())
21/164:
#We want to know how many of them is high potency agaisnt LAT1 protein
#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution
#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()
# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 10−9
# Take the molar value and apply -log10
# Delete the standard_value column and create a new pIC50 column

#before we do that ,we need to install nescessery package

import numpy as np

def pIC50(input):
    pIC50 = []

    for i in input['standard_value_norm']:
        molar = i*(10**-9) # Converts nM to M
        pIC50.append(-np.log10(molar))

    input['pIC50'] = pIC50
    x = input.drop('standard_value_norm', 1)
        
    return x
21/165: final_df_combined.standard_value.describe()
21/166: final_df_combined
21/167: df% = pIC50(final_df_combined)
21/168: df5 = pIC50(final_df_combined)
21/169:
def norm_value(input):
    norm = []

    for i in input['standard_value']:
        if i > 100000000:
          i = 100000000
        norm.append(i)

    input['standard_value_norm'] = norm
    x = input.drop('standard_value', 1)
        
    return x
21/170:
def norm_value(input):
    norm = []

    for i in input['standard_value']:
        if i > 100000000:
          i = 100000000
        norm.append(i)

    input['standard_value_norm'] = norm
    x = input.drop('standard_value', 1)
        
    return x
21/171:
df_norm = norm_value(df_combined)
df_norm
21/172:
def norm_value(input):
    norm = []

    for i in input['standard_value']:
        if int(i) > 100000000:
          i = 100000000
        norm.append(i)

    input['standard_value_norm'] = norm
    x = input.drop('standard_value', 1)
        
    return x
21/173:
df_norm = norm_value(final_df_combined)
df_norm
21/174:
def norm_value(input):
    norm = []

    for i in input['standard_value']:
        if float(i) > 100000000:
          i = 100000000
        norm.append(i)

    input['standard_value_norm'] = norm
    x = input.drop('standard_value', 1)
        
    return x
21/175:
df_norm = norm_value(final_df_combined)
df_norm
21/176:
#We want to know how many of them is high potency agaisnt LAT1 protein
#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution
#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()
# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 10−9
# Take the molar value and apply -log10
# Delete the standard_value column and create a new pIC50 column

#before we do that ,we need to install nescessery package

import numpy as np

def pIC50(input):
    pIC50 = []

    for i in input['standard_value_norm']:
        molar = i*(10**-9) # Converts nM to M
        pIC50.append(-np.log10(molar))

    input['pIC50'] = pIC50
    x = input.drop('standard_value_norm', 1)
        
    return x
21/177: final_df_combined.standard_value.describe()
21/178:
def norm_value(input):
    norm = []

    for i in input['standard_value']:
        if float(i) > 100000000:
          i = 100000000
        norm.append(i)

    input['standard_value_norm'] = norm
    x = input.drop('standard_value', 1)
        
    return x
21/179:
df_norm = norm_value(final_df_combined)
df_norm
21/180: df_norm.standard_value_norm.describe()
21/181:
df_final = pIC50(df_norm)
df_final
21/182:
#We want to know how many of them is high potency agaisnt LAT1 protein
#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution
#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()
# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 10−9
# Take the molar value and apply -log10
# Delete the standard_value column and create a new pIC50 column

#before we do that ,we need to install nescessery package

import numpy as np

def pIC50(input):
    pIC50 = []

    for i in input['standard_value_norm']:
        molar = float(i)*(10**-9) # Converts nM to M
        pIC50.append(-np.log10(molar))

    input['pIC50'] = pIC50
    x = input.drop('standard_value_norm', 1)
        
    return x
21/183:
df_final = pIC50(df_norm)
df_final
21/184:
# define active, inactive, intermediate compounds

STATUS = []

for i in fix:
    if i <=1000:
        STATUS.append("active") #active
        
    elif i >=10000:
        STATUS.append("inactive") #inactive
        
    else:
        STATUS.append("intermediate") #intermediate
21/185: -np.log10(10000)
21/186: np.log10(10000)
21/187: -np.log10(0.000001)
21/188: -np.log10(0.00001)
21/189:
bioactivity_class = []
for i in df_final.pIC50:
  if float(i) >= 6:
    bioactivity_class.append("inactive")
  elif float(i) <= 5:
    bioactivity_class.append("active")
21/190:
bioactivity_class = pd.Series(bioactivity_class, name='bioactivity_class')
df_class = pd.concat([df_final, bioactivity_class], axis=1)
df_class
21/191:
bioactivity_class = []
for i in df_final.pIC50:
  if float(i) >= 6:
    bioactivity_class.append("inactive")
  elif float(i) <= 5:
    bioactivity_class.append("active")
      else :
        bioactivity_class.append("intermediate")
21/192:
bioactivity_class = []
for i in df_final.pIC50:
  if float(i) >= 6:
    bioactivity_class.append("inactive")
  elif float(i) <= 5:
    bioactivity_class.append("active")
    else :
        bioactivity_class.append("intermediate")
21/193:
bioactivity_class = []
for i in df_final.pIC50:
  if float(i) >= 6:
    bioactivity_class.append("inactive")
  elif float(i) <= 5:
    bioactivity_class.append("active")
  else :
        bioactivity_class.append("intermediate")
21/194:
bioactivity_class = pd.Series(bioactivity_class, name='bioactivity_class')
df_class = pd.concat([df_final, bioactivity_class], axis=1)
df_class
21/195:
bioactivity_class = []
for i in df_final.pIC50:
  if float(i) >= 6:
    bioactivity_class.append("inactive")
  elif float(i) <= 5:
    bioactivity_class.append("active")
  else:
        bioactivity_class.append("intermediate")
21/196:
bioactivity_class = pd.Series(bioactivity_class, name='bioactivity_class')
df_class = pd.concat([df_final, bioactivity_class], axis=1)
df_class
21/197:
bioactivity_class = []
for i in df_final.pIC50:
  if float(i) >= 6:
    bioactivity_class.append("inactive")
  elif float(i) <= 5:
    bioactivity_class.append("active")
  else:
    bioactivity_class.append("intermediate")
21/198:
bioactivity_class = pd.Series(bioactivity_class, name='bioactivity_class')
df_class = pd.concat([df_final, bioactivity_class], axis=1)
df_class
21/199: bioactivity_class
21/200: df_final.pIC50
21/201: bioactivity_class
21/202: print(bioactivity_class)
21/203: print(bioactivity_class.unique())
21/204: len(bioactivity_class.unique())
21/205: len(bioactivity_class)
21/206: len(df_final)
21/207: result = pd.concat([df_final, bioactivity_class], axis=1, join="inner")
21/208:
result = pd.concat([df_final, bioactivity_class], axis=1, join="inner")
result
21/209: len(result)
21/210: bioactivity_class
21/211: df_class = pd.DataFrame(bioactivity_class)
21/212:
df_class = pd.DataFrame(bioactivity_class)
df_class
21/213: df_final_2 = pdf.concat[df_final, df_class]
21/214: df_final_2 = pd.concat[df_final, df_class]
21/215: df_final_2 = pd.concat([df_final, df_class])
21/216:
df_final_2 = pd.concat([df_final, df_class])
df_final_2
21/217:
df_final_2 = pd.concat([df_final, df_class], axis =1)
df_final_2
21/218: len(df_class)
21/219: len(df_final)
21/220: print(df_final_2[bioactivity_class].unique())
21/221: print(df_final_2['bioactivity_class'].unique())
21/222:
df_class = pd.DataFrame(bioactivity_class)
df_class
df.to_csv('df_class.csv')
21/223: df_class
21/224:
df_class = pd.DataFrame(bioactivity_class)
df_class
df_class.to_csv('df_class.csv')
21/225: df_final,to_csv('df_final.csv')
21/226: df_final.to_csv('df_final.csv')
21/227:
df_final_3 = df_final['molecule)chembl_id', 'canonical_smiles', 'MW', 'LogP','NumHDonors',
                      'NumHAcceptors', 'NumRotableBonds', 'pIC50']
21/228:
df_final_3 = pd.DataFrame.df_final['molecule)chembl_id', 'canonical_smiles', 'MW', 'LogP','NumHDonors',
                      'NumHAcceptors', 'NumRotableBonds', 'pIC50']
21/229:
df_final_3 = df_final['molecule)chembl_id', 'canonical_smiles', 'MW', 'LogP','NumHDonors',
                      'NumHAcceptors', 'NumRotableBonds', 'pIC50']
21/230:
selection = ['molecule)chembl_id', 'canonical_smiles', 'MW', 'LogP','NumHDonors',
                      'NumHAcceptors', 'NumRotableBonds', 'pIC50']
df_final_3 = df_final['molecule)chembl_id', 'canonical_smiles', 'MW', 'LogP','NumHDonors',
                      'NumHAcceptors', 'NumRotableBonds', 'pIC50']
21/231:
selection = ['molecule)chembl_id', 'canonical_smiles', 'MW', 'LogP','NumHDonors',
                      'NumHAcceptors', 'NumRotableBonds', 'pIC50']
df_final_3 = df_final[selection]
21/232:
selection = ['molecule_chembl_id', 'canonical_smiles', 'MW', 'LogP','NumHDonors',
                      'NumHAcceptors', 'NumRotatableBonds', 'pIC50']
df_final_3 = df_final[selection]
21/233:
selection = ['molecule_chembl_id', 'canonical_smiles', 'MW', 'LogP','NumHDonors',
                      'NumHAcceptors', 'NumRotatableBonds', 'pIC50']
df_final_3 = df_final[selection]
df_final_3
21/234:
df_final_4 = pd.concat([df_final_3, df_class], axis =1)
df_final_2
21/235: pd.read_csv('df_final.csv')
21/236:
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt
21/237:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

# Histogram for MW
hist, bins = np.histogram(MW, 50)
width = 1 * (bins[1] - bins[0])
center = (bins[:-1] + bins[1:]) / 2
plt1.bar(center, hist, align='center', width=width, color='blue',edgecolor='black',\
         error_kw=dict(ecolor='gray', lw=2, capsize=5, capthick=2,linestyle='-', linewidth=0.5))
21/238:
#Here we got RO5 and pIC50 of 71 cpds agaisnt LAT1 protein

Df_analysis = pd.read_csv('df_final.csv')
21/239:
#Here we got RO5 and pIC50 of 71 cpds agaisnt LAT1 protein

df_analysis = pd.read_csv('df_final.csv')
21/240:
#Here we got RO5 and pIC50 of 71 cpds agaisnt LAT1 protein

data = pd.read_csv('df_final.csv')
21/241:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

# Histogram for MW
plt.hist(data['MW'])
21/242:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

# Histogram for MW
plt.hist(data['MW'], frequency = False)
21/243:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

# Histogram for MW
plt.hist(data['MW'], density=True)
21/244:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

# Histogram for MW
plt.hist(data['MW'], density=False)
21/245:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

# Histogram for MW
plt.hist(data['MW'], density=False)
ply.xlabel = Molecular Weight
21/246:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

# Histogram for MW
plt.hist(data['MW'], density=False)
ply.xlabel("Molecular Weight")
21/247:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

# Histogram for MW
plt.hist(data['MW'], density=False)
plt.xlabel("Molecular Weight")
21/248:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

# Histogram for MW
plt.hist(data['MW'], density=False)
plt.xlabel("Molecular Weight")
plt.ylabel("Frequency")
21/249:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

# Histogram for MW
plt.hist(data['MW'], density=True)
plt.xlabel("Molecular Weight")
plt.ylabel("Frequency")
21/250:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

# Histogram for MW
plt.hist(data['MW'], density=False, bins= 20)
plt.xlabel("Molecular Weight")
plt.ylabel("Frequency")
21/251:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

# Histogram for MW
plt.hist(data['MW'], density=False, bins= 50)
plt.xlabel("Molecular Weight")
plt.ylabel("Frequency")
21/252:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

# Histogram for MW
plt.hist(data['MW'], density=False, bins= 10)
plt.xlabel("Molecular Weight")
plt.ylabel("Frequency")
21/253:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

# Histogram for MW
plt.hist(data['MW'], density=False, bins= 5)
plt.xlabel("Molecular Weight")
plt.ylabel("Frequency")
21/254:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

# Histogram for MW
plt.hist(data['MW'], density=False, bins= 5)
plt.xlabel("Molecular Weight")
plt.ylabel("Frequency")
21/255:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

# Histogram for MW
plt.hist(data['MW'], density=False, bins= 15)
plt.xlabel("Molecular Weight")
plt.ylabel("Frequency")
21/256:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

# Histogram for MW
plt.hist(data['MW'], density=False, bins= 1)
plt.xlabel("Molecular Weight")
plt.ylabel("Frequency")
21/257:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

# Histogram for MW
plt.hist(data['MW'], density=False, bins= 10)
plt.xlabel("Molecular Weight")
plt.ylabel("Frequency")
21/258:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

# Histogram for MW
plt.hist(data['MW'], density=False, bins= 10)
plt.xlabel("Molecular Weight")
plt.ylabel("Frequency")
plt1.tick_params(axis='both', which='major', labelsize=14)
21/259:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

# Histogram for MW
plt.hist(data['MW'], density=False, bins= 10)
plt.xlabel("Molecular Weight")
plt.ylabel("Frequency")
plt.tick_params(axis='both', which='major', labelsize=14)
21/260:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

# Histogram for MW
plt.hist(data['MW'], density=False, bins= 10)
plt.xlabel("Molecular Weight")
plt.ylabel("Frequency")
21/261:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4)) = plt.subplots(2, 2)
figure.set_size_inches(15,10)

# Histogram for MW
plt.hist(data['MW'], density=False, bins= 10)
plt.xlabel("Molecular Weight")
plt.ylabel("Frequency")
21/262:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4)) = plt.subplots(2, 2)
figure.set_size_inches(15,10)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 10)
plt1.xlabel("Molecular Weight")
plt1.ylabel("Frequency")
21/263:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4)) = plt.subplots(2, 2)
figure.set_size_inches(15,10)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 10)
plt1.set_xlabel("Molecular Weight")
plt1.ylabel("Frequency")
21/264:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4)) = plt.subplots(2, 2)
figure.set_size_inches(15,10)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 10)
plt1.set_xlabel("Molecular Weight")
plt1.set_ylabel("Frequency")
21/265:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4)) = plt.subplots(2, 2)
figure.set_size_inches(15,10)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 10)
plt1.set_xlabel("Molecular Weight")
plt1.set_ylabel("Frequency")

#Histogram for Log P
plt1.hist(data['LogP'], density=False, bins= 10)
plt1.set_xlabel("LogP")
plt1.set_ylabel("Frequency")

#Histogram for Log P
plt1.hist(data['NumHDonors'], density=False, bins= 10)
plt1.set_xlabel("NumHDonors")
plt1.set_ylabel("Frequency")

#Histogram for Log P
plt1.hist(data['NumHAcceptors'], density=False, bins= 10)
plt1.set_xlabel("NumHAcceptors")
plt1.set_ylabel("Frequency")

#Histogram for Log P
plt1.hist(data['NumRotatableBonds'], density=False, bins= 10)
plt1.set_xlabel("NumRotatableBonds")
plt1.set_ylabel("Frequency")
21/266:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4)) = plt.subplots(2, 2)
figure.set_size_inches(15,10)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 10)
plt1.set_xlabel("Molecular Weight")
plt1.set_ylabel("Frequency")

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 10)
plt2.set_xlabel("LogP")
plt2.set_ylabel("Frequency")

#Histogram for Log P
plt3.hist(data['NumHDonors'], density=False, bins= 10)
plt3.set_xlabel("NumHDonors")
plt3.set_ylabel("Frequency")

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 10)
plt4.set_xlabel("NumHAcceptors")
plt4.set_ylabel("Frequency")

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)
plt5.set_xlabel("NumRotatableBonds")
plt5.set_ylabel("Frequency")
21/267:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,)) = plt.subplots(3, 3)
figure.set_size_inches(15,10)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 10)
plt1.set_xlabel("Molecular Weight")
plt1.set_ylabel("Frequency")

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 10)
plt2.set_xlabel("LogP")
plt2.set_ylabel("Frequency")

#Histogram for Log P
plt3.hist(data['NumHDonors'], density=False, bins= 10)
plt3.set_xlabel("NumHDonors")
plt3.set_ylabel("Frequency")

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 10)
plt4.set_xlabel("NumHAcceptors")
plt4.set_ylabel("Frequency")

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)
plt5.set_xlabel("NumRotatableBonds")
plt5.set_ylabel("Frequency")
21/268:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,)) = plt.subplots(2, 2)
figure.set_size_inches(15,10)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 10)
plt1.set_xlabel("Molecular Weight")
plt1.set_ylabel("Frequency")

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 10)
plt2.set_xlabel("LogP")
plt2.set_ylabel("Frequency")

#Histogram for Log P
plt3.hist(data['NumHDonors'], density=False, bins= 10)
plt3.set_xlabel("NumHDonors")
plt3.set_ylabel("Frequency")

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 10)
plt4.set_xlabel("NumHAcceptors")
plt4.set_ylabel("Frequency")

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)
plt5.set_xlabel("NumRotatableBonds")
plt5.set_ylabel("Frequency")
21/269:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,)) = plt.subplots(2, 2)
figure.set_size_inches(15,10)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 10)
plt1.set_xlabel("Molecular Weight")
plt1.set_ylabel("Frequency")

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 10)
plt2.set_xlabel("LogP")
plt2.set_ylabel("Frequency")

#Histogram for Log P
plt3.hist(data['NumHDonors'], density=False, bins= 10)
plt3.set_xlabel("NumHDonors")
plt3.set_ylabel("Frequency")

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 10)
plt4.set_xlabel("NumHAcceptors")
plt4.set_ylabel("Frequency")

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)
plt5.set_xlabel("NumRotatableBonds")
plt5.set_ylabel("Frequency")
21/270:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,)) = plt.subplots(3, 2)
figure.set_size_inches(15,10)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 10)
plt1.set_xlabel("Molecular Weight")
plt1.set_ylabel("Frequency")

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 10)
plt2.set_xlabel("LogP")
plt2.set_ylabel("Frequency")

#Histogram for Log P
plt3.hist(data['NumHDonors'], density=False, bins= 10)
plt3.set_xlabel("NumHDonors")
plt3.set_ylabel("Frequency")

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 10)
plt4.set_xlabel("NumHAcceptors")
plt4.set_ylabel("Frequency")

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)
plt5.set_xlabel("NumRotatableBonds")
plt5.set_ylabel("Frequency")
21/271:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,10)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 10)
plt1.set_xlabel("Molecular Weight")
plt1.set_ylabel("Frequency")

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 10)
plt2.set_xlabel("LogP")
plt2.set_ylabel("Frequency")

#Histogram for Log P
plt3.hist(data['NumHDonors'], density=False, bins= 10)
plt3.set_xlabel("NumHDonors")
plt3.set_ylabel("Frequency")

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 10)
plt4.set_xlabel("NumHAcceptors")
plt4.set_ylabel("Frequency")

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)
plt5.set_xlabel("NumRotatableBonds")
plt5.set_ylabel("Frequency")
21/272:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,10)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 10)
plt1.set_xlabel("Molecular Weight")
plt1.set_ylabel("Frequency")
plt.axhline(y=0, x=500)


#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 10)
plt2.set_xlabel("LogP")
plt2.set_ylabel("Frequency")

#Histogram for Log P
plt3.hist(data['NumHDonors'], density=False, bins= 10)
plt3.set_xlabel("NumHDonors")
plt3.set_ylabel("Frequency")

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 10)
plt4.set_xlabel("NumHAcceptors")
plt4.set_ylabel("Frequency")

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)
plt5.set_xlabel("NumRotatableBonds")
plt5.set_ylabel("Frequency")
21/273:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,10)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 10)
plt1.set_xlabel("Molecular Weight")
plt1.set_ylabel("Frequency")
plt.axhline(y=0, xmax=500)


#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 10)
plt2.set_xlabel("LogP")
plt2.set_ylabel("Frequency")

#Histogram for Log P
plt3.hist(data['NumHDonors'], density=False, bins= 10)
plt3.set_xlabel("NumHDonors")
plt3.set_ylabel("Frequency")

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 10)
plt4.set_xlabel("NumHAcceptors")
plt4.set_ylabel("Frequency")

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)
plt5.set_xlabel("NumRotatableBonds")
plt5.set_ylabel("Frequency")
21/274:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,10)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 10)
plt1.set_xlabel("Molecular Weight")
plt1.set_ylabel("Frequency")
plt.axhline(y=0, xmax=500)


#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 10)
plt2.set_xlabel("LogP")
plt2.set_ylabel("Frequency")

#Histogram for Log P
plt3.hist(data['NumHDonors'], density=False, bins= 10)
plt3.set_xlabel("NumHDonors")
plt3.set_ylabel("Frequency")

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 10)
plt4.set_xlabel("NumHAcceptors")
plt4.set_ylabel("Frequency")

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)
plt5.set_xlabel("NumRotatableBonds")
plt5.set_ylabel("Frequency")
21/275:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,10)

# Histogram for MW
hist, bins = np.histogram(MW, 50)
width = 1 * (bins[1] - bins[0])
center = (bins[:-1] + bins[1:]) / 2
plt1.bar(center, hist, align='center', width=width, color='blue',edgecolor='black',\
         error_kw=dict(ecolor='gray', lw=2, capsize=5, capthick=2,linestyle='-', linewidth=0.5))
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel('Frequency', fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)
#plt1.set_xlim(200,900)
plt1.set_ylim(0, 1200)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)


#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 10)
plt2.set_xlabel("LogP")
plt2.set_ylabel("Frequency")

#Histogram for Log P
plt3.hist(data['NumHDonors'], density=False, bins= 10)
plt3.set_xlabel("NumHDonors")
plt3.set_ylabel("Frequency")

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 10)
plt4.set_xlabel("NumHAcceptors")
plt4.set_ylabel("Frequency")

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)
plt5.set_xlabel("NumRotatableBonds")
plt5.set_ylabel("Frequency")
21/276:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,10)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 10)
plt1.set_xlabel("Molecular Weight")
plt1.set_ylabel("Frequency")
plt1.tick_params(axis='both', which='major', labelsize=14)
#plt1.set_xlim(200,900)
plt1.set_ylim(0, 1200)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 10)
plt2.set_xlabel("LogP")
plt2.set_ylabel("Frequency")

#Histogram for Log P
plt3.hist(data['NumHDonors'], density=False, bins= 10)
plt3.set_xlabel("NumHDonors")
plt3.set_ylabel("Frequency")

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 10)
plt4.set_xlabel("NumHAcceptors")
plt4.set_ylabel("Frequency")

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)
plt5.set_xlabel("NumRotatableBonds")
plt5.set_ylabel("Frequency")
21/277:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,10)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 10)
plt1.set_xlabel("Molecular Weight")
plt1.set_ylabel("Frequency")
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 10)
plt2.set_xlabel("LogP")
plt2.set_ylabel("Frequency")

#Histogram for Log P
plt3.hist(data['NumHDonors'], density=False, bins= 10)
plt3.set_xlabel("NumHDonors")
plt3.set_ylabel("Frequency")

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 10)
plt4.set_xlabel("NumHAcceptors")
plt4.set_ylabel("Frequency")

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)
plt5.set_xlabel("NumRotatableBonds")
plt5.set_ylabel("Frequency")
21/278:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,10)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 10)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency")
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 10)
plt2.set_xlabel("LogP")
plt2.set_ylabel("Frequency")

#Histogram for Log P
plt3.hist(data['NumHDonors'], density=False, bins= 10)
plt3.set_xlabel("NumHDonors")
plt3.set_ylabel("Frequency")

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 10)
plt4.set_xlabel("NumHAcceptors")
plt4.set_ylabel("Frequency")

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)
plt5.set_xlabel("NumRotatableBonds")
plt5.set_ylabel("Frequency")
21/279:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(20,10)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 10)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency")
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 10)
plt2.set_xlabel("LogP")
plt2.set_ylabel("Frequency")

#Histogram for Log P
plt3.hist(data['NumHDonors'], density=False, bins= 10)
plt3.set_xlabel("NumHDonors")
plt3.set_ylabel("Frequency")

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 10)
plt4.set_xlabel("NumHAcceptors")
plt4.set_ylabel("Frequency")

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)
plt5.set_xlabel("NumRotatableBonds")
plt5.set_ylabel("Frequency")
21/280:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,20)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 10)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency")
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 10)
plt2.set_xlabel("LogP")
plt2.set_ylabel("Frequency")

#Histogram for Log P
plt3.hist(data['NumHDonors'], density=False, bins= 10)
plt3.set_xlabel("NumHDonors")
plt3.set_ylabel("Frequency")

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 10)
plt4.set_xlabel("NumHAcceptors")
plt4.set_ylabel("Frequency")

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)
plt5.set_xlabel("NumRotatableBonds")
plt5.set_ylabel("Frequency")
21/281:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 10)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency")
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 10)
plt2.set_xlabel("LogP")
plt2.set_ylabel("Frequency")

#Histogram for Log P
plt3.hist(data['NumHDonors'], density=False, bins= 10)
plt3.set_xlabel("NumHDonors")
plt3.set_ylabel("Frequency")

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 10)
plt4.set_xlabel("NumHAcceptors")
plt4.set_ylabel("Frequency")

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)
plt5.set_xlabel("NumRotatableBonds")
plt5.set_ylabel("Frequency")
21/282:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 10)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 10)
plt2.set_xlabel("LogP")
plt2.set_ylabel("Frequency")

#Histogram for Log P
plt3.hist(data['NumHDonors'], density=False, bins= 10)
plt3.set_xlabel("NumHDonors")
plt3.set_ylabel("Frequency")

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 10)
plt4.set_xlabel("NumHAcceptors")
plt4.set_ylabel("Frequency")

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)
plt5.set_xlabel("NumRotatableBonds")
plt5.set_ylabel("Frequency")
21/283:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 10, color='blue',edgecolor='black',\
         error_kw=dict(ecolor='gray', lw=2, capsize=5, capthick=2,linestyle='-', linewidth=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 10)
plt2.set_xlabel("LogP")
plt2.set_ylabel("Frequency")

#Histogram for Log P
plt3.hist(data['NumHDonors'], density=False, bins= 10)
plt3.set_xlabel("NumHDonors")
plt3.set_ylabel("Frequency")

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 10)
plt4.set_xlabel("NumHAcceptors")
plt4.set_ylabel("Frequency")

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)
plt5.set_xlabel("NumRotatableBonds")
plt5.set_ylabel("Frequency")
21/284:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 10, color='blue', edgecolor='black', linewidth=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 10)
plt2.set_xlabel("LogP")
plt2.set_ylabel("Frequency")

#Histogram for Log P
plt3.hist(data['NumHDonors'], density=False, bins= 10)
plt3.set_xlabel("NumHDonors")
plt3.set_ylabel("Frequency")

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 10)
plt4.set_xlabel("NumHAcceptors")
plt4.set_ylabel("Frequency")

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)
plt5.set_xlabel("NumRotatableBonds")
plt5.set_ylabel("Frequency")
21/285:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 10, color='blue', edgecolor='black', linewidth=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 10, color='blue', edgecolor='black', linewidth=0.5)
plt2.set_xlabel("LogP")
plt2.set_ylabel("Frequency")

#Histogram for Log P
plt3.hist(data['NumHDonors'], density=False, bins= 10)
plt3.set_xlabel("NumHDonors")
plt3.set_ylabel("Frequency")

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 10)
plt4.set_xlabel("NumHAcceptors")
plt4.set_ylabel("Frequency")

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)
plt5.set_xlabel("NumRotatableBonds")
plt5.set_ylabel("Frequency")
21/286:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 10, color='blue', edgecolor='black', linewidth=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 10, color='red', edgecolor='black', linewidth=0.5)
plt2.set_xlabel("LogP")
plt2.set_ylabel("Frequency")
plt2.set_ylim(0, 50)
plt1.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
#Histogram for Log P
plt3.hist(data['NumHDonors'], density=False, bins= 10)
plt3.set_xlabel("NumHDonors")
plt3.set_ylabel("Frequency")

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 10)
plt4.set_xlabel("NumHAcceptors")
plt4.set_ylabel("Frequency")

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)
plt5.set_xlabel("NumRotatableBonds")
plt5.set_ylabel("Frequency")
21/287:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 10, color='blue', edgecolor='black', linewidth=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 10, color='red', edgecolor='black', linewidth=0.5)
plt2.set_xlabel("LogP")
plt2.set_ylabel("Frequency")
plt2.set_ylim(0, 50)
plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
#Histogram for Log P
plt3.hist(data['NumHDonors'], density=False, bins= 10)
plt3.set_xlabel("NumHDonors")
plt3.set_ylabel("Frequency")

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 10)
plt4.set_xlabel("NumHAcceptors")
plt4.set_ylabel("Frequency")

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 10)
plt5.set_xlabel("NumRotatableBonds")
plt5.set_ylabel("Frequency")
21/288:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 10, color='blue', edgecolor='black', linewidth=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 10, color='red', edgecolor='black', linewidth=0.5)
plt2.set_xlabel("LogP", fontsize=16, fontweight='bold')
plt2.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt2.set_ylim(0, 50)
plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)


#Histogram for NumHDonors
plt3.hist(data['NumHDonors'], density=False, bins= 10, color='yellow', edgecolor='black', linewidth=0.5)
plt3.set_xlabel("Number of Hydrogen Bond Donors", fontsize=16, fontweight='bold')
plt3.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt3.set_ylim(0, 50)
plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 10, color='green', edgecolor='black', linewidth=0.5)
plt4.set_xlabel("Number of Hydrogen Bond Acceptors", fontsize=16, fontweight='bold')
plt4.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt4.set_ylim(0, 50)
plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 10, color='orange', edgecolor='black', linewidth=0.5)
plt5.set_xlabel("Number of RotatableBonds", fontsize=16, fontweight='bold')
plt5.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt5.set_ylim(0, 50)
plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
21/289:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 10, color='blue', edgecolor='black', linewidth=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 10, color='red', edgecolor='black', linewidth=0.5)
plt2.set_xlabel("LogP", fontsize=16, fontweight='bold')
plt2.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt2.set_ylim(0, 50)
plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)


#Histogram for NumHDonors
plt3.hist(data['NumHDonors'], density=False, bins= 10, color='yellow', edgecolor='black', linewidth=0.5)
plt3.set_xlabel("Number of Hydrogen Bond Donors", fontsize=16, fontweight='bold')
plt3.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt3.set_ylim(0, 50)
plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 10, color='green', edgecolor='black', linewidth=0.5)
plt4.set_xlabel("Number of Hydrogen Bond Acceptors", fontsize=16, fontweight='bold')
plt4.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt4.set_ylim(0, 50)
plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 10, color='orange', edgecolor='black', linewidth=0.5)
plt5.set_xlabel("Number of RotatableBonds", fontsize=16, fontweight='bold')
plt5.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt5.set_ylim(0, 50)
plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
plt.savefig('Result/LAT1 plots of the descriptors.pdf', dpi=300)
21/290:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 10, color='blue', edgecolor='black', linewidth=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 10, color='red', edgecolor='black', linewidth=0.5)
plt2.set_xlabel("LogP", fontsize=16, fontweight='bold')
plt2.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt2.set_ylim(0, 50)
plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)


#Histogram for NumHDonors
plt3.hist(data['NumHDonors'], density=False, bins= 10, color='yellow', edgecolor='black', linewidth=0.5)
plt3.set_xlabel("Number of Hydrogen Bond Donors", fontsize=16, fontweight='bold')
plt3.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt3.set_ylim(0, 50)
plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 10, color='green', edgecolor='black', linewidth=0.5)
plt4.set_xlabel("Number of Hydrogen Bond Acceptors", fontsize=16, fontweight='bold')
plt4.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt4.set_ylim(0, 50)
plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 10, color='orange', edgecolor='black', linewidth=0.5)
plt5.set_xlabel("Number of RotatableBonds", fontsize=16, fontweight='bold')
plt5.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt5.set_ylim(0, 50)
plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
plt.savefig('LAT1 plots of the descriptors.pdf', dpi=300)
21/291:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 10, color='blue', edgecolor='black', linewidth=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 10, color='red', edgecolor='black', linewidth=0.5)
plt2.set_xlabel("LogP", fontsize=16, fontweight='bold')
plt2.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)
plt2.set_ylim(0, 50)
plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)


#Histogram for NumHDonors
plt3.hist(data['NumHDonors'], density=False, bins= 10, color='yellow', edgecolor='black', linewidth=0.5)
plt3.set_xlabel("Number of Hydrogen Bond Donors", fontsize=16, fontweight='bold')
plt3.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)
plt3.set_ylim(0, 50)
plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 10, color='green', edgecolor='black', linewidth=0.5)
plt4.set_xlabel("Number of Hydrogen Bond Acceptors", fontsize=16, fontweight='bold')
plt4.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)
plt4.set_ylim(0, 50)
plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 10, color='orange', edgecolor='black', linewidth=0.5)
plt5.set_xlabel("Number of RotatableBonds", fontsize=16, fontweight='bold')
plt5.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)
plt5.set_ylim(0, 50)
plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#save file
plt.savefig('LAT1 plots of the descriptors.pdf', dpi=300)
21/292:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 10, color='3EC1D3', edgecolor='black', linewidth=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 10, color='red', edgecolor='black', linewidth=0.5)
plt2.set_xlabel("LogP", fontsize=16, fontweight='bold')
plt2.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)
plt2.set_ylim(0, 50)
plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)


#Histogram for NumHDonors
plt3.hist(data['NumHDonors'], density=False, bins= 10, color='yellow', edgecolor='black', linewidth=0.5)
plt3.set_xlabel("Number of Hydrogen Bond Donors", fontsize=16, fontweight='bold')
plt3.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)
plt3.set_ylim(0, 50)
plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 10, color='green', edgecolor='black', linewidth=0.5)
plt4.set_xlabel("Number of Hydrogen Bond Acceptors", fontsize=16, fontweight='bold')
plt4.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)
plt4.set_ylim(0, 50)
plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 10, color='orange', edgecolor='black', linewidth=0.5)
plt5.set_xlabel("Number of RotatableBonds", fontsize=16, fontweight='bold')
plt5.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)
plt5.set_ylim(0, 50)
plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#save file
plt.savefig('LAT1 plots of the descriptors.pdf', dpi=300)
21/293:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 10, color='#3EC1D3', edgecolor='black', linewidth=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 10, color='red', edgecolor='black', linewidth=0.5)
plt2.set_xlabel("LogP", fontsize=16, fontweight='bold')
plt2.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)
plt2.set_ylim(0, 50)
plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)


#Histogram for NumHDonors
plt3.hist(data['NumHDonors'], density=False, bins= 10, color='yellow', edgecolor='black', linewidth=0.5)
plt3.set_xlabel("Number of Hydrogen Bond Donors", fontsize=16, fontweight='bold')
plt3.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)
plt3.set_ylim(0, 50)
plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 10, color='green', edgecolor='black', linewidth=0.5)
plt4.set_xlabel("Number of Hydrogen Bond Acceptors", fontsize=16, fontweight='bold')
plt4.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)
plt4.set_ylim(0, 50)
plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 10, color='orange', edgecolor='black', linewidth=0.5)
plt5.set_xlabel("Number of RotatableBonds", fontsize=16, fontweight='bold')
plt5.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)
plt5.set_ylim(0, 50)
plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#save file
plt.savefig('LAT1 plots of the descriptors.pdf', dpi=300)
21/294:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 10, color='#3EC1D3', edgecolor='black', linewidth=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 10, color='#F6F7D7', edgecolor='black', linewidth=0.5)
plt2.set_xlabel("LogP", fontsize=16, fontweight='bold')
plt2.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)
plt2.set_ylim(0, 50)
plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)


#Histogram for NumHDonors
plt3.hist(data['NumHDonors'], density=False, bins= 10, color='#FF9A00', edgecolor='black', linewidth=0.5)
plt3.set_xlabel("Number of Hydrogen Bond Donors", fontsize=16, fontweight='bold')
plt3.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)
plt3.set_ylim(0, 50)
plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 10, color='#FF165D', edgecolor='black', linewidth=0.5)
plt4.set_xlabel("Number of Hydrogen Bond Acceptors", fontsize=16, fontweight='bold')
plt4.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)
plt4.set_ylim(0, 50)
plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 10, color='#52006A', edgecolor='black', linewidth=0.5)
plt5.set_xlabel("Number of RotatableBonds", fontsize=16, fontweight='bold')
plt5.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)
plt5.set_ylim(0, 50)
plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#save file
plt.savefig('LAT1 plots of the descriptors.pdf', dpi=300)
21/295:
import seaborn as sns

# visualize the relationship between the features and the response using scatterplots
sns.pairplot(RO5, x_vars=['MW','LogP','nHAcc','nHDon'], y_vars='value', size=7, aspect=0.7)
21/296: pip install seaborn
21/297: pip install seaborn[all]
21/298: import seaborn as sns
21/299:
import seaborn as sns

# visualize the relationship between the features and the response using scatterplots
sns.pairplot(RO5, x_vars=['MW','LogP','nHAcc','nHDon'], y_vars='pIC50', size=7, aspect=0.7)
21/300:
import seaborn as sns

# visualize the relationship between the features and the response using scatterplots
sns.pairplot(data, x_vars=['MW','LogP','nHAcc','nHDon'], y_vars='pIC50', size=7, aspect=0.7)
21/301:
import seaborn as sns

# visualize the relationship between the features and the response using scatterplots
sns.pairplot(data, x_vars=['MW','LogP','NumHDonors','Number of Hydrogen Bond Acceptors','NumRotatableBonds'], y_vars='pIC50', size=7, aspect=0.7)
21/302:
import seaborn as sns

# visualize the relationship between the features and the response using scatterplots
sns.pairplot(data, x_vars=['MW','LogP','NumHDonors','NumHAcceptors','NumRotatableBonds'], y_vars='pIC50', size=7, aspect=0.7)
21/303:
import seaborn as sns

# visualize the relationship between the features and the response using scatterplots
sns.pairplot(data, x_vars=['MW','LogP','NumHDonors','NumHAcceptors','NumRotatableBonds'], y_vars='pIC50', height=7, aspect=0.7)
21/304:
#Visuallize pIC50
plt.hist(data['pIC50']
21/305:
#Visuallize pIC50
plt.hist(data['pIC50'], density=False, bins= 10)
21/306:
#Visuallize pIC50
plt.hist(data['pIC50'], density=False, bins= 10, color=#FF7600)
21/307:
#Visuallize pIC50
plt.hist(data['pIC50'], density=False, bins= 10, color='#FF7600')
21/308:
#Visuallize pIC50
plt.hist(data['pIC50'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.set_xlabel('pIC50', fontsize=16, fontweight='bold')
plt.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.set_ylim(0, 50)
#plt1.grid(True)
plt.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
21/309:
#Visuallize pIC50
plt.hist(data['pIC50'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('pIC50', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.set_ylim(0, 50)
#plt1.grid(True)
plt.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
21/310:
#Visuallize pIC50
plt.hist(data['pIC50'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('pIC50', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 50)
#plt1.grid(True)
plt.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
21/311:
#Visuallize pIC50
plt.hist(data['pIC50'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('pIC50', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 50)
#plt1.grid(True)
plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
21/312:
#Visuallize pIC50
plt.hist(data['pIC50'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('pIC50', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 50)
#plt1.grid(True)
plt.axvline(5,6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
21/313:
#Visuallize pIC50
plt.hist(data['pIC50'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('pIC50', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 50)
#plt1.grid(True)
plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
21/314:
#Visuallize pIC50
plt.hist(data['pIC50'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('pIC50', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 50)
#plt1.grid(True)
plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
21/315:
#Visuallize pIC50
plt.hist(data['pIC50'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('pIC50', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 50)
#plt1.grid(True)
plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5, lab=inactive)
plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
21/316:
#Visuallize pIC50
plt.hist(data['pIC50'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('pIC50', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 50)
#plt1.grid(True)
plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5, lab='inactive')
plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
21/317:
#Visuallize pIC50
plt.hist(data['pIC50'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('pIC50', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 50)
#plt1.grid(True)
plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
plt.text(x, 5, 'hello', transform=trans
21/318:
#Visuallize pIC50
plt.hist(data['pIC50'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('pIC50', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 50)
#plt1.grid(True)
plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
plt.text(x, 5, 'hello', transform=trans)
21/319:
#Visuallize pIC50
plt.hist(data['pIC50'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('pIC50', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 50)
#plt1.grid(True)
plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
plt.text(5, 'hello', transform=trans)
21/320:
#Visuallize pIC50
plt.hist(data['pIC50'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('pIC50', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 50)
#plt1.grid(True)
plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
plt.text(5, 'hello', transform='trans')
21/321:
#Visuallize pIC50
plt.hist(data['pIC50'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('pIC50', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 50)
#plt1.grid(True)
plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
plt.texts(5, 'hello', transform='trans')
21/322:
#Visuallize pIC50
plt.hist(data['pIC50'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('pIC50', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 50)
#plt1.grid(True)
plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
21/323:
#Visuallize pIC50
plt.hist(data['pIC50'], density=False, bins= 20, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('pIC50', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 50)
#plt1.grid(True)
plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
21/324:
#Visuallize pIC50
plt.hist(data['pIC50'], density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('pIC50', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 50)
#plt1.grid(True)
plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
21/325:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 10, color='#F6F7D7', edgecolor='black', linewidth=0.5)
plt2.set_xlabel("LogP", fontsize=16, fontweight='bold')
plt2.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)
plt2.set_ylim(0, 50)
plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)


#Histogram for NumHDonors
plt3.hist(data['NumHDonors'], density=False, bins= 10, color='#FF9A00', edgecolor='black', linewidth=0.5)
plt3.set_xlabel("Number of Hydrogen Bond Donors", fontsize=16, fontweight='bold')
plt3.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)
plt3.set_ylim(0, 50)
plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 10, color='#FF165D', edgecolor='black', linewidth=0.5)
plt4.set_xlabel("Number of Hydrogen Bond Acceptors", fontsize=16, fontweight='bold')
plt4.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)
plt4.set_ylim(0, 50)
plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 10, color='#52006A', edgecolor='black', linewidth=0.5)
plt5.set_xlabel("Number of RotatableBonds", fontsize=16, fontweight='bold')
plt5.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)
plt5.set_ylim(0, 50)
plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#save file
plt.savefig('LAT1 plots of the descriptors.pdf', dpi=300)
21/326:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)
plt2.set_xlabel("LogP", fontsize=16, fontweight='bold')
plt2.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)
plt2.set_ylim(0, 50)
plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)


#Histogram for NumHDonors
plt3.hist(data['NumHDonors'], density=False, bins= 30, color='#FF9A00', edgecolor='black', linewidth=0.5)
plt3.set_xlabel("Number of Hydrogen Bond Donors", fontsize=16, fontweight='bold')
plt3.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)
plt3.set_ylim(0, 50)
plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 30, color='#FF165D', edgecolor='black', linewidth=0.5)
plt4.set_xlabel("Number of Hydrogen Bond Acceptors", fontsize=16, fontweight='bold')
plt4.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)
plt4.set_ylim(0, 50)
plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 30, color='#52006A', edgecolor='black', linewidth=0.5)
plt5.set_xlabel("Number of RotatableBonds", fontsize=16, fontweight='bold')
plt5.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)
plt5.set_ylim(0, 50)
plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#save file
plt.savefig('LAT1 plots of the descriptors.pdf', dpi=300)
21/327:
#Visuallize pIC50
plt.hist(data['pIC50'], density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('pIC50', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 50)
#plt1.grid(True)
plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

plt.savefig('LAT1 plots-pIC50 distribution.pdf', dpi=300)
21/328:
#Visuallize pIC50
plt.hist(data['pIC50'], density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('pIC50', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 50)
#plt1.grid(True)
plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

fig.tight_layout()
plt.savefig('LAT1 plots-pIC50 distribution.pdf', dpi=300)
21/329:
#Visuallize pIC50
plt.hist(data['pIC50'], density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('pIC50', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 50)
#plt1.grid(True)
plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

plt.tight_layout()
plt.savefig('LAT1 plots-pIC50 distribution.pdf', dpi=300)
21/330:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)
plt2.set_xlabel("LogP", fontsize=16, fontweight='bold')
plt2.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)
plt2.set_ylim(0, 50)
plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)


#Histogram for NumHDonors
plt3.hist(data['NumHDonors'], density=False, bins= 30, color='#FF9A00', edgecolor='black', linewidth=0.5)
plt3.set_xlabel("Number of Hydrogen Bond Donors", fontsize=16, fontweight='bold')
plt3.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)
plt3.set_ylim(0, 50)
plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 30, color='#FF165D', edgecolor='black', linewidth=0.5)
plt4.set_xlabel("Number of Hydrogen Bond Acceptors", fontsize=16, fontweight='bold')
plt4.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)
plt4.set_ylim(0, 50)
plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 30, color='#52006A', edgecolor='black', linewidth=0.5)
plt5.set_xlabel("Number of RotatableBonds", fontsize=16, fontweight='bold')
plt5.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)
plt5.set_ylim(0, 50)
plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#save file
plt.tight_layout()
plt.savefig('LAT1 plots-basic descriptors.pdf', dpi=300)
21/331:
#Visuallize pIC50
plt.hist(data['pIC50'], density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('pIC50', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 50)
#plt1.grid(True)
plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#save file
plt.tight_layout()
plt.savefig('LAT1 plots-pIC50 distribution.pdf', dpi=300)
24/1: ## Objective
28/1:
#How many of molecules are active against LAT1
plt.scatter(data['pIC50'], density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)
28/2:
#import library
import pandas as pd
28/3:
#Input yourname for this lab notebook
labname = input('what is your name?')
28/4: print(labname)
28/5:
#We will use Chembl database for molecular modeling
#install package dependencies
pip install requests_cache==0.6.4
28/6:
#We will use Chembl database for molecular modeling
#install package dependencies
pip install requests_cache==0.6.4
28/7:
#We will use Chembl database for molecular modeling
#install package dependencies
pip install requests_cache==0.6.4
28/8:
#We will use Chembl database for molecular modeling
#install package dependencies
!pip install requests_cache==0.6.4
28/9:
#Install Chemabl database
pip install chembl_webresource_client
28/10:
#Install Chemabl database
!pip install chembl_webresource_client
28/11:
#How many of molecules are active against LAT1
plt.scatter(data['pIC50'], density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)
28/12:
import numpy as np
import rdkit
from time import time

from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
28/13:
#How many of molecules are active against LAT1
plt.scatter(data['pIC50'], density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)
28/14:
#import library
import pandas as pd
28/15:
#Input yourname for this lab notebook
labname = input('what is your name?')
28/16: print(labname)
28/17:
#We will use Chembl database for molecular modeling
#install package dependencies
!pip install requests_cache==0.6.4
28/18:
#Install Chemabl database
!pip install chembl_webresource_client
28/19:
#Chembl API access...
from chembl_webresource_client.new_client import new_client
target = new_client.target
#Search target protein, we will use LAT1 as a target protein
target_query = target.search('LAT1')
targets = pd.DataFrame.from_dict(target_query)
targets
28/20:
#As we can see, we will select H.sapiens LAT1 as atarget
selected_target = targets.target_chembl_id[1]
selected_target
28/21:
#Here we will access the activity of compounds in this target, first we will try IC50
#IC50 is tell us about the potency of the compound that can reduce LAT1 activity for 50%
activity = new_client.activity
res = activity.filter(target_chembl_id=selected_target).filter(standard_type="IC50")
28/22:
#show dataframe of IC50
df = pd.DataFrame.from_dict(res)
df
28/23:
#if any compound is missing the IC50 value, then it should drop out
df2 = df[df.standard_value.notna()]
df2
28/24:
#I want to know what data do we have
print(df2.columns)
28/25:
#we want to see the molecule structure via canonical_smiles
df2.canonical_smiles
28/26:
#We want to know the structure and activity
selection = ['molecule_chembl_id','canonical_smiles','standard_units', 'standard_value']
df3 = df2[selection]
df3
28/27:
df4 = df3[df.canonical_smiles.notna()]
df4
28/28:
#install rdkit
pip install rdkit-pypi
28/29:
#install rdkit
!pip install rdkit-pypi
28/30:
import numpy as np
import rdkit
from time import time

from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
28/31:
#We are going to calculate Lipinski rule of five
from rdkit import Chem
from rdkit.Chem import Descriptors, Lipinski
28/32: print(df4['canonical_smiles'].unique())
28/33:
import pandas as pd
def lipinski(smiles, verbose=False):

    moldata= []
    for elem in smiles:
        mol=Chem.MolFromSmiles(elem) 
        moldata.append(mol)
       
    baseData= np.arange(1,1)
    i=0  
    for mol in moldata:        
       
        desc_MolWt = Descriptors.MolWt(mol)
        desc_MolLogP = Descriptors.MolLogP(mol)
        desc_NumHDonors = Lipinski.NumHDonors(mol)
        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)
        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)
           
        row = np.array([desc_MolWt,
                        desc_MolLogP,
                        desc_NumHDonors,
                        desc_NumHAcceptors,
                        desc_NumRotatableBonds])   
    
        if(i==0):
            baseData=row
        else:
            baseData=np.vstack([baseData, row])
        i=i+1      
    
    columnNames=["MW","LogP","NumHDonors","NumHAcceptors", "NumRotatableBonds"]   
    descriptors = pd.DataFrame(data=baseData,columns=columnNames)
    
    return descriptors
28/34:
df_lipinski = lipinski(df4.canonical_smiles)
df_lipinski
df_lipinski
28/35:
#combine results
df_combined = pd.concat([df3,df_lipinski], axis=1)
df_combined
28/36:
#remove na
final_df_combined = df_combined.replace(['None','NONE','none'],np.nan)
final_df_combined = final_df_combined.dropna()
print('Unique value in MW column',final_df_combined['MW'].unique())
len(final_df_combined)
28/37:
#We want to know how many of them is high potency agaisnt LAT1 protein
#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution
#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()
# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 10−9
# Take the molar value and apply -log10
# Delete the standard_value column and create a new pIC50 column

#before we do that ,we need to install nescessery package

import numpy as np

def pIC50(input):
    pIC50 = []

    for i in input['standard_value_norm']:
        molar = float(i)*(10**-9) # Converts nM to M
        pIC50.append(-np.log10(molar))

    input['pIC50'] = pIC50
    x = input.drop('standard_value_norm', 1)
        
    return x
28/38: final_df_combined.standard_value.describe()
28/39:
def norm_value(input):
    norm = []

    for i in input['standard_value']:
        if float(i) > 100000000:
          i = 100000000
        norm.append(i)

    input['standard_value_norm'] = norm
    x = input.drop('standard_value', 1)
        
    return x
28/40:
df_norm = norm_value(final_df_combined)
df_norm
28/41: df_norm.standard_value_norm.describe()
28/42:
df_final = pIC50(df_norm)
df_final
28/43: -np.log10(0.000001)
28/44: -np.log10(0.00001)
28/45: -np.log10(0.00001)
28/46:
bioactivity_class = []
for i in df_final.pIC50:
  if float(i) >= 6:
    bioactivity_class.append("inactive")
  elif float(i) <= 5:
    bioactivity_class.append("active")
  else:
    bioactivity_class.append("intermediate")
28/47: len(bioactivity_class)
28/48: bioactivity_class
28/49: len(df_final)
28/50:
df_class = pd.DataFrame(bioactivity_class)
df_class
df_class.to_csv('df_class.csv')
28/51: df_class
28/52: df_final.to_csv('df_final.csv')
28/53:
selection = ['molecule_chembl_id', 'canonical_smiles', 'MW', 'LogP','NumHDonors',
                      'NumHAcceptors', 'NumRotatableBonds', 'pIC50']
df_final_3 = df_final[selection]
df_final_3
28/54:
df_final_4 = pd.concat([df_final_3, df_class], axis =1)
df_final_2
28/55:
df_final_4 = pd.concat([df_final_3, df_class], axis =1)
df_final_4
28/56: print(df_final_4['bioactivity_class'].unique())
28/57: print(df_final_4['bioactivity_class'].unique())
28/58: print(df_final_2['bioactivity_class'].unique())
28/59:
#Here we got RO5 and pIC50 of 71 cpds agaisnt LAT1 protein

data = pd.read_csv('df_final.csv')
28/60:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)
plt2.set_xlabel("LogP", fontsize=16, fontweight='bold')
plt2.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)
plt2.set_ylim(0, 50)
plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)


#Histogram for NumHDonors
plt3.hist(data['NumHDonors'], density=False, bins= 30, color='#FF9A00', edgecolor='black', linewidth=0.5)
plt3.set_xlabel("Number of Hydrogen Bond Donors", fontsize=16, fontweight='bold')
plt3.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)
plt3.set_ylim(0, 50)
plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 30, color='#FF165D', edgecolor='black', linewidth=0.5)
plt4.set_xlabel("Number of Hydrogen Bond Acceptors", fontsize=16, fontweight='bold')
plt4.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)
plt4.set_ylim(0, 50)
plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 30, color='#52006A', edgecolor='black', linewidth=0.5)
plt5.set_xlabel("Number of RotatableBonds", fontsize=16, fontweight='bold')
plt5.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)
plt5.set_ylim(0, 50)
plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#save file
plt.tight_layout()
plt.savefig('LAT1 plots-basic descriptors.pdf', dpi=300)
28/61:
#Visuallize pIC50
plt.hist(data['pIC50'], density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('pIC50', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 50)
#plt1.grid(True)
plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#save file
plt.tight_layout()
plt.savefig('LAT1 plots-pIC50 distribution.pdf', dpi=300)
28/62:
#Install statistic seabon
pip install seaborn
28/63:
import seaborn as sns

# visualize the relationship between the features and the response using scatterplots
sns.pairplot(data, x_vars=['MW','LogP','NumHDonors','NumHAcceptors','NumRotatableBonds'], y_vars='pIC50', height=7, aspect=0.7)
28/64:
#How many of molecules are active against LAT1
plt.scatter(data['pIC50'], density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)
28/65:
#How many of molecules are active against LAT1
plt.scatter(data['pIC50', 'class'])
28/66: print(data)
28/67: df.data
28/68: df_data
28/69:
df_final_4 = pd.concat([df_final_3, df_class], axis =1)
df_final_4
28/70:
df_class = pd.DataFrame(bioactivity_class, header = True)
df_class
df_class.to_csv('df_class.csv')
28/71:
df_class = pd.DataFrame(bioactivity_class, head = True)
df_class
df_class.to_csv('df_class.csv')
28/72:
df_class = pd.DataFrame(bioactivity_class, columns="bioactivity_class")
df_class
df_class.to_csv('df_class.csv')
28/73:
df_class = pd.DataFrame(bioactivity_class, name="bioactivity_class")
df_class
df_class.to_csv('df_class.csv')
28/74:
df_class = pd.DataFrame(bioactivity_class, columns=['bioactivity_class'])
df_class
df_class.to_csv('df_class.csv')
28/75: df_class
28/76: df_final.to_csv('df_final.csv')
28/77:
selection = ['molecule_chembl_id', 'canonical_smiles', 'MW', 'LogP','NumHDonors',
                      'NumHAcceptors', 'NumRotatableBonds', 'pIC50']
df_final_3 = df_final[selection]
df_final_3
28/78:
df_final_4 = pd.concat([df_final_3, df_class], axis =1)
df_final_4
28/79: print(df_final_2['bioactivity_class'].unique())
28/80: print(df_final_4['bioactivity_class'].unique())
28/81: df_final_2 = read.csv("df_final_adjusted")
28/82: df_final_2 = pd.read.csv("df_final_adjusted")
28/83: df_final_2 = pd.read_csv("df_final_adjusted")
28/84: df_final_2 = pd.read_csv("df_final_adjusted.csv")
28/85:
df_final_2 = pd.read_csv("df_final_adjusted.csv")
df_final_2
28/86:
selection = ['molecule_chembl_id', 'canonical_smiles', 'MW', 'LogP','NumHDonors',
                      'NumHAcceptors', 'NumRotatableBonds', 'pIC50']
df_final_3 = df_final[selection]
df_final_3
28/87:
df_final_4 = pd.concat([df_final_3, df_class], axis =1)
df_final_4
28/88:
selection = ['molecule_chembl_id', 'canonical_smiles', 'MW', 'LogP','NumHDonors',
                      'NumHAcceptors', 'NumRotatableBonds', 'pIC50']
df_final_3 = df_final_adjiusted[selection]
df_final_3
28/89:
selection = ['molecule_chembl_id', 'canonical_smiles', 'MW', 'LogP','NumHDonors',
                      'NumHAcceptors', 'NumRotatableBonds', 'pIC50']
df_final_3 = df_final_2[selection]
df_final_3
28/90:
df_final_4 = pd.concat([df_final_3, df_class], axis =1)
df_final_4
28/91: print(df_final_4['bioactivity_class'].unique())
28/92:
df_final_4 = pd.concat([df_final_3, df_class], axis =1)
df_final_4
df_final_4.to_csv('df_final_4.csv')
28/93: print(df_final_4['bioactivity_class'].unique())
28/94:
#Here we got RO5 and pIC50 of 71 cpds agaisnt LAT1 protein

data = pd.read_csv('df_final_4.csv')
28/95:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)
plt2.set_xlabel("LogP", fontsize=16, fontweight='bold')
plt2.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)
plt2.set_ylim(0, 50)
plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)


#Histogram for NumHDonors
plt3.hist(data['NumHDonors'], density=False, bins= 30, color='#FF9A00', edgecolor='black', linewidth=0.5)
plt3.set_xlabel("Number of Hydrogen Bond Donors", fontsize=16, fontweight='bold')
plt3.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)
plt3.set_ylim(0, 50)
plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 30, color='#FF165D', edgecolor='black', linewidth=0.5)
plt4.set_xlabel("Number of Hydrogen Bond Acceptors", fontsize=16, fontweight='bold')
plt4.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)
plt4.set_ylim(0, 50)
plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 30, color='#52006A', edgecolor='black', linewidth=0.5)
plt5.set_xlabel("Number of RotatableBonds", fontsize=16, fontweight='bold')
plt5.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)
plt5.set_ylim(0, 50)
plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#save file
plt.tight_layout()
plt.savefig('LAT1 plots-basic descriptors.pdf', dpi=300)
28/96:
#Visuallize pIC50
plt.hist(data['pIC50'], density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('pIC50', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 50)
#plt1.grid(True)
plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#save file
plt.tight_layout()
plt.savefig('LAT1 plots-pIC50 distribution.pdf', dpi=300)
28/97:
#Install statistic seabon
!pip install seaborn
28/98:
import seaborn as sns

# visualize the relationship between the features and the response using scatterplots
sns.pairplot(data, x_vars=['MW','LogP','NumHDonors','NumHAcceptors','NumRotatableBonds'], y_vars='pIC50', height=7, aspect=0.7)
28/99:
#How many of molecules are active against LAT1
plt.scatter(data['pIC50', 'bioactivity_class'])
28/100:
#How many of molecules are active against LAT1
plt.bar(data['pIC50', 'bioactivity_class'])
28/101:
#How many of molecules are active against LAT1
plt.bar('pIC50', 'bioactivity_class')
28/102:
#How many of molecules are active against LAT1
plt.bar(data['pIC50', 'bioactivity_class'])
28/103:
#How many of molecules are active against LAT1
plt.bar(hist['pIC50', 'bioactivity_class'])
28/104:
#How many of molecules are active against LAT1
plt.bar(his['pIC50', 'bioactivity_class'])
28/105:
#How many of molecules are active against LAT1
plt.hist(data['pIC50', 'bioactivity_class'])
28/106:
#How many of molecules are active against LAT1
plt.scatter(data['pIC50', 'bioactivity_class'])
28/107:
#How many of molecules are active against LAT1
plt.hist(data['bioactivity_class'])
28/108:
#How many of molecules are active against LAT1
plt.hist(data['bioactivity_class'] , density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('pIC50', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
28/109:
#How many of molecules are active against LAT1
plt.hist(data['bioactivity_class'] , density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('pIC50', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 100)
28/110: count(data['bioactivity_class'])
28/111: data.count(bioactivity_class)
28/112: data.count("bioactivity_class")
28/113: data
28/114: data.bioactivity_class.describe()
28/115: data.bioactivity_class(count)
28/116: data.bioactivity_class.count()
28/117: data.bioactivity_class.inactive.count()
28/118: data.bioactivity_class.inactive()
28/119: data.bioactivity_class.inactive.count()
28/120: data.inactive.bioactivity_class.count()
28/121: ply.scatter(data['bioactivity_class'])
28/122: ply.scatter(data['pIC50', 'MW'])
28/123: plt.scatter(data['pIC50', 'MW'])
28/124: plt.scatter('pIC50', 'MW')
28/125: plt.scatter(x='pIC50', y='MW')
28/126: df.plt.scatter(x='pIC50', y='MW')
28/127: df.plot.scatter(x='pIC50', y='MW')
28/128: df.plt.scatter(x='pIC50', y='MW')
28/129: plt.scatter(x='pIC50', y='MW')
28/130: sns.pairplot(data, x_vars=['Bioactivity_class'], y_vars='pIC50', height=7, aspect=0.7)
28/131: sns.pairplot(data, x_vars=['bioactivity_class'], y_vars='pIC50', height=7, aspect=0.7)
28/132:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)
28/133:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)


x = data['MW']
y = data['pIC50']
colors = data['bioactivity_class']
area = (30 * data['pIC50'])**2  # 0 to 15 point radii

plt.scatter(x, y, s=area, c=colors, alpha=0.5)
plt.show()
28/134:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)


x = data['MW']
y = data['pIC50']
colors = data['bioactivity_class']
area = (30 * data['pIC50'])**2  # 0 to 15 point radii

plt.scatter(x, y, s=area,  alpha=0.5)
plt.show()
28/135:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)


x = data['MW']
y = data['pIC50']
colors = data['bioactivity_class']
area = (30 * data['pIC50'])  # 0 to 15 point radii

plt.scatter(x, y, s=area,  alpha=0.5)
plt.show()
28/136:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)


x = data['MW']
y = data['pIC50']
colors = data['bioactivity_class']
area = (30 * data['pIC50'])  # 0 to 15 point radii

plt.scatter(x, y, s=area, c=colors, alpha=0.5)
plt.show()
28/137:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)


x = data['MW']
y = data['pIC50']
colors = ['active', 'inactive', 'intermediate']
area = (30 * data['pIC50'])  # 0 to 15 point radii

plt.scatter(x, y, s=area, c=colors, alpha=0.5)
plt.show()
28/138:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)


x = data['MW']
y = data['pIC50']
colors = ['active', 'inactive', 'intermediate']
area = (30 * data['pIC50'])  # 0 to 15 point radii

plt.scatter(x, y, s=area, c=df[colors], alpha=0.5)
plt.show()
28/139:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)


x = data['MW']
y = data['pIC50']
colors = data['active', 'inactive', 'intermediate']
area = (30 * data['pIC50'])  # 0 to 15 point radii

plt.scatter(x, y, s=area, c=df[colors], alpha=0.5)
plt.show()
28/140:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)


x = data['MW']
y = data['pIC50']
colors = data['active', 'inactive', 'intermediate']
area = (30 * data['pIC50'])  # 0 to 15 point radii

plt.scatter(x, y, s=area, c=df[colors], alpha=0.5)
plt.show()
28/141:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)


x = data['MW']
y = data['pIC50']
colors = data['active', 'inactive', 'intermediate']
area = (30 * data['pIC50'])  # 0 to 15 point radii

plt.scatter(x, y, s=area, c=colors, alpha=0.5)
plt.show()
28/142: sns.lmplot('pIC50', 'MW', data=data, hue='bioactivity_class', fit_reg=False)
28/143: sns.lmplot('MW', 'pIC50', data=data, hue='bioactivity_class', fit_reg=False)
28/144:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)


x = data['MW']
y = data['pIC50']
colors = data['active', 'inactive', 'intermediate']
area = (30 * data['pIC50'])  # 0 to 15 point radii

plt.scatter(x, y, s=area, alpha=0.5)
plt.show()
28/145:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)


x = data['MW']
y = data['pIC50']
area = (30 * data['pIC50'])  # 0 to 15 point radii

plt.scatter(x, y, s=area, alpha=0.5)
plt.show()
28/146:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt




x = data['MW']
y = data['pIC50']
area = (30 * data['pIC50'])  # 0 to 15 point radii

plt.scatter(x, y, s=area, alpha=0.5)
plt.show()
28/147: sns.lmplot('MW', 'pIC50', data=data, hue='bioactivity_class', size='pIC50' fit_reg=False)
28/148: sns.lmplot('MW', 'pIC50', data=data, hue='bioactivity_class', size='pIC50', fit_reg=False)
28/149: sns.lmplot('MW', 'pIC50', data=data, hue='bioactivity_class', area='pIC50', fit_reg=False)
28/150: sns.lmplot('MW', 'pIC50', data=data, hue='bioactivity_class', size='pIC50', fit_reg=False)
28/151: sns.lmplot('MW', 'pIC50', data=data, hue='bioactivity_class', size='pIC50', )
28/152: sns.lmplot('MW', 'pIC50', data=data, hue='bioactivity_class', size='LogP', fit_reg=False)
28/153: sns.lmplot('MW', 'pIC50', data=data, hue='bioactivity_class', fit_reg=False)
28/154:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt




x = data['MW']
y = data['pIC50']
area = (30 * data['pIC50'])  # 0 to 15 point radii

plt.scatter(x, y, s=area, alpha=0.5)
plt.xlabel('MW', fontsize=16, fontweight='bold')
plt.ylabel("pIC50", fontsize=16, fontweight='bold')
plt.show()
28/155:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt




x = data['MW']
y = data['pIC50']
area = (30 * data['pIC50'])  # 0 to 15 point radii

plt.scatter(x, y, s=area, alpha=0.5)
plt.xlabel('MW', fontsize=16, fontweight='bold')
plt.ylabel("pIC50", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.show()
28/156:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt




x = data['MW']
y = data['pIC50']
area = (30 * data['pIC50'])  # 0 to 15 point radii

plt.scatter(x, y, s=area, alpha=0.5)
plt.xlabel('MW', fontsize=16, fontweight='bold')
plt.ylabel("pIC50", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.show()

#save file
plt.tight_layout()
plt.savefig('LAT1 plots-pIC50 distribution.pdf', dpi=300)
28/157:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt




x = data['MW']
y = data['pIC50']
area = (30 * data['LogP'])  # 0 to 15 point radii

plt.scatter(x, y, s=area, alpha=0.5)
plt.xlabel('MW', fontsize=16, fontweight='bold')
plt.ylabel("pIC50", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.show()

#save file
plt.tight_layout()
plt.savefig('LAT1 plots-pIC50 distribution.pdf', dpi=300)
28/158:
#Visuallize pIC50
plt.hist(data['pIC50'], density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('pIC50', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 50)
#plt1.grid(True)
plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#save file
plt.tight_layout()
plt.savefig('LAT1 plots-pIC50 distribution.pdf', dpi=300)
28/159:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt




x = data['MW']
y = data['pIC50']
area = (30 * data['bioactivity_class'])  # 0 to 15 point radii

plt.scatter(x, y, s=area, alpha=0.5)
plt.xlabel('MW', fontsize=16, fontweight='bold')
plt.ylabel("pIC50", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.show()

#save file
plt.tight_layout()
plt.savefig('MW_and_pIC50.pdf', dpi=300)
28/160:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt




x = data['MW']
y = data['pIC50']
area = (30 * data['LogP'])  # 0 to 15 point radii

plt.scatter(x, y, s=area, c=data['bioactivity_class'] alpha=0.5)
plt.xlabel('MW', fontsize=16, fontweight='bold')
plt.ylabel("pIC50", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.show()

#save file
plt.tight_layout()
plt.savefig('MW_and_pIC50.pdf', dpi=300)
28/161:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt




x = data['MW']
y = data['pIC50']
area = (30 * data['LogP'])  # 0 to 15 point radii

plt.scatter(x, y, s=area, c=data['bioactivity_class'], alpha=0.5)
plt.xlabel('MW', fontsize=16, fontweight='bold')
plt.ylabel("pIC50", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.show()

#save file
plt.tight_layout()
plt.savefig('MW_and_pIC50.pdf', dpi=300)
28/162:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt




x = data['MW']
y = data['pIC50']
colors = data.groupby(data['bioactivity_class'])
area = (30 * data['LogP'])  # 0 to 15 point radii

plt.scatter(x, y, s=area, c=colors, alpha=0.5)
plt.xlabel('MW', fontsize=16, fontweight='bold')
plt.ylabel("pIC50", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.show()

#save file
plt.tight_layout()
plt.savefig('MW_and_pIC50.pdf', dpi=300)
28/163:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt




x = data['MW']
y = data['pIC50']
#colors = data.groupby(data['bioactivity_class'])
area = (30 * data['LogP'])  # 0 to 15 point radii

plt.scatter(x, y, s=area, , alpha=0.5)
plt.xlabel('MW', fontsize=16, fontweight='bold')
plt.ylabel("pIC50", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.show()

#save file
plt.tight_layout()
plt.savefig('MW_and_pIC50.pdf', dpi=300)
28/164:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt




x = data['MW']
y = data['pIC50']
#colors = data.groupby(data['bioactivity_class'])
area = (30 * data['LogP'])  # 0 to 15 point radii

plt.scatter(x, y, s=area, alpha=0.5)
plt.xlabel('MW', fontsize=16, fontweight='bold')
plt.ylabel("pIC50", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.show()

#save file
plt.tight_layout()
plt.savefig('MW_and_pIC50.pdf', dpi=300)
28/165:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt




x = data['MW']
y = data['pIC50']
#colors = data.groupby(data['bioactivity_class'])
area = (30 * data['LogP'])  # 0 to 15 point radii

plt.scatter(x, y, s=area, alpha=0.5)
plt.xlabel('MW', fontsize=16, fontweight='bold')
plt.ylabel("pIC50", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.show()

#save file
plt.tight_layout()
plt.savefig('MW_and_pIC50_LogP.pdf', dpi=300)
28/166:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt




x = data['MW']
y = data['pIC50']
#colors = data.groupby(data['bioactivity_class'])
area = (30 * data['LogP'])  # 0 to 15 point radii

plt.scatter(x, y, s=area, alpha=0.5)
plt.xlabel('MW', fontsize=16, fontweight='bold')
plt.ylabel("pIC50", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.legend(area)
plt.show()

#save file
plt.tight_layout()
plt.savefig('MW_and_pIC50_LogP.pdf', dpi=300)
28/167:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt




x = data['MW']
y = data['pIC50']
#colors = data.groupby(data['bioactivity_class'])
area = (30 * data['LogP'])  # 0 to 15 point radii

plt.scatter(x, y, s=area, alpha=0.5)
plt.xlabel('MW', fontsize=16, fontweight='bold')
plt.ylabel("pIC50", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.legend('area')
plt.show()

#save file
plt.tight_layout()
plt.savefig('MW_and_pIC50_LogP.pdf', dpi=300)
28/168:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt




x = data['MW']
y = data['pIC50']
#colors = data.groupby(data['bioactivity_class'])
area = (30 * data['LogP'])  # 0 to 15 point radii

plt.scatter(x, y, s=area, alpha=0.5)
plt.xlabel('MW', fontsize=16, fontweight='bold')
plt.ylabel("pIC50", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.legend('LogP')
plt.show()

#save file
plt.tight_layout()
plt.savefig('MW_and_pIC50_LogP.pdf', dpi=300)
28/169:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt




x = data['MW']
y = data['pIC50']
#colors = data.groupby(data['bioactivity_class'])
area = (30 * data['LogP'])  # 0 to 15 point radii

plt.scatter(x, y, s=area, alpha=0.5)
plt.xlabel('MW', fontsize=16, fontweight='bold')
plt.ylabel("pIC50", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.legend()
plt.show()

#save file
plt.tight_layout()
plt.savefig('MW_and_pIC50_LogP.pdf', dpi=300)
28/170:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt




x = data['MW']
y = data['pIC50']
#colors = data.groupby(data['bioactivity_class'])
area = (30 * data['LogP'])  # 0 to 15 point radii

plt.scatter(x, y, s=area, alpha=0.5)
plt.xlabel('MW', fontsize=16, fontweight='bold')
plt.ylabel("pIC50", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.legend('LogP', )
plt.show()

#save file
plt.tight_layout()
plt.savefig('MW_and_pIC50_LogP.pdf', dpi=300)
28/171:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt




x = data['MW']
y = data['pIC50']
#colors = data.groupby(data['bioactivity_class'])
area = (30 * data['LogP'])  # 0 to 15 point radii

plt.scatter(x, y, s=area, alpha=0.5)
plt.xlabel('MW', fontsize=16, fontweight='bold')
plt.ylabel("pIC50", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.legend('LogP',)
plt.show()

#save file
plt.tight_layout()
plt.savefig('MW_and_pIC50_LogP.pdf', dpi=300)
29/1: sns.lmplot('MW', 'pIC50', data=data, hue='bioactivity_class', fit_reg=False)
29/2:
#import library
import pandas as pd
29/3:
#Input yourname for this lab notebook
labname = input('what is your name?')
29/4: print(labname)
29/5:
#We will use Chembl database for molecular modeling
#install package dependencies
!pip install requests_cache==0.6.4
29/6:
#Install Chemabl database
!pip install chembl_webresource_client
29/7:
#Chembl API access...
from chembl_webresource_client.new_client import new_client
target = new_client.target
#Search target protein, we will use LAT1 as a target protein
target_query = target.search('LAT1')
targets = pd.DataFrame.from_dict(target_query)
targets
29/8:
#As we can see, we will select H.sapiens LAT1 as atarget
selected_target = targets.target_chembl_id[1]
selected_target
29/9:
#Here we will access the activity of compounds in this target, first we will try IC50
#IC50 is tell us about the potency of the compound that can reduce LAT1 activity for 50%
activity = new_client.activity
res = activity.filter(target_chembl_id=selected_target).filter(standard_type="IC50")
29/10:
#show dataframe of IC50
df = pd.DataFrame.from_dict(res)
df
29/11:
#if any compound is missing the IC50 value, then it should drop out
df2 = df[df.standard_value.notna()]
df2
29/12:
#I want to know what data do we have
print(df2.columns)
29/13:
#we want to see the molecule structure via canonical_smiles
df2.canonical_smiles
29/14:
#We want to know the structure and activity
selection = ['molecule_chembl_id','canonical_smiles','standard_units', 'standard_value']
df3 = df2[selection]
df3
29/15:
df4 = df3[df.canonical_smiles.notna()]
df4
29/16:
#install rdkit
!pip install rdkit-pypi
29/17:
import numpy as np
import rdkit
from time import time

from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
29/18:
#We are going to calculate Lipinski rule of five
from rdkit import Chem
from rdkit.Chem import Descriptors, Lipinski
29/19: print(df4['canonical_smiles'].unique())
29/20:
import pandas as pd
def lipinski(smiles, verbose=False):

    moldata= []
    for elem in smiles:
        mol=Chem.MolFromSmiles(elem) 
        moldata.append(mol)
       
    baseData= np.arange(1,1)
    i=0  
    for mol in moldata:        
       
        desc_MolWt = Descriptors.MolWt(mol)
        desc_MolLogP = Descriptors.MolLogP(mol)
        desc_NumHDonors = Lipinski.NumHDonors(mol)
        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)
        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)
           
        row = np.array([desc_MolWt,
                        desc_MolLogP,
                        desc_NumHDonors,
                        desc_NumHAcceptors,
                        desc_NumRotatableBonds])   
    
        if(i==0):
            baseData=row
        else:
            baseData=np.vstack([baseData, row])
        i=i+1      
    
    columnNames=["MW","LogP","NumHDonors","NumHAcceptors", "NumRotatableBonds"]   
    descriptors = pd.DataFrame(data=baseData,columns=columnNames)
    
    return descriptors
29/21:
df_lipinski = lipinski(df4.canonical_smiles)
df_lipinski
df_lipinski
29/22:
#combine results
df_combined = pd.concat([df3,df_lipinski], axis=1)
df_combined
29/23:
#remove na
final_df_combined = df_combined.replace(['None','NONE','none'],np.nan)
final_df_combined = final_df_combined.dropna()
print('Unique value in MW column',final_df_combined['MW'].unique())
len(final_df_combined)
29/24: final_df_combined
29/25:
#We want to know how many of them is high potency agaisnt LAT1 protein
#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution
#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()
# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 10−9
# Take the molar value and apply -log10
# Delete the standard_value column and create a new pIC50 column

#before we do that ,we need to install nescessery package

import numpy as np

def pIC50(input):
    pIC50 = []

    for i in input['standard_value_norm']:
        molar = float(i)*(10**-9) # Converts nM to M
        pIC50.append(-np.log10(molar))

    input['pIC50'] = pIC50
    x = input.drop('standard_value_norm', 1)
        
    return x
29/26: final_df_combined.standard_value.describe()
29/27:
def norm_value(input):
    norm = []

    for i in input['standard_value']:
        if float(i) > 100000000:
          i = 100000000
        norm.append(i)

    input['standard_value_norm'] = norm
    x = input.drop('standard_value', 1)
        
    return x
29/28:
df_norm = norm_value(final_df_combined)
df_norm
29/29: df_norm.standard_value_norm.describe()
29/30:
df_final = pIC50(df_norm)
df_final
29/31: -np.log10(0.000001)
29/32: -np.log10(0.00001)
29/33:
bioactivity_class = []
for i in df_final.pIC50:
  if float(i) >= 6:
    bioactivity_class.append("inactive")
  elif float(i) <= 5:
    bioactivity_class.append("active")
  else:
    bioactivity_class.append("intermediate")
29/34: len(bioactivity_class)
29/35: bioactivity_class
29/36: len(df_final)
29/37:
df_class = pd.DataFrame(bioactivity_class, columns=['bioactivity_class'])
df_class
df_class.to_csv('df_class.csv')
29/38: df_class
29/39: df_final.to_csv('df_final.csv')
29/40:
df_final_2 = pd.read_csv("df_final_adjusted.csv")
df_final_2
29/41:
selection = ['molecule_chembl_id', 'canonical_smiles', 'MW', 'LogP','NumHDonors',
                      'NumHAcceptors', 'NumRotatableBonds', 'pIC50']
df_final_3 = df_final_2[selection]
df_final_3
29/42:
df_final_4 = pd.concat([df_final_3, df_class], axis =1)
df_final_4
df_final_4.to_csv('df_final_4.csv')
29/43: print(df_final_4['bioactivity_class'].unique())
29/44:
#Here we got RO5 and pIC50 of 71 cpds agaisnt LAT1 protein

data = pd.read_csv('df_final_4.csv')
29/45:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)
plt2.set_xlabel("LogP", fontsize=16, fontweight='bold')
plt2.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)
plt2.set_ylim(0, 50)
plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)


#Histogram for NumHDonors
plt3.hist(data['NumHDonors'], density=False, bins= 30, color='#FF9A00', edgecolor='black', linewidth=0.5)
plt3.set_xlabel("Number of Hydrogen Bond Donors", fontsize=16, fontweight='bold')
plt3.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)
plt3.set_ylim(0, 50)
plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 30, color='#FF165D', edgecolor='black', linewidth=0.5)
plt4.set_xlabel("Number of Hydrogen Bond Acceptors", fontsize=16, fontweight='bold')
plt4.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)
plt4.set_ylim(0, 50)
plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 30, color='#52006A', edgecolor='black', linewidth=0.5)
plt5.set_xlabel("Number of RotatableBonds", fontsize=16, fontweight='bold')
plt5.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)
plt5.set_ylim(0, 50)
plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#save file
plt.tight_layout()
plt.savefig('LAT1 plots-basic descriptors.pdf', dpi=300)
29/46:
#Visuallize pIC50
plt.hist(data['pIC50'], density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('pIC50', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 50)
#plt1.grid(True)
plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#save file
plt.tight_layout()
plt.savefig('LAT1 plots-pIC50 distribution.pdf', dpi=300)
29/47:
#Install statistic seabon
!pip install seaborn
29/48:
import seaborn as sns

# visualize the relationship between the features and the response using scatterplots
sns.pairplot(data, x_vars=['MW','LogP','NumHDonors','NumHAcceptors','NumRotatableBonds'], y_vars='pIC50', height=7, aspect=0.7)
29/49:
#How many of molecules are active against LAT1
plt.hist(data['bioactivity_class'] , density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('pIC50', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 100)
29/50: sns.pairplot(data, x_vars=['bioactivity_class'], y_vars='pIC50', height=7, aspect=0.7)
29/51:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt




x = data['MW']
y = data['pIC50']
#colors = data.groupby(data['bioactivity_class'])
area = (30 * data['LogP'])  # 0 to 15 point radii

plt.scatter(x, y, s=area, alpha=0.5)
plt.xlabel('MW', fontsize=16, fontweight='bold')
plt.ylabel("pIC50", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.legend('LogP', )
plt.show()

#save file
plt.tight_layout()
plt.savefig('MW_and_pIC50_LogP.pdf', dpi=300)
29/52: sns.lmplot('MW', 'pIC50', data=data, hue='bioactivity_class', fit_reg=False)
29/53:
bioactivity_class = []
for i in df_final.pIC50:
  if float(i) >= 6:
    bioactivity_class.append("inactive")
  elif float(i) <= 4:
    bioactivity_class.append("active")
  else:
    bioactivity_class.append("intermediate")
29/54: len(bioactivity_class)
29/55: bioactivity_class
29/56: len(df_final)
29/57:
df_class = pd.DataFrame(bioactivity_class, columns=['bioactivity_class'])
df_class
df_class.to_csv('df_class.csv')
29/58: df_class
29/59: -np.log10(0.000112)
29/60: -np.log10(0.0000182)
29/61:
bioactivity_class = []
for i in df_final.pIC50:
  if float(i) >= 6:
    bioactivity_class.append("inactive")
  elif float(i) <= 5:
    bioactivity_class.append("active")
  else:
    bioactivity_class.append("intermediate")
29/62:
#active IC50 < 1000
-np.log10(0.000001)
29/63: -np.log10(0.00001)
29/64:
bioactivity_class = []
for i in df_final.pIC50:
  if float(i) >= 6:
    bioactivity_class.append("active")
  elif float(i) <= 5:
    bioactivity_class.append("inactive")
  else:
    bioactivity_class.append("intermediate")
29/65: len(bioactivity_class)
29/66: len(bioactivity_class)
29/67: bioactivity_class
29/68: len(df_final)
29/69:
df_class = pd.DataFrame(bioactivity_class, columns=['bioactivity_class'])
df_class
df_class.to_csv('df_class.csv')
29/70: df_class
29/71:
df_final = pIC50(df_norm)
df_final
df_final.to_csv('df_final.csv')
29/72:
df_final = pIC50(df_norm)
df_final
29/73: df_final.to_csv('df_final.csv')
29/74: df_class
29/75: df_class.count()
29/76: df_class.describe()
29/77:
df_final_2 = pd.read_csv("df_final_adjusted.csv")
df_final_2
29/78:
df_final_2 = pd.read_csv("df_final.csv")
df_final_2
29/79:
selection = ['molecule_chembl_id', 'canonical_smiles', 'MW', 'LogP','NumHDonors',
                      'NumHAcceptors', 'NumRotatableBonds', 'pIC50']
df_final_3 = df_final_2[selection]
df_final_3
29/80:
df_final_4 = pd.concat([df_final_3, df_class], axis =1)
df_final_4
29/81: print(df_final_4['bioactivity_class'].unique())
29/82: df_final_4.to_csv('df_final_4.csv')
29/83:
#Here we got RO5 and pIC50 of 71 cpds against LAT1 protein
data = pd.read_csv('df_final_4.csv')
29/84:
#Here we got RO5 and pIC50 of 71 cpds against LAT1 protein
data = pd.read_csv('df_final_4.csv')
29/85:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)
plt2.set_xlabel("LogP", fontsize=16, fontweight='bold')
plt2.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)
plt2.set_ylim(0, 50)
plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)


#Histogram for NumHDonors
plt3.hist(data['NumHDonors'], density=False, bins= 30, color='#FF9A00', edgecolor='black', linewidth=0.5)
plt3.set_xlabel("Number of Hydrogen Bond Donors", fontsize=16, fontweight='bold')
plt3.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)
plt3.set_ylim(0, 50)
plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 30, color='#FF165D', edgecolor='black', linewidth=0.5)
plt4.set_xlabel("Number of Hydrogen Bond Acceptors", fontsize=16, fontweight='bold')
plt4.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)
plt4.set_ylim(0, 50)
plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 30, color='#52006A', edgecolor='black', linewidth=0.5)
plt5.set_xlabel("Number of RotatableBonds", fontsize=16, fontweight='bold')
plt5.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)
plt5.set_ylim(0, 50)
plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#save file
plt.tight_layout()
plt.savefig('LAT1 plots-basic descriptors.pdf', dpi=300)
29/86:
import seaborn as sns

# visualize the relationship between the features and the response using scatterplots
sns.pairplot(data, x_vars=['MW','LogP','NumHDonors','NumHAcceptors','NumRotatableBonds'], y_vars='pIC50', height=7, aspect=0.7)
29/87:
#How many of molecules are active against LAT1
plt.hist(data['bioactivity_class'] , density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('pIC50', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 100)
29/88: sns.pairplot(data, x_vars=['bioactivity_class'], y_vars='pIC50', height=7, aspect=0.7)
29/89:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt



figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

x = data['MW']
y = data['pIC50']
#colors = data.groupby(data['bioactivity_class'])
area = (30 * data['LogP'])  # 0 to 15 point radii

plt.scatter(x, y, s=area, alpha=0.5)
plt.xlabel('MW', fontsize=16, fontweight='bold')
plt.ylabel("pIC50", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.legend('LogP', )
plt.show()

#save file
plt.tight_layout()
plt.savefig('MW_and_pIC50_LogP.pdf', dpi=300)
29/90: sns.lmplot('MW', 'pIC50', data=data, hue='bioactivity_class', fit_reg=False)
31/1:
%matplotlib inline
import matplotlib.pyplot as plt
y = [3,2,4,3,4]
x = ['Jan','Feb','Mar', 'Apr', 'May']
plt.bar(x,y)
31/2:
%matplotlib inline
import matplotlib.pyplot as plt
y = [3,2,4,3,4]
x = ['Jan','Feb','Mar', 'Apr', 'May']
plt.bar(x,y)
30/1:
bioactivity_class = []
for i in df_final.pIC50:
  if float(i) >= 6:
    bioactivity_class.append("active")
  elif float(i) <= 4:
    bioactivity_class.append("inactive")
  else:
    bioactivity_class.append("intermediate")
30/2:
#import library
import pandas as pd
30/3:
#Input yourname for this lab notebook
labname = input('what is your name?')
30/4: print(labname)
30/5:
#We will use Chembl database for molecular modeling
#install package dependencies
!pip install requests_cache==0.6.4
30/6:
#Install Chemabl database
!pip install chembl_webresource_client
30/7:
#Chembl API access...
from chembl_webresource_client.new_client import new_client
target = new_client.target
#Search target protein, we will use LAT1 as a target protein
target_query = target.search('LAT1')
targets = pd.DataFrame.from_dict(target_query)
targets
30/8:
#As we can see, we will select H.sapiens LAT1 as atarget
selected_target = targets.target_chembl_id[1]
selected_target
30/9:
#Here we will access the activity of compounds in this target, first we will try IC50
#IC50 is tell us about the potency of the compound that can reduce LAT1 activity for 50%
activity = new_client.activity
res = activity.filter(target_chembl_id=selected_target).filter(standard_type="IC50")
30/10:
#show dataframe of IC50
df = pd.DataFrame.from_dict(res)
df
30/11:
#if any compound is missing the IC50 value, then it should drop out
df2 = df[df.standard_value.notna()]
df2
30/12:
#I want to know what data do we have
print(df2.columns)
30/13:
#we want to see the molecule structure via canonical_smiles
df2.canonical_smiles
30/14:
#We want to know the structure and activity
selection = ['molecule_chembl_id','canonical_smiles','standard_units', 'standard_value']
df3 = df2[selection]
df3
30/15:
df4 = df3[df.canonical_smiles.notna()]
df4
30/16:
#install rdkit
!pip install rdkit-pypi
30/17:
import numpy as np
import rdkit
from time import time

from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
30/18:
#We are going to calculate Lipinski rule of five
from rdkit import Chem
from rdkit.Chem import Descriptors, Lipinski
30/19: print(df4['canonical_smiles'].unique())
30/20:
import pandas as pd
def lipinski(smiles, verbose=False):

    moldata= []
    for elem in smiles:
        mol=Chem.MolFromSmiles(elem) 
        moldata.append(mol)
       
    baseData= np.arange(1,1)
    i=0  
    for mol in moldata:        
       
        desc_MolWt = Descriptors.MolWt(mol)
        desc_MolLogP = Descriptors.MolLogP(mol)
        desc_NumHDonors = Lipinski.NumHDonors(mol)
        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)
        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)
           
        row = np.array([desc_MolWt,
                        desc_MolLogP,
                        desc_NumHDonors,
                        desc_NumHAcceptors,
                        desc_NumRotatableBonds])   
    
        if(i==0):
            baseData=row
        else:
            baseData=np.vstack([baseData, row])
        i=i+1      
    
    columnNames=["MW","LogP","NumHDonors","NumHAcceptors", "NumRotatableBonds"]   
    descriptors = pd.DataFrame(data=baseData,columns=columnNames)
    
    return descriptors
30/21:
df_lipinski = lipinski(df4.canonical_smiles)
df_lipinski
df_lipinski
30/22:
#combine results
df_combined = pd.concat([df3,df_lipinski], axis=1)
df_combined
30/23:
#remove na
final_df_combined = df_combined.replace(['None','NONE','none'],np.nan)
final_df_combined = final_df_combined.dropna()
print('Unique value in MW column',final_df_combined['MW'].unique())
len(final_df_combined)
30/24: final_df_combined
30/25:
#We want to know how many of them is high potency agaisnt LAT1 protein
#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution
#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()
# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 10−9
# Take the molar value and apply -log10
# Delete the standard_value column and create a new pIC50 column

#before we do that ,we need to install nescessery package

import numpy as np

def pIC50(input):
    pIC50 = []

    for i in input['standard_value_norm']:
        molar = float(i)*(10**-9) # Converts nM to M
        pIC50.append(-np.log10(molar))

    input['pIC50'] = pIC50
    x = input.drop('standard_value_norm', 1)
        
    return x
30/26: final_df_combined.standard_value.describe()
30/27:
def norm_value(input):
    norm = []

    for i in input['standard_value']:
        if float(i) > 100000000:
          i = 100000000
        norm.append(i)

    input['standard_value_norm'] = norm
    x = input.drop('standard_value', 1)
        
    return x
30/28:
df_norm = norm_value(final_df_combined)
df_norm
30/29: df_norm.standard_value_norm.describe()
30/30:
df_final = pIC50(df_norm)
df_final
30/31: df_final.to_csv('df_final.csv')
30/32:
#active IC50 < 1 UM
-np.log10(0.000001)
30/33:
#active IC50 < 10 uM

-np.log10(0.00001)
30/34:
bioactivity_class = []
for i in df_final.pIC50:
  if float(i) >= 6:
    bioactivity_class.append("active")
  elif float(i) <= 4:
    bioactivity_class.append("inactive")
  else:
    bioactivity_class.append("intermediate")
30/35: len(bioactivity_class)
30/36: bioactivity_class
30/37: len(df_final)
30/38:
df_class = pd.DataFrame(bioactivity_class, columns=['bioactivity_class'])
df_class
df_class.to_csv('df_class.csv')
30/39: df_class.describe()
30/40:
df_final_2 = pd.read_csv("df_final.csv")
df_final_2
30/41:
selection = ['molecule_chembl_id', 'canonical_smiles', 'MW', 'LogP','NumHDonors',
                      'NumHAcceptors', 'NumRotatableBonds', 'pIC50']
df_final_3 = df_final_2[selection]
df_final_3
30/42:
df_final_4 = pd.concat([df_final_3, df_class], axis =1)
df_final_4
30/43: print(df_final_4['bioactivity_class'].unique())
30/44: df_final_4.to_csv('df_final_4.csv')
30/45:
#Here we got RO5 and pIC50 of 71 cpds against LAT1 protein
data = pd.read_csv('df_final_4.csv')
30/46:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)
plt2.set_xlabel("LogP", fontsize=16, fontweight='bold')
plt2.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)
plt2.set_ylim(0, 50)
plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)


#Histogram for NumHDonors
plt3.hist(data['NumHDonors'], density=False, bins= 30, color='#FF9A00', edgecolor='black', linewidth=0.5)
plt3.set_xlabel("Number of Hydrogen Bond Donors", fontsize=16, fontweight='bold')
plt3.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)
plt3.set_ylim(0, 50)
plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 30, color='#FF165D', edgecolor='black', linewidth=0.5)
plt4.set_xlabel("Number of Hydrogen Bond Acceptors", fontsize=16, fontweight='bold')
plt4.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)
plt4.set_ylim(0, 50)
plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 30, color='#52006A', edgecolor='black', linewidth=0.5)
plt5.set_xlabel("Number of RotatableBonds", fontsize=16, fontweight='bold')
plt5.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)
plt5.set_ylim(0, 50)
plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#save file
plt.tight_layout()
plt.savefig('LAT1 plots-basic descriptors.pdf', dpi=300)
30/47:
#Visuallize pIC50
plt.hist(data['pIC50'], density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('pIC50', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 50)
#plt1.grid(True)
plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#save file
plt.tight_layout()
plt.savefig('LAT1 plots-pIC50 distribution.pdf', dpi=300)
30/48:
#Install statistic seabon
!pip install seaborn
30/49:
import seaborn as sns

# visualize the relationship between the features and the response using scatterplots
sns.pairplot(data, x_vars=['MW','LogP','NumHDonors','NumHAcceptors','NumRotatableBonds'], y_vars='pIC50', height=7, aspect=0.7)
30/50:
#How many of molecules are active against LAT1
plt.hist(data['bioactivity_class'] , density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('pIC50', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 100)
30/51: sns.pairplot(data, x_vars=['bioactivity_class'], y_vars='pIC50', height=7, aspect=0.7)
30/52:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt



figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

x = data['MW']
y = data['pIC50']
#colors = data.groupby(data['bioactivity_class'])
area = (30 * data['LogP'])  # 0 to 15 point radii

plt.scatter(x, y, s=area, alpha=0.5)
plt.xlabel('MW', fontsize=16, fontweight='bold')
plt.ylabel("pIC50", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.legend('LogP', )
plt.show()

#save file
plt.tight_layout()
plt.savefig('MW_and_pIC50_LogP.pdf', dpi=300)
30/53: sns.lmplot('MW', 'pIC50', data=data, hue='bioactivity_class', fit_reg=False)
30/54: list seabon
30/55: list seaborn
30/56: list seaborn
31/3:
x=44-32
x
30/57:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt



figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

x1 = data['MW']
x2 = data['LogP']
x3 = data['NumHDonors']
x4 = data['NumHAcceptors']
x5 = data['NumRotatableBonds']
y = data['pIC50']
#colors = data.groupby(data['bioactivity_class'])
area = (30 * data['pIC50'])  # 0 to 15 point radii

#MW
plt1.scatter(x1, y, s=area, alpha=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)

#LogP
plt2.scatter(x2, y, s=area, alpha=0.5)
plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')
plt2.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)

#NumHbondDonors
plt3.scatter(x3, y, s=area, alpha=0.5)
plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')
plt3.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt4.scatter(x4, y, s=area, alpha=0.5)
plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')
plt4.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt5.scatter(x5, y, s=area, alpha=0.5)
plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')
plt5.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)


#save file
plt.tight_layout()
plt.savefig('MW_and_pIC50_LogP.pdf', dpi=300)
31/4:
price= 110
if price >= 100
    print('Discount 5%')
else
    print('discount 2%')
31/5:
price= 110
if price >= 100:
    print('Discount 5%')
else
    print('discount 2%')
31/6:
price= 110
if price >= 100:
    print('Discount 5%')
else:
    print('discount 2%')
31/7:
price= 110
if price >= 100:
    print('Discount 5%')
else:
    print('discount 2%')
print('Good bye')
31/8: price=90
31/9:
price= 110
def discount(price)
if price >= 100:
    print('Discount 5%')
else:
    print('discount 2%')
print('Good bye')
return print
31/10:
price= 110
def discount(price):
if price >= 100:
    print('Discount 5%')
else:
    print('discount 2%')
print('Good bye')
return print
31/11:
price= 110
def discount(price):
    if price >= 100:
        print('Discount 5%')
    else:
        print('discount 2%')
        print('Good bye')
return print
31/12:
price= 110
def discount(price):
    if price >= 100:
        print('Discount 5%')
    else:
        print('discount 2%')
        print('Good bye')
    return print
31/13: price=90
31/14: discount(price)
31/15: price=110
31/16: discount(price)
31/17:
price= 110
def discount(price):
    if price >= 100:
        print('Discount 5%')
    else:
        print('discount 2%')
        print('Good bye')
31/18: discount(price)
31/19: price=90
31/20: discount(price)
31/21:
temp=32
def fanoperation(temp):
    if temp < 30:
        print('off')
    elif temp >=30 and temp < 40:
        print('moderate')
    else:
        print('high')
31/22: fanoperation(temp)
31/23: temp=40
31/24: fanoperation(temp)
31/25: temp=29
31/26: fanoperation(temp)
31/27:
for i in range(3):
    print("test for")
    print(count)
print('done')
31/28:
for i in range(3):
    print("test for")
    print(i)
print('done')
31/29:
for i in range(4,6):
    print(i)
print('done')
31/30:
count = 0
while count < 3:
    print('Hello')
    count = count + 1
31/31:
while True:
    print('Hello')
print('done')
31/32:
for i in input()
    x = i*10
print('done')
31/33:
for i in input():
    x = i*10
print('done')
31/34:
for i in input():
    x = i*10
print('x')
31/35:
for i in input():
    x = i*10
print(x)
31/36:
for i in input():
    x = i*10
print(i)
31/37:
for i in input():
    x = i*10
print(x)
31/38:
for i in input():
    x = i*10
print(x)
31/39:
for i in input():
    x = i+10
print(x)
31/40:
for i in input():
    x = int(i)+10
print(x)
31/41:
for i in input():
    x = int(i) + 10
print(x)
31/42:
for i in input():
    x = int(i) + 10
print(x)
31/43:
for i in input():
    x = int(i) + 10
print(x)
31/44:
for i in input():
    x = int(i) + 10
print(x)
31/45:
x = 10
name = tarapong
print(name % x)
31/46:
x = 10
name = 'tarapong'
print(name % x)
31/47:
x = 10
name = 'tarapong'
print(name, x)
31/48: print(f"tar)
31/49: print(f"tar")
31/50: print(f"tar" + x)
31/51: print(name + x)
31/52: print(f' My name is {name} + {x})
31/53: print(f' My name is {name} + {x}')
31/54: print('a = {}'.format(x))
31/55: print('a = {x}')
31/56: print('a = ', x)
31/57: print('a =', x)
31/58:
def area(input):
    area = input * input
    return area
31/59: b = area(5):
31/60: b = area(5)
31/61:
b = area(5)
print('b box area is', b)
31/62: print('b = %.2f' % (b))
31/63: print('b=', float(b))
31/64: print('b = %.10f' % (b))
31/65: print('b=', float(b).0)
31/66: print('b=', float(b))
31/67:
import math
print('Pi= %.5f' % (math.pi))
31/68:
print('Pi= %s and %.2f' % (name, math.pi
                        ))
31/69: box = area(10)
31/70: box
31/71:
def boxarea(lenght):
    x=lenght*lenght
    return x
31/72: ab = boxarea(50)
31/73: ab
31/74:
if __name__ == '__main__':
    b = area(4)
31/75:
if __name__ == '__main__':
    b = area(4)
    print('area =', b)
30/58:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt



figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

x1 = data['MW']
x2 = data['LogP']
x3 = data['NumHDonors']
x4 = data['NumHAcceptors']
x5 = data['NumRotatableBonds']
y = data['pIC50']
#colors = data.groupby(data['bioactivity_class'])
area = (30 * data['pIC50'])  # 0 to 15 point radii

#MW
plt1.scatter(x1, y, s=area, alpha=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)

#LogP
plt2.scatter(x2, y, s=area, alpha=0.5)
plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')
plt2.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)

#NumHbondDonors
plt3.scatter(x3, y, s=area, alpha=0.5)
plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')
plt3.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt4.scatter(x4, y, s=area, alpha=0.5)
plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')
plt4.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt5.scatter(x5, y, s=area, alpha=0.5)
plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')
plt5.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)


#save file
plt.tight_layout()
plt.savefig('_and_pIC50.pdf', dpi=300)
30/59:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt



figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

x1 = data['MW']
x2 = data['LogP']
x3 = data['NumHDonors']
x4 = data['NumHAcceptors']
x5 = data['NumRotatableBonds']
y = data['pIC50']
colors = {'active':'red', 'inactive':'blue', 'intermediate':'yellow'}
area = (30 * data['pIC50'])  # 0 to 15 point radii

#MW
plt1.scatter(x1, y, s=area, c=data['bioacitvity_class'] alpha=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)

#LogP
plt2.scatter(x2, y, s=area, alpha=0.5)
plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')
plt2.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)

#NumHbondDonors
plt3.scatter(x3, y, s=area, alpha=0.5)
plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')
plt3.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt4.scatter(x4, y, s=area, alpha=0.5)
plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')
plt4.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt5.scatter(x5, y, s=area, alpha=0.5)
plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')
plt5.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)


#save file
plt.tight_layout()
plt.savefig('_and_pIC50.pdf', dpi=300)
30/60:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt



figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

x1 = data['MW']
x2 = data['LogP']
x3 = data['NumHDonors']
x4 = data['NumHAcceptors']
x5 = data['NumRotatableBonds']
y = data['pIC50']
colors = {'active':'red', 'inactive':'blue', 'intermediate':'yellow'}
area = (30 * data['pIC50'])  # 0 to 15 point radii

#MW
plt1.scatter(x1, y, s=area, c=data['bioacitvity_class'], alpha=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)

#LogP
plt2.scatter(x2, y, s=area, alpha=0.5)
plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')
plt2.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)

#NumHbondDonors
plt3.scatter(x3, y, s=area, alpha=0.5)
plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')
plt3.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt4.scatter(x4, y, s=area, alpha=0.5)
plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')
plt4.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt5.scatter(x5, y, s=area, alpha=0.5)
plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')
plt5.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)


#save file
plt.tight_layout()
plt.savefig('_and_pIC50.pdf', dpi=300)
30/61:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt



figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

x1 = data['MW']
x2 = data['LogP']
x3 = data['NumHDonors']
x4 = data['NumHAcceptors']
x5 = data['NumRotatableBonds']
y = data['pIC50']
colors = {'active':'red', 'inactive':'blue', 'intermediate':'yellow'}
area = (30 * data['pIC50'])  # 0 to 15 point radii

#MW
plt1.scatter(x1, y, s=area, c=data['bioacitivity_class'], alpha=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)

#LogP
plt2.scatter(x2, y, s=area, alpha=0.5)
plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')
plt2.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)

#NumHbondDonors
plt3.scatter(x3, y, s=area, alpha=0.5)
plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')
plt3.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt4.scatter(x4, y, s=area, alpha=0.5)
plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')
plt4.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt5.scatter(x5, y, s=area, alpha=0.5)
plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')
plt5.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)


#save file
plt.tight_layout()
plt.savefig('_and_pIC50.pdf', dpi=300)
30/62:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt



figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

x1 = data['MW']
x2 = data['LogP']
x3 = data['NumHDonors']
x4 = data['NumHAcceptors']
x5 = data['NumRotatableBonds']
y = data['pIC50']
colors = {'active':'red', 'inactive':'blue', 'intermediate':'yellow'}
area = (30 * data['pIC50'])  # 0 to 15 point radii

#MW
plt1.scatter(x1, y, s=area, c=data['bioactivity_class'], alpha=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)

#LogP
plt2.scatter(x2, y, s=area, alpha=0.5)
plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')
plt2.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)

#NumHbondDonors
plt3.scatter(x3, y, s=area, alpha=0.5)
plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')
plt3.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt4.scatter(x4, y, s=area, alpha=0.5)
plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')
plt4.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt5.scatter(x5, y, s=area, alpha=0.5)
plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')
plt5.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)


#save file
plt.tight_layout()
plt.savefig('_and_pIC50.pdf', dpi=300)
30/63:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt



figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

x1 = data['MW']
x2 = data['LogP']
x3 = data['NumHDonors']
x4 = data['NumHAcceptors']
x5 = data['NumRotatableBonds']
y = data['pIC50']
colors = {'active':'red', 'inactive':'blue', 'intermediate':'yellow'}
area = (30 * data['pIC50'])  # 0 to 15 point radii

#MW
plt1.scatter(x1, y, s=area, c=data['bioactivity_class'].map(colors), alpha=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)

#LogP
plt2.scatter(x2, y, s=area, alpha=0.5)
plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')
plt2.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)

#NumHbondDonors
plt3.scatter(x3, y, s=area, alpha=0.5)
plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')
plt3.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt4.scatter(x4, y, s=area, alpha=0.5)
plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')
plt4.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt5.scatter(x5, y, s=area, alpha=0.5)
plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')
plt5.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)


#save file
plt.tight_layout()
plt.savefig('_and_pIC50.pdf', dpi=300)
30/64:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt



figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

x1 = data['MW']
x2 = data['LogP']
x3 = data['NumHDonors']
x4 = data['NumHAcceptors']
x5 = data['NumRotatableBonds']
y = data['pIC50']
colors = {'active':'red', 'inactive':'blue', 'intermediate':'green'}
area = (30 * data['pIC50'])  # 0 to 15 point radii

#MW
plt1.scatter(x1, y, s=area, c=data['bioactivity_class'].map(colors), alpha=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)

#LogP
plt2.scatter(x2, y, s=area, alpha=0.5)
plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')
plt2.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)

#NumHbondDonors
plt3.scatter(x3, y, s=area, alpha=0.5)
plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')
plt3.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt4.scatter(x4, y, s=area, alpha=0.5)
plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')
plt4.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt5.scatter(x5, y, s=area, alpha=0.5)
plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')
plt5.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)


#save file
plt.tight_layout()
plt.savefig('_and_pIC50.pdf', dpi=300)
30/65:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt



figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

x1 = data['MW']
x2 = data['LogP']
x3 = data['NumHDonors']
x4 = data['NumHAcceptors']
x5 = data['NumRotatableBonds']
y = data['pIC50']
colors = {'active':'red', 'inactive':'blue', 'intermediate':'green'}
area = (30 * data['pIC50'])  # 0 to 15 point radii

#MW
plt1.scatter(x1, y, s=area, c=data['bioactivity_class'].map(colors), alpha=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)

#LogP
plt2.scatter(x2, y, s=area,c=data['bioactivity_class'].map(colors), alpha=0.5)
plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')
plt2.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)

#NumHbondDonors
plt3.scatter(x3, y, s=area, c=data['bioactivity_class'].map(colors), alpha=0.5)
plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')
plt3.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt4.scatter(x4, y, s=area, c=data['bioactivity_class'].map(colors), alpha=0.5)
plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')
plt4.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt5.scatter(x5, y, s=area, c=data['bioactivity_class'].map(colors), alpha=0.5)
plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')
plt5.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)


#save file
plt.tight_layout()
plt.savefig('_and_pIC50.pdf', dpi=300)
31/76: data=[1,2,3,4,5,6,7]
31/77: data(type)
31/78: type(data)
31/79: data.append(20)
31/80: data
31/81: ave= sum(data)/len(data)
31/82: ave
31/83: data=[[1,2,3],[2,3,4]]
31/84: data
31/85: data[0]
31/86: data[0,2]
31/87: data[0][2]
31/88: tp1 = 2,3,4
31/89: type(tp1)
31/90: data = [2,1,3,4,5,6,7,8]
31/91: data[3:4]
31/92: data[0:-1]
31/93: data[:3]
31/94:
for k in data:
    b=k*2
    print(b)
31/95:
for k in data:
    b=k*2
    a.append(b)
    print(a)
31/96:
a=[]
for k in data:
    b=k*2
    a.append(b)
    print(a)
31/97:
a=[]
for k in data:
    b=k*2
a.append(b)
  
    print(a)
31/98:
a=[]
for k in data:
    b=k*2
31/99:
a=[]
for k in data:
    b=k*2
a.append(b)

    print(a)
31/100:
a=[]
for k in data:
    b=k*2
a.append(b)

print(a)
31/101:
a=[]
for k in data:
    b=k*2
    a.append(b)
input['a'] = a
print(a)
31/102:
a=[]
for k in data:
    b=k*2
    a.append(b)
input['b'] = a
print(a)
31/103:
a=[]
for k in data:
    b=k*2
    a.append(b)
input[b] = a
print(a)
31/104:
a=[]
for k in data:
    b=k*2
    a.append(b)
print(a)
31/105:
a=[]
for k in data:
    b=k*2
    a.append(b)
    print(a)
31/106: datas = {'Name':'A', 'Age':'29', 'Score':'35'}
31/107: type(datas)
31/108: datas[0]['Name']
31/109: datas = {'Name':'A', 'Age':'29', 'Score':'35'}
31/110: datas[0]["Name"]
31/111: datas = [{'Name':'A', 'Age':'29', 'Score':'35'}, {'Name':'B', 'Age': '20', 'Score':'30'}]
31/112: datas[0][Name]
31/113: datas[0]['Name']
31/114:
for k in datas:
    print({':1'}{}.format(k['Name'], k['Score']))
31/115:
for k in datas:
    print({':1'} {}.format(k['Name'], k['Score']))
31/116:
for k in datas:
    print('{:1} {}.format(k['Name'], k['Score']))
31/117:
for k in datas:
    print('{:1} {}'.format(k['Name'], k['Score']))
31/118:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
31/119: np.mean(data)
31/120: np.std(data)
31/121: np.median(data)
30/66:
#How many of inactive
np.max(data['bioacitivity_class, inactive'])
30/67:
#How many of inactive
import numpy as np
np.max(data['bioacitivity_class, inactive'])
30/68:
#How many of inactive
import numpy as np
np.max(data['bioacitivity_class'])
30/69:
#How many of inactive
import numpy as np
np.max(data[bioacitivity_class])
30/70:
#How many of inactive
import numpy as np
np.max(data['bioactivity_class'])
30/71:
#How many of inactive?
import numpy as np
df.data['bioactivity_class'].value_counts
30/72:
#How many of inactive?
import numpy as np
df = pd.Dataframe.['bioactivity_class'].value_counts
30/73:
#How many of inactive?
import numpy as np
df = pd.Dataframe.data['bioactivity_class'].value_counts
30/74:
#How many of inactive?
import numpy as np
df = np.Dataframe.data['bioactivity_class'].value_counts
30/75:
#How many of inactive?
import numpy as np
df = pd.DataFrame.data['bioactivity_class'].value_counts
30/76:
#How many of inactive?
import numpy as np
data['bioactivity_class']
30/77:
#How many of inactive?
import numpy as np
data['bioactivity_class'].value_counts
30/78:
#How many of inactive?
import numpy as np
import pandas as pd
df = DataFrame(data)
data['bioactivity_class'].value_counts
30/79:
#How many of inactive?
import numpy as np
import pandas as pd
df = pd.DataFrame(data)
data['bioactivity_class'].value_counts
30/80:
#How many of inactive?
import numpy as np
import pandas as pd
df = pd.DataFrame(data['bioactivity_class'])
df.data()
30/81:
#How many of inactive?
import numpy as np
import pandas as pd
df = pd.DataFrame(data['bioactivity_class'])
30/82:
#How many of inactive?
import numpy as np
import pandas as pd
df = pd.DataFrame(data['bioactivity_class'])
df
30/83:
#How many of inactive?
import numpy as np
import pandas as pd
df = pd.DataFrame(data['bioactivity_class'])
df.bioactivity_class.value_count
30/84:
#How many of inactive?
import numpy as np
import pandas as pd
df = pd.DataFrame(data['bioactivity_class'])
df.bioactivity_class.value_counts
30/85:
#How many of inactive?
import numpy as np
import pandas as pd
df = pd.DataFrame(data['bioactivity_class'])
df.bioactivity_class.value_counts()
31/122: x = [1,2,3,4,5,56,6,6767]
31/123: np.array(x)
31/124: x=[[6,7,4,5,1],[2,8,3,6,4],[1,3,4,5,6],[8,9,10,12,8]]
31/125: npx=np.matrix(x)
31/126: npx
31/127: npx1=np.array(x)
31/128: npx1
31/129: npx.shape
31/130: npx1.shape
31/131: npx1[0,3]
31/132: npx[0,3]
31/133: npx[1,3]
31/134:
X=npx[:, :-1]
X
31/135: y=npx[-1:]
31/136: y
31/137: y=npx[:, -1:]
31/138: y
31/139: z=npx[:1,:]
31/140: z
31/141: npx[-1:,:]
31/142: npx > 5
31/143: np[npx>5]
31/144: np[npx > 5]
31/145: npx[npx > 5]
31/146: npx1[npx1 > 5]
31/147: npx.T
31/148: npx.reshape(1,-1)
31/149: npx.reshape(-1,1)
31/150: np.random.rand(4)
31/151: np.random.rand(4)*30+20
31/152:
mu, sd = 100, 10
np.random.normal(mu, sd, 1000)
31/153:
mu, sd = 100, 10
s = np.random.normal(mu, sd, 1000)
31/154: s[:10]
31/155:
import matplotlib as plt
plt.hist(s)
31/156:
import matplotlib.pyplot as plt
plt.hist(s)
31/157:
import matplotlib.pyplot as plt
plt.hist(s, bin=40)
31/158:
import matplotlib.pyplot as plt
plt.hist(s, bins=40)
31/159:
import panddas as pd
S = pd.Series([1,2,3,xx,3,5])
31/160:
import pandas as pd
S = pd.Series([1,2,3,xx,3,5])
31/161:
import pandas as pd
S = pd.Series([1,2,3,"xx",3,5])
31/162: S
31/163: datas = [13,12,15,16]
31/164: ndata = np.array(datas)
31/165: ps = pd.Series(ndata)
31/166: ps
31/167:
idx = ['a','b','c','']
psd = pd.Series(datas, index=idx)
31/168: psd
31/169: data = {'Name':'Tarapong', 'Class':'New'}
31/170: ps =pd.Series(data)
31/171: ps
31/172: ps['Name']
31/173: data = {'Name':'Tarapong', 'Class':'New', 'Class2':'New2', 'Class3':'New3'}
31/174: ps['Name']
31/175: ps[0]
31/176: ps
31/177: ps =pd.Series(data)
31/178: ps
31/179: ps['Name']
31/180: ps[0]
31/181: ps
31/182: ps[:]
31/183: ps[:2]
31/184: ps[-1]
33/1:
#import library
import pandas as pd
33/2:
#Input yourname for this lab notebook
labname = input('what is your name?')
33/3: print(labname)
33/4:
#We will use Chembl database for molecular modeling
#install package dependencies
!pip install requests_cache==0.6.4
33/5:
#Install Chemabl database
!pip install chembl_webresource_client
33/6:
#Chembl API access...
from chembl_webresource_client.new_client import new_client
target = new_client.target
#Search target protein, we will use LAT1 as a target protein
target_query = target.search('LAT1')
targets = pd.DataFrame.from_dict(target_query)
targets
33/7:
#As we can see, we will select H.sapiens LAT1 as atarget
selected_target = targets.target_chembl_id[1]
selected_target
33/8:
#Here we will access the activity of compounds in this target, first we will try IC50
#IC50 is tell us about the potency of the compound that can reduce LAT1 activity for 50%
activity = new_client.activity
res = activity.filter(target_chembl_id=selected_target).filter(standard_type="IC50")
33/9:
#show dataframe of IC50
df = pd.DataFrame.from_dict(res)
df
33/10:
#if any compound is missing the IC50 value, then it should drop out
df2 = df[df.standard_value.notna()]
df2
33/11:
#I want to know what data do we have
print(df2.columns)
33/12:
#we want to see the molecule structure via canonical_smiles
df2.canonical_smiles
33/13:
#We want to know the structure and activity
selection = ['molecule_chembl_id','canonical_smiles','standard_units', 'standard_value']
df3 = df2[selection]
df3
33/14:
df4 = df3[df.canonical_smiles.notna()]
df4
33/15:
#install rdkit
!pip install rdkit-pypi
33/16:
import numpy as np
import rdkit
from time import time

from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
33/17:
#We are going to calculate Lipinski rule of five
from rdkit import Chem
from rdkit.Chem import Descriptors, Lipinski
33/18: print(df4['canonical_smiles'].unique())
33/19:
import pandas as pd
def lipinski(smiles, verbose=False):

    moldata= []
    for elem in smiles:
        mol=Chem.MolFromSmiles(elem) 
        moldata.append(mol)
       
    baseData= np.arange(1,1)
    i=0  
    for mol in moldata:        
       
        desc_MolWt = Descriptors.MolWt(mol)
        desc_MolLogP = Descriptors.MolLogP(mol)
        desc_NumHDonors = Lipinski.NumHDonors(mol)
        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)
        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)
           
        row = np.array([desc_MolWt,
                        desc_MolLogP,
                        desc_NumHDonors,
                        desc_NumHAcceptors,
                        desc_NumRotatableBonds])   
    
        if(i==0):
            baseData=row
        else:
            baseData=np.vstack([baseData, row])
        i=i+1      
    
    columnNames=["MW","LogP","NumHDonors","NumHAcceptors", "NumRotatableBonds"]   
    descriptors = pd.DataFrame(data=baseData,columns=columnNames)
    
    return descriptors
33/20:
df_lipinski = lipinski(df4.canonical_smiles)
df_lipinski
df_lipinski
33/21:
#combine results
df_combined = pd.concat([df3,df_lipinski], axis=1)
df_combined
33/22:
#remove na
final_df_combined = df_combined.replace(['None','NONE','none'],np.nan)
final_df_combined = final_df_combined.dropna()
print('Unique value in MW column',final_df_combined['MW'].unique())
len(final_df_combined)
33/23: final_df_combined
33/24:
#We want to know how many of them is high potency agaisnt LAT1 protein
#First we should rearrange from IC50 into pIC50 for a better uniformnity distribution
#We will convert IC50 to the negative logarithmic scale which is essentially -log10(IC50) or pIC50()
# First Take the IC50 values from the standard_value column and converts it from nM to M by multiplying the value by 10−9
# Take the molar value and apply -log10
# Delete the standard_value column and create a new pIC50 column

#before we do that ,we need to install nescessery package

import numpy as np

def pIC50(input):
    pIC50 = []

    for i in input['standard_value_norm']:
        molar = float(i)*(10**-9) # Converts nM to M
        pIC50.append(-np.log10(molar))

    input['pIC50'] = pIC50
    x = input.drop('standard_value_norm', 1)
        
    return x
33/25: final_df_combined.standard_value.describe()
33/26:
def norm_value(input):
    norm = []

    for i in input['standard_value']:
        if float(i) > 100000000:
          i = 100000000
        norm.append(i)

    input['standard_value_norm'] = norm
    x = input.drop('standard_value', 1)
        
    return x
33/27:
df_norm = norm_value(final_df_combined)
df_norm
33/28: df_norm.standard_value_norm.describe()
33/29:
df_final = pIC50(df_norm)
df_final
33/30: df_final.to_csv('df_final.csv')
33/31:
#active IC50 < 1 UM
-np.log10(0.000001)
33/32:
#active IC50 < 10 uM

-np.log10(0.00001)
33/33:
bioactivity_class = []
for i in df_final.pIC50:
  if float(i) >= 6:
    bioactivity_class.append("active")
  elif float(i) <= 4:
    bioactivity_class.append("inactive")
  else:
    bioactivity_class.append("intermediate")
33/34: len(bioactivity_class)
33/35: bioactivity_class
33/36: len(df_final)
33/37:
df_class = pd.DataFrame(bioactivity_class, columns=['bioactivity_class'])
df_class
df_class.to_csv('df_class.csv')
33/38: df_class.describe()
33/39:
df_final_2 = pd.read_csv("df_final.csv")
df_final_2
33/40:
selection = ['molecule_chembl_id', 'canonical_smiles', 'MW', 'LogP','NumHDonors',
                      'NumHAcceptors', 'NumRotatableBonds', 'pIC50']
df_final_3 = df_final_2[selection]
df_final_3
33/41:
df_final_4 = pd.concat([df_final_3, df_class], axis =1)
df_final_4
33/42: print(df_final_4['bioactivity_class'].unique())
33/43: df_final_4.to_csv('df_final_4.csv')
33/44:
#Here we got RO5 and pIC50 of 71 cpds against LAT1 protein
data = pd.read_csv('df_final_4.csv')
33/45:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)
plt2.set_xlabel("LogP", fontsize=16, fontweight='bold')
plt2.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)
plt2.set_ylim(0, 50)
plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)


#Histogram for NumHDonors
plt3.hist(data['NumHDonors'], density=False, bins= 30, color='#FF9A00', edgecolor='black', linewidth=0.5)
plt3.set_xlabel("Number of Hydrogen Bond Donors", fontsize=16, fontweight='bold')
plt3.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)
plt3.set_ylim(0, 50)
plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 30, color='#FF165D', edgecolor='black', linewidth=0.5)
plt4.set_xlabel("Number of Hydrogen Bond Acceptors", fontsize=16, fontweight='bold')
plt4.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)
plt4.set_ylim(0, 50)
plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 30, color='#52006A', edgecolor='black', linewidth=0.5)
plt5.set_xlabel("Number of RotatableBonds", fontsize=16, fontweight='bold')
plt5.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)
plt5.set_ylim(0, 50)
plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#save file
plt.tight_layout()
plt.savefig('LAT1 plots-basic descriptors.pdf', dpi=300)
33/46:
#Visuallize pIC50
plt.hist(data['pIC50'], density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('pIC50', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 50)
#plt1.grid(True)
plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#save file
plt.tight_layout()
plt.savefig('LAT1 plots-pIC50 distribution.pdf', dpi=300)
33/47:
#Install statistic seabon
!pip install seaborn
33/48:
import seaborn as sns

# visualize the relationship between the features and the response using scatterplots
sns.pairplot(data, x_vars=['MW','LogP','NumHDonors','NumHAcceptors','NumRotatableBonds'], y_vars='pIC50', height=7, aspect=0.7)
33/49:
#How many of molecules are active against LAT1
plt.hist(data['bioactivity_class'] , density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('pIC50', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 100)
33/50:
#How many of each categoryinactive?
import pandas as pd
df = pd.DataFrame(data['bioactivity_class'])
df.bioactivity_class.value_counts()
33/51: sns.pairplot(data, x_vars=['bioactivity_class'], y_vars='pIC50', height=7, aspect=0.7)
33/52:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt



figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

x1 = data['MW']
x2 = data['LogP']
x3 = data['NumHDonors']
x4 = data['NumHAcceptors']
x5 = data['NumRotatableBonds']
y = data['pIC50']
colors = {'active':'red', 'inactive':'blue', 'intermediate':'green'}
area = (30 * data['pIC50'])  # 0 to 15 point radii

#MW
plt1.scatter(x1, y, s=area, c=data['bioactivity_class'].map(colors), alpha=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)

#LogP
plt2.scatter(x2, y, s=area,c=data['bioactivity_class'].map(colors), alpha=0.5)
plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')
plt2.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)

#NumHbondDonors
plt3.scatter(x3, y, s=area, c=data['bioactivity_class'].map(colors), alpha=0.5)
plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')
plt3.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt4.scatter(x4, y, s=area, c=data['bioactivity_class'].map(colors), alpha=0.5)
plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')
plt4.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt5.scatter(x5, y, s=area, c=data['bioactivity_class'].map(colors), alpha=0.5)
plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')
plt5.set_ylabel("pIC50", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)


#save file
plt.tight_layout()
plt.savefig('RO5_and_pIC50.pdf', dpi=300)
34/1:
%matplotlib inline
import matplotlib.pyplot as plt
y = [3,2,4,3,4]
x = ['Jan','Feb','Mar', 'Apr', 'May']
plt.bar(x,y)
34/2:
x=44-32
x
34/3:
price= 110
def discount(price):
    if price >= 100:
        print('Discount 5%')
    else:
        print('discount 2%')
        print('Good bye')
34/4: price=90
34/5: discount(price)
34/6: price=110
34/7: discount(price)
34/8:
temp=32
def fanoperation(temp):
    if temp < 30:
        print('off')
    elif temp >=30 and temp < 40:
        print('moderate')
    else:
        print('high')
34/9: fanoperation(temp)
34/10: temp=40
34/11: fanoperation(temp)
34/12: temp=29
34/13: fanoperation(temp)
34/14:
for i in range(3):
    print("test for")
    print(i)
print('done')
34/15:
for i in range(4,6):
    print(i)
print('done')
34/16:
count = 0
while count < 3:
    print('Hello')
    count = count + 1
34/17:
for i in input():
    x = i*10
print('done')
35/1:
list_Fp = ['Fingerprinter.xml', 'ExtendedFingerprinter.xml', 'EStateFingerprinter.xml',
           'GraphOnlyFingerprinter.xml', 'MACCSFingerprinter.xml', 'PubchemFingerprinter.xml',
           'SubstructureFingerprinter.xml', 'KlekotaRothFingerprinter.xml', 'AtomPairs2DFingerprinter.xml']
35/2:
def Fingerprint(name , Fp):
    
    group = name
    group = [name.replace(".smi", ".csv") for name in group]
    result = ', '.join(group)
    
    Fp_name = Fp
    Fp_name = Fp.replace(".xml", "_")
    
    print name
    print result
    print Fp
    print Fp_name

    ! java -jar -Xms64m -Xmx200m /ml/PaDEL-Descriptor/PaDEL-Descriptor.jar
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes /Users/zeromtmu/Desktop/Thesis/PaDEL-Descriptor/$Fp \
    -dir /Users/zeromtmu/Desktop/Thesis/ER_alpha_Regression/smiles/$name \
    -file /Users/zeromtmu/Desktop/Thesis/ER_alpha_Regression/Fingerprint/$Fp_name$result
35/3:
def Fingerprint(name , Fp):
    
    group = name
    group = [name.replace(".smi", ".csv") for name in group]
    result = ', '.join(group)
    
    Fp_name = Fp
    Fp_name = Fp.replace(".xml", "_")
    
    print (name)
    print (result)
    print (Fp)
    print (Fp_name)

    ! java -jar -Xms64m -Xmx200m /ml/PaDEL-Descriptor/PaDEL-Descriptor.jar
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes /Users/zeromtmu/Desktop/Thesis/PaDEL-Descriptor/$Fp \
    -dir /Users/zeromtmu/Desktop/Thesis/ER_alpha_Regression/smiles/$name \
    -file /Users/zeromtmu/Desktop/Thesis/ER_alpha_Regression/Fingerprint/$Fp_name$result
35/4:
def Fingerprint(name , Fp):
    
    group = name
    group = [name.replace(".smi", ".csv") for name in group]
    result = ', '.join(group)
    
    Fp_name = Fp
    Fp_name = Fp.replace(".xml", "_")
    
    print (name)
    print (result)
    print (Fp)
    print (Fp_name)

    ! java -jar -Xms64m -Xmx200m /ml/PaDEL-Descriptor/PaDEL-Descriptor.jar
    -removesalt -standardizenitro  \
35/5:
def Fingerprint(name , Fp):
    
    group = name
    group = [name.replace(".smi", ".csv") for name in group]
    result = ', '.join(group)
    
    Fp_name = Fp
    Fp_name = Fp.replace(".xml", "_")
    
    print (name)
    print (result)
    print (Fp)
    print (Fp_name)

    ! java -jar -Xms64m -Xmx200m /ml/PaDEL-Descriptor/PaDEL-Descriptor.jar
    -removesalt -standardizenitro  \
35/6:
def Fingerprint(name , Fp):
    
    group = name
    group = [name.replace(".smi", ".csv") for name in group]
    result = ', '.join(group)
    
    Fp_name = Fp
    Fp_name = Fp.replace(".xml", "_")
    
    print (name)
    print (result)
    print (Fp)
    print (Fp_name)

    ! java -jar -Xms64m -Xmx200m /ml/PaDEL-Descriptor/PaDEL-Descriptor.jar
    -removesalt -standardizenitro  \
35/7:
import os
import glob

path =  r'smiles/'
for f in glob.glob(path + '*.smi'):
    names = [os.path.basename(f)]
    for Fp in (list_Fp):
        Fingerprint(names, Fp)
35/8:
import pandas as pd
df = pd.read_csv('df_final_4.csv')
35/9: df
35/10:
selection = ['canonical_smiles','molecule_chembl_id']
df3_selection = df[selection]
df3_selection.to_csv('molecule.smi', sep='\t', index=False, header=False)
35/11:
selection = ['canonical_smiles','molecule_chembl_id']
df_selection = df[selection]
df_selection.to_csv('molecule.smi', sep='\t', index=False, header=False)
35/12: print(df_selection)
35/13: print(molecule.smi)
35/14: !cat molecule.smi
35/15:
type(molecule.smi
    )
35/16: ! cat padel.sh
35/17: ! bash padel.sh
35/18: ! cat padel.sh
35/19: ! bash padel.sh
35/20: df2 = pd.read_csv(descriptors_output.csv)
35/21: df2 = pd.read_csv('descriptors_output.csv')
35/22: df2
35/23:
df3 = pd.drop.(df2['Name'])
df3
35/24:
df3 = pd.drop.df2['Name']
df3
35/25:
X = df2.drop.colums['Name']
X
35/26:
X = df2.drop.columns['Name']
X
35/27: X = df2.drop.Name
35/28: X = df2.drop(columns='Name')
35/29: X
35/30: Y = df['pIC50']
35/31: Y
35/32: df3 = pd.concat([X, Y])
35/33: df3
35/34: df3 = pd.concat([X, Y], 1)
35/35: df3
35/36: df3.to_csv('df3_pubchem_Fp_pIC50.csv', index=False)
35/37: ! pip install pydelpy
35/38: ! pip install padelpy
35/39: from padelpy import from_smiles
35/40:
from padelpy import padeldescriptor
padeldescriptor(mol_dir='molecules.smi', d_file='descriptors.csv')
35/41:
from padelpy import padeldescriptor
padeldescriptor(mol_dir='molecule.smi', d_file='descriptors.csv')
35/42:
selection = ['canonical_smiles']
df_selection_test = df[selection]
df_selection_test.to_csv('molecule_test.smi', sep='\t', index=False, header=False)
35/43:
from padelpy import padeldescriptor
padeldescriptor(mol_dir='molecule_test.smi', d_file='descriptors.csv')
35/44:
from padelpy import from_smiles

# calculate molecular descriptors for propane
descriptors = from_smiles('CCC')
35/45:
from padelpy import from_smiles

# in addition to descriptors, calculate PubChem fingerprints
desc_fp = from_smiles('CCC', fingerprints=True)
35/46: desc_fp
35/47:
from padelpy import from_smiles

# in addition to descriptors, calculate PubChem fingerprints
desc_fp = from_smiles('CCC', fingerprints=True)
desc_fp[:3]
35/48: X
35/49: Y
35/50:
import pandas as pd
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
35/51: X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)
35/52: X_train.shape, Y_train.shape
35/53: X_test.shape, Y_test.shape
35/54:
model = RandomForestRegressor(n_estimators=100)
model.fit(X_train, Y_train)
r2 = model.score(X_test, Y_test)
r2
35/55: Y_pred = model.predict(X_test)
35/56:
import matplotlib.pyplot as plt

sns.set(color_codes=True)
sns.set_style("white")

ax = sns.regplot(Y_test, Y_pred, scatter_kws={'alpha':0.4})
ax.set_xlabel('Experimental pIC50', fontsize='large', fontweight='bold')
ax.set_ylabel('Predicted pIC50', fontsize='large', fontweight='bold')
ax.set_xlim(0, 12)
ax.set_ylim(0, 12)
ax.figure.set_size_inches(5, 5)
plt.show
35/57: Y_pred = model.predict(X_train)
35/58:
import matplotlib.pyplot as plt

sns.set(color_codes=True)
sns.set_style("white")

ax = sns.regplot(Y_test, Y_pred, scatter_kws={'alpha':0.4})
ax.set_xlabel('Experimental pIC50', fontsize='large', fontweight='bold')
ax.set_ylabel('Predicted pIC50', fontsize='large', fontweight='bold')
ax.set_xlim(0, 12)
ax.set_ylim(0, 12)
ax.figure.set_size_inches(5, 5)
plt.show
35/59:
import matplotlib.pyplot as plt

sns.set(color_codes=True)
sns.set_style("white")

ax = sns.regplot(Y_train, Y_pred, scatter_kws={'alpha':0.4})
ax.set_xlabel('Experimental pIC50', fontsize='large', fontweight='bold')
ax.set_ylabel('Predicted pIC50', fontsize='large', fontweight='bold')
ax.set_xlim(0, 12)
ax.set_ylim(0, 12)
ax.figure.set_size_inches(5, 5)
plt.show
35/60:
import matplotlib.pyplot as plt

sns.set(color_codes=True)
sns.set_style("white")

ax = sns.regplot(Y_test, Y_pred, scatter_kws={'alpha':0.4})
ax.set_xlabel('Experimental pIC50', fontsize='large', fontweight='bold')
ax.set_ylabel('Predicted pIC50', fontsize='large', fontweight='bold')
ax.set_xlim(0, 12)
ax.set_ylim(0, 12)
ax.figure.set_size_inches(5, 5)
plt.show
35/61: Y_pred = model.predict(X_test)
35/62:
import matplotlib.pyplot as plt

sns.set(color_codes=True)
sns.set_style("white")

ax = sns.regplot(Y_test, Y_pred, scatter_kws={'alpha':0.4})
ax.set_xlabel('Experimental pIC50', fontsize='large', fontweight='bold')
ax.set_ylabel('Predicted pIC50', fontsize='large', fontweight='bold')
ax.set_xlim(0, 12)
ax.set_ylim(0, 12)
ax.figure.set_size_inches(5, 5)
plt.show
35/63:
plt.scatter(X,Y)
plt.show()
35/64:
plt.bar(X,Y)
plt.show()
35/65:
plt.hist(X,Y)
plt.show()
35/66:
plt.bar(X,Y)
plt.show()
35/67:
plt.bar(X, Y)
plt.show()
35/68: sns.pairplot(df3, x_vars=[X], y_vars='pIC50', height=7, aspect=0.7)
35/69: sns.pairplot(df3, x_vars=[], y_vars='pIC50', height=7, aspect=0.7)
35/70: sns.pairplot(df3, x_vars=[;-1], y_vars='pIC50', height=7, aspect=0.7)
35/71: sns.pairplot(df3, x_vars=[:-1], y_vars='pIC50', height=7, aspect=0.7)
35/72: df.heaed(3)
35/73: df.head(3)
35/74: df3.head(3)
35/75: sns.pairplot(df3, x_vars=['PubchemFP0', 'PubchemFP1'], y_vars='pIC50', height=4, kind='reg')
33/53:
import seaborn as sns

# visualize the relationship between the features and the response using scatterplots
sns.pairplot(data, x_vars=['MW','LogP','NumHDonors','NumHAcceptors','NumRotatableBonds'], y_vars='pIC50', height=7, aspect=0.7, kind='reg')
33/54:
import seaborn as sns

# visualize the relationship between the features and the response using scatterplots
sns.pairplot(data, x_vars=['MW','LogP','NumHDonors','NumHAcceptors','NumRotatableBonds'], y_vars='pIC50', height=4, aspect=0.7, kind='reg')
33/55:
import seaborn as sns

# visualize the relationship between the features and the response using scatterplots
sns.pairplot(data, x_vars=['MW','LogP','NumHDonors','NumHAcceptors','NumRotatableBonds'], y_vars='pIC50', height=4, kind='reg')
33/56:
import seaborn as sns

# visualize the relationship between the features and the response using scatterplots
sns.pairplot(data, x_vars=['MW','LogP','NumHDonors','NumHAcceptors','NumRotatableBonds'], y_vars='pIC50', height=4, aspect=1, kind='reg')
33/57:
import seaborn as sns

# visualize the relationship between the features and the response using scatterplots
g = sns.pairplot(data, x_vars=['MW','LogP','NumHDonors','NumHAcceptors','NumRotatableBonds'], y_vars='pIC50', height=4, aspect=1, kind='reg')
g.fig.set_size_inches9,9)
33/58:
import seaborn as sns

# visualize the relationship between the features and the response using scatterplots
g = sns.pairplot(data, x_vars=['MW','LogP','NumHDonors','NumHAcceptors','NumRotatableBonds'], y_vars='pIC50', height=4, aspect=1, kind='reg')
g.fig.set_size_inches(9,9)
33/59: sns.pairplot(df3, x_vars=['PubchemFP0', 'PubchemFP1'], y_vars='pIC50', height=4, kind='reg')
33/60:
import seaborn as sns

# visualize the relationship between the features and the response using scatterplots
g = sns.pairplot(data, x_vars=['MW','LogP','NumHDonors','NumHAcceptors','NumRotatableBonds'], y_vars='pIC50', height=4, aspect=1, kind='reg')
g.fig.set_size_inches(5,5)
33/61:
import seaborn as sns

# visualize the relationship between the features and the response using scatterplots
sns.pairplot(data, x_vars=['MW','LogP','NumHDonors','NumHAcceptors','NumRotatableBonds'], y_vars='pIC50', height=4, aspect=1, kind='reg')
39/1:
Years = [2018,2019,2020,2021]
Per_Biologic_products = [21.1, 29.6, 39.2, 39.2]
 df = pd.Series(Per_Biologic_products, index=Years)
39/2:
Years = [2018,2019,2020,2021]
Per_Biologic_products = [21.1, 29.6, 39.2, 39.2]
df = pd.Series(Per_Biologic_products, index=Years)
39/3:
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
Years = [2018,2019,2020,2021]
Per_Biologic_products = [21.1, 29.6, 39.2, 39.2]
df = pd.Series(Per_Biologic_products, index=Years)
39/4: df
39/5: data = {'Name':'Tarapong', 'Class':'New', 'Class2':'New2', 'Class3':'New3'}
39/6: df
39/7: data
39/8:
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
Years = [2018,2019,2020,2021]
Per_Biologic_products = [21.1, 29.6, 39.2, 39.2]
data = list(zip(Years, Per_Biologic_products))
cols = ['Year', 'Percent Biologi products']
df = pd.DataFrame(data, columns=cols)
39/9: df
39/10: plt.bar(df)
39/11: plt.bar(data)
39/12:
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
Years = [2018,2019,2020,2021]
Per_Biologic_products = [21.1, 29.6, 39.2, 39.2]
data = list(zip(Years, Per_Biologic_products))
cols = ['Year', 'Percent Biologic products']
df = pd.DataFrame(data, columns=cols)
39/13: df
39/14:
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
Years = [2018,2019,2020,2021]
Per_Biologic_products = [21.1, 29.6, 39.2, 39.2]
data = list(zip(Years, Per_Biologic_products))
cols = ['Year', '%Biologic products']
df = pd.DataFrame(data, columns=cols)
39/15: df
39/16: plt.bar(data['%Biologic products'])
39/17: plt.bar(data['%Biologic products'], height=Year)
39/18: plt.hist(data['%Biologic products'], height=Year)
39/19: plt.hist(data['%Biologic products'])
39/20:
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
Years = [2018,2019,2020,2021]
Per_Biologic_products = [21.1, 29.6, 39.2, 39.2]
data = list(zip(Years, Per_Biologic_products))
cols = ['Year', '%Biologic products']
df = pd.DataFrame(data, columns=cols)
39/21:
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
Years = [2018,2019,2020,2021]
Per_Biologic_products = [21.1, 29.6, 39.2, 39.2]
data = list(zip(Years, Per_Biologic_products))
cols = ['Year', '%Biologic products']
df = pd.DataFrame(data, columns=cols)
39/22: df
39/23: plt.hist(data['%Biologic products'])
39/24: type(df)
39/25: df
39/26:
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
Years = [2018,2019,2020,2021]
Per_Biologic_products = [21.1, 29.6, 39.2, 39.2]
data = list(zip(Years, Per_Biologic_products))
cols = ['Year', 'Biologic products']
df = pd.DataFrame(data, columns=cols)
39/27: df
39/28: plt.hist(data['Biologic products'])
39/29:
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
Years = [2018,2019,2020,2021]
Per_Biologic_products = [21.1, 29.6, 39.2, 39.2]
data = list(zip(Years, Per_Biologic_products))
cols = ['Year', '%Biologic products']
df = pd.DataFrame(data, columns=cols)
39/30: df|
39/31: df
39/32: plt.hist(data['%Biologic products'])
39/33: plt.bar(data['%Biologic products'])
39/34: plt.bar(df['%Biologic products'])
39/35: plt.bar(df['%Biologic products'], height=Year)
39/36: plt.bar(df['%Biologic products'], height=df['Year'])
39/37: plt.bar(df['Year'], height=df['%Biologic products'])
39/38: plt.scatter(df['Year'], height=df['%Biologic products'])
39/39:
X=df['Year']
Y=df['%Biologic products']
plt.scatter(X, Y)
39/40:
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
Years = ['2018','2019','2020','2021']
Per_Biologic_products = [21.1, 29.6, 39.2, 39.2]
data = list(zip(Years, Per_Biologic_products))
cols = ['Year', '%Biologic products']
df = pd.DataFrame(data, columns=cols)
39/41: df
39/42: plt.bar(df['Year'], height=df['%Biologic products'])
39/43:
X=df['Year']
Y=df['%Biologic products']
plt.scatter(X, Y)
39/44: plt.bar(df['Year'], height=df['%Biologic products'], color='#3EC1D3', edgecolor='black', linewidth=0.5)
39/45:
plt.bar(df['Year'], height=df['%Biologic products'], color='#3EC1D3', edgecolor='black', linewidth=0.5)
plt.annotate(df['%Biologic products'])
39/46:
plt.bar(df['Year'], height=df['%Biologic products'], color='#3EC1D3', edgecolor='black', linewidth=0.5)
plt.annotate(df['%Biologic products'], xy)
51/1: pip install ocv
51/2: print('heeoo')
51/3: py -m pip install opencv-python
51/4: !pip install opencv-python
51/5: !pip list
51/6:
import numoy as np
import opencv as cv
51/7:
import numpy as np
import opencv as cv
51/8:
import numpy as np
import opencv-python as cv
51/9:
import numpy as np
import cv2
51/10:
CLASSES = ["BACKGROUND", "AEROPLANE", "BICYCLE", "BIRD", "BOAT",
    "BOTTLE", "BUS", "CAR", "CAT", "CHAIR", "COW", "DININGTABLE",
    "DOG", "HORSE", "MOTORBIKE", "PERSON", "POTTEDPLANT", "SHEEP",
    "SOFA", "TRAIN", "TVMONITOR"]
54/1:
#We will use Chembl database for molecular modeling
#install package dependencies
!pip install requests_cache==0.6.4
#Install Chemabl database
!pip install chembl_webresource_client
54/2:
#import important library
import pandas as pd
import numpy as np
%matplotlib incline
import matplotlib.pyplot as plt
import rdkit
54/3:
#import important library
import pandas as pd
import numpy as np
%matplotlib inline
import matplotlib.pyplot as plt
import rdkit
54/4:
#Chembl API access...
from chembl_webresource_client.new_client import new_client
target = new_client.target
#Search target protein, we will use LAT1 as a target protein
target_query = target.search('Papp')
targets = pd.DataFrame.from_dict(target_query)
targets
54/5:
#Chembl API access...
from chembl_webresource_client.new_client import new_client
target = new_client.target
#Search target protein, we will use LAT1 as a target protein
target_query = target.search('ADMET')
targets = pd.DataFrame.from_dict(target_query)
targets
54/6:
#Chembl API access...
from chembl_webresource_client.new_client import new_client
target = new_client.target
#Search target protein, we will use LAT1 as a target protein
target_query = target.search('Drug uptake')
targets = pd.DataFrame.from_dict(target_query)
targets
54/7: targets.tail(6)
54/8: df = pd.read_csv('Rawdata.csv')
54/9: df = pd.read_csv("Rawdata.csv")
54/10: df = pd.read_csv("rawdata.csv")
54/11:
rawdata = 'rawdata.csv'
df = pd.read_csv("rawdata.csv")
54/12: rawdata = 'rawdata.csv'
54/13: rawdata
54/14: rawdata = '\rawdata.csv'
54/15: rawdata
54/16: rawdata = 'rawdata.csv'
54/17: rawdata = "rawdata.csv"
54/18: rawdata
54/19: rawdata = "rawdata.csv"
54/20: rawdata
54/21: df = pd.read_csv(rawdata)
54/22: df
54/23: df.head
54/24: df.columns
54/25:
selection = ['Molecule ChEMBL ID', '#RO5 Violations', 'Smiles', 'Standard Value', 'Standard Units', 'Target Organism']
df2 = df[selection]
54/26: df2
54/27:
#Chembl API access...
from chembl_webresource_client.new_client import new_client
target = new_client.target
#Search target protein, we will use LAT1 as a target protein
target_query = Activities.search('Drug uptake')
targets = pd.DataFrame.from_dict(target_query)
targets
54/28:
#Chembl API access...
from chembl_webresource_client.new_client import new_client
target = new_client.target
#Search target protein, we will use LAT1 as a target protein
target_query = target.search('Drug uptake')
targets = pd.DataFrame.from_dict(target_query)
targets
54/29:
#Chembl API access...
from chembl_webresource_client.new_client import new_client
target = new_client.target
#Search target protein, we will use LAT1 as a target protein
target_query = target.search('CHEMBL612558')
targets = pd.DataFrame.from_dict(target_query)
targets
54/30:
from chembl_webresource_client import *

assays = AssayResource()
targets = TargetResource()
compounds = CompoundResource()

# Once we have our resources, getting activities is easy:

bs = assays.bioactivities('CHEMBL1217643')
len(bs)
1
print bs
54/31:
from chembl_webresource_client import *

assays = AssayResource()
targets = TargetResource()
compounds = CompoundResource()

# Once we have our resources, getting activities is easy:

bs = assays.bioactivities('CHEMBL1217643')
len(bs)
1
print(bs)
54/32:
from chembl_webresource_client import *

assays = AssayResource()
targets = TargetResource()
compounds = CompoundResource()

# Once we have our resources, getting activities is easy:

bs = assays.bioactivities('CHEMBL1217643')
len(bs)
1
print(bs)
54/33:
selection = ['Molecule ChEMBL ID', 'Smiles', 'Standard Value', 'Standard Units', 'Assay Organism']
df2 = df[selection]
54/34: df2
54/35: df2.columns = ['molecule_chembl_id', 'canonical_smiles', 'standard_value', 'standard_units', 'assay_organism']
54/36: df2
54/37: df2.standard_units.unique()
54/38: df2.standard_units.str.match('pmol/min/mg')
54/39: df2.standard_units.str.match('pmol/min/mg', case=False)
54/40: df3 = df2[df2["standard_units"] = 'pmol/min/mg']
54/41: df3 = df2[df2["standard_units"]=='pmol/min/mg']
54/42: df3
54/43: df3 = df2[df2["assay_organism"]=='Homo sapiens']
54/44: df3
54/45: df3.drop(NA)
54/46: df3.drop('NaN')
54/47: df3.drop(labels='NaN', axis=0, inplace=False)
54/48: df3.to_csv('df.csv')
54/49:
df4 = df3[df.canonical_smiles.notna()]
df4
54/50:
df4 = df2[df.canonical_smiles.notna()]
df4
54/51:
df4 = df3[df3.canonical_smiles.notna()]
df4
54/52: df5 = df4[df4.standard_value.notna()]
54/53:
df5 = df4[df4.standard_value.notna()]
df5
54/54: df6 = df5[df5.standard_units.notna()]
54/55:
df6 = df5[df5.standard_units.notna()]
df6
54/56:
df6.drop_duplicates(subset ="canonical_smiles",
                     keep = False, inplace = True)
54/57:
df6.drop_duplicates(subset ="canonical_smiles",
                     keep = False, inplace = True)
df6
54/58:
df6 = df5[df5.standard_units.notna()]
df6
57/1:
#Chembl API access...
from chembl_webresource_client.new_client import new_client
target = new_client.target
#Search target protein, we will use LAT1 as a target protein
target_query = target.search('LAT1')
targets = pd.DataFrame.from_dict(target_query)
targets
57/2:
#We will use Chembl database for molecular modeling
#install package dependencies
!pip install requests_cache==0.6.4
#Install Chemabl database
!pip install chembl_webresource_client
57/3:
#import important library
import pandas as pd
import numpy as np
%matplotlib inline
import matplotlib.pyplot as plt
import rdkit
57/4:
#Chembl API access...
from chembl_webresource_client.new_client import new_client
target = new_client.target
#Search target protein, we will use LAT1 as a target protein
target_query = target.search('LAT1')
targets = pd.DataFrame.from_dict(target_query)
targets
57/5:
#Chembl API access...
from chembl_webresource_client.new_client import new_client
target = new_client.target
#Search target protein, we will use LAT1 as a target protein
target_query = target.search('Drug uptake')
targets = pd.DataFrame.from_dict(target_query)
targets
57/6:
#Chembl API access...
from chembl_webresource_client.new_client import new_client
target = new_client.target
#Search target protein, we will use LAT1 as a target protein
target_query = target.search('Drug uptake')
targets = pd.DataFrame.from_dict(target_query)
targets
57/7:
#As we can see, we will select H.sapiens LAT1 as atarget
selected_target = targets.target_chembl_id[1]
selected_target
57/8:
#Here we will access the activity of compounds in this target, first we will try IC50
#IC50 is tell us about the potency of the compound that can reduce LAT1 activity for 50%
activity = new_client.activity
res = activity.filter(target_chembl_id=selected_target))
57/9:
#Here we will access the activity of compounds in this target, first we will try IC50
#IC50 is tell us about the potency of the compound that can reduce LAT1 activity for 50%
activity = new_client.activity
res = activity.filter(target_chembl_id=selected_target)
57/10:
#Here we will access the activity of compounds in this target, first we will try IC50
#IC50 is tell us about the potency of the compound that can reduce LAT1 activity for 50%
activity = new_client.activity
res = activity.filter(target_chembl_id=selected_target)
res
57/11:
#Here we will access the activity of compounds in this target, first we will try IC50
#IC50 is tell us about the potency of the compound that can reduce LAT1 activity for 50%
activity = new_client.activity
res = activity.filter(target_chembl_id=selected_target)
res1 = pd.DataFrame.from_dict(res)
res1
59/1:
import keras
import matplotlib.pyplot as plt
59/2:
import keras
import matplotlib.pyplot as plt
59/3:
import keras
import matplotlib.pyplot as plt
59/4: keras.backend
59/5: keras.backend.backend()
59/6: from keras import fashion_mnist, mnist
59/7: from keras import fashion_mnist. mnist
59/8: from keras import fashion_mnist.mnist
60/1:
import keras
import matplotlib.pyplot as plt
60/2: keras.backend.backend()
60/3: from keras.datasets import fashion_mnist, mnist
60/4: (x_train, y_train), (x_test,y_test) = fashion_mnist.load_data()
60/5: x_train.shape
60/6: y_train.shape
60/7: x_test.shape
60/8: y_test.shape
60/9: plt.imshow(x_train[0], cmap = plt.cm.binary)
60/10: x_train[0]
60/11:
x_train = x_train/255
x_test = x_test/255
60/12: x_train[0]
60/13:
x_train = x_train/255
x_test = x_test/255
60/14: x_train[0]
60/15:
from keras.models import Sequential
from keras.layers import Flatten, Dense
60/16:
model = Sequential()
model.add(Flatten(input_shape=[28,28]))
model.add(Dense(128, activation="relu"))
model.add(Dense(128, activation="relu"))
model.add(Dense(10, activation="softmax"))
60/17: model.summary()
60/18:
model.compile(loss="sparse_categorical_crossentropy",
             optimizer="adam",
             metrics=["accuracy"])
60/19:
from keras.models import Sequential
from keras.layers import Flatten, Dense
62/1:
import matplotlib.pyplot as plt
import numpy as np

from sklearn.linear_model import LinearRegression
62/2:
x_data = [1.0, 1.8, 3.0, 4.1, 5.2, 6.0]
y_data = [1, 1.3, 2.2, 2.5, 2.8, 3.6]

x = np.array(x_data)
y = np.array(y_data)
62/3: x
62/4: y
62/5:
plt.scatter(x, y)
plt.grid()
plt.show()
62/6:
# ws02
x = x.reshape(-1, 1)
x
62/7:
y = y.reshape(-1, 1)
y
62/8:
m = (3.45-1) / (6-1)
# b = 0.5
b = 3.45 - m * 6

x_input = [2.0, 2.5, 3, 5.0]

for k in x_input:
#     print(k*2)
#     print(type(k))
    y2 = m * k + b
    print('x={:3.1f} y={:4.2f} '.format(k, y2))
62/9:
y = y.reshape(-1, 1)
y
62/10:
from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(x, y)
62/11: model.predict([[2.5]])
62/12:
x_input = [[2.0],
           [2.5],
           [3],
           [5.0]] 

# x_input = [[6.0]]

y_predict = model.predict(x_input)
y_predict[1][0]
y_predict[2][0]
62/13:
x_input = [[2.0],
           [2.5],
           [3],
           [5.0]] 

# x_input = [[6.0]]

y_predict = model.predict(x_input)
62/14:
x_input = [[2.0],
           [2.5],
           [3],
           [5.0]] 

# x_input = [[6.0]]

y_predict = model.predict(x_input)
y_predict
62/15: y_predict[1][0]
62/16: y_predict[0][0]
62/17:
x_input = [2.0, 2.5, 3, 5.0]
x_input = np.array(x_input).reshape(-1, 1)

model.predict(x_input)
62/18:
y_predict = model.predict(x_input)
y_predict
62/19:
output_predict = y_predict.reshape(1, -1)[0]

i = 0
for k in x_input.reshape(1, -1)[0]:
#     print(type(k))
    print('x={} y={:.5f} '.format(k, output_predict[i]) )
    i += 1
62/20:
y2_predict = y_predict.reshape(1, -1)

x_input_list = x_input
# print(x_input_list)    

for i, item in enumerate(x_input_list):
#     print(type(k))
    print('x={} y={:.5f} '.format(item[0], y2_predict[0][i]) )
62/21:
y2_predict = y_predict.reshape(1, -1)

x_input_list = x_input
# print(x_input_list)    

for i, item in enumerate(x_input_list):
#     print(type(k))
    print('x={} y={:.5f} '.format(item, y2_predict) )
62/22:
y2_predict = y_predict.reshape(1, -1)

x_input_list = x_input
# print(x_input_list)    

for i, item in enumerate(x_input_list):
#     print(type(k))
    print('x={} y={:.5f} '.format(item[0], y2_predict) )
62/23:
y2_predict = y_predict.reshape(1, -1)

x_input_list = x_input
# print(x_input_list)    

for i, item in enumerate(x_input_list):
#     print(type(k))
    print('x={} y={:.5f} '.format(item[0], y2_predict[0]) )
62/24:
y2_predict = y_predict.reshape(1, -1)

x_input_list = x_input
# print(x_input_list)    

for i, item in enumerate(x_input_list):
#     print(type(k))
    print('x={} y={:.5f} '.format(item[0], y2_predict[0][i]) )
58/1:
import pandas as pd
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
63/1: import rdkit
63/2: from rdkit import Chem
63/3:
suppl = Chem.SDMolSupplier('molecules.sdf')
for mol in suppl:
print(mol.GetNumAtoms())
63/4:
suppl = Chem.SDMolSupplier('molecules.sdf')
for mol in suppl:
    print(mol.GetNumAtoms())
63/5:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
63/6: smiles_list = ['OC1=CC=C2OCOC2=C1', 'C1C2C(COC2C3=CC4=C(C=C3)OCO4)C(O1)C5=CC6=C(C=C5)OCO6', 'C1C2C(COC2OC3=CC4=C(C=C3)OCO4)C(O1)C5=CC6=C(C=C5)OCO6']
63/7: smiles_list
63/8:
smiles_list
for smiles in smiles_list:
    mol = Chem.MolFromSmiles(smiles)
    mol_list.append(mol)
img = Draw.MolsToGridImage(mol_list)
img
63/9:
smiles_list
for smiles in smiles_list:
    mol = Chem.MolFromSmiles(smiles)
    mol_;ist = []
    mol_list.append(mol)
img = Draw.MolsToGridImage(mol_list)
img
63/10:
smiles_list
for smiles in smiles_list:
    mol = Chem.MolFromSmiles(smiles)
    mol_ist = []
    mol_list.append(mol)
img = Draw.MolsToGridImage(mol_list)
img
63/11:
smiles_list
for smiles in smiles_list:
    mol = Chem.MolFromSmiles(smiles)
    mol_list = []
    mol_list.append(mol)
img = Draw.MolsToGridImage(mol_list)
img
63/12:
smiles_list
for smiles in smiles_list:
    mol = Chem.MolFromSmiles(smiles)
    mol_list = []
    mol_list.append(mol)
img = Draw.MolsToGridImage(mol_list, Molsperrow = 4)
img
63/13:
smiles_list
for smiles in smiles_list:
    mol = Chem.MolFromSmiles(smiles)
    mol_list = []
    mol_list.append(mol)
img = Draw.MolsToGridImage(mol_list, molsPerRow = 4)
img
63/14:
smiles_list
for smiles in smiles_list:
    mol = Chem.MolFromSmiles(smiles)
    mol_list.append(mol)
img = Draw.MolsToGridImage(mol_list, molsPerRow = 4)
img
63/15:
smiles_list
for smiles in smiles_list:
    mol = Chem.MolFromSmiles(smiles)
    mol_list.append(mol)
img = Draw.MolsToGridImage(mol_list, molsPerRow = 4)
img
63/16:
smiles_list
for smiles in smiles_list:
    mol = Chem.MolFromSmiles(smiles)
    mol_list.append(mol)
img = Draw.MolsToGridImage(mol_list, molsPerRow = 4)
img
63/17:
smiles_list
for smiles in smiles_list:
    mol = Chem.MolFromSmiles(smiles)
    mol_list.append(mol)
img = Draw.MolsToGridImage(mol_list)
img
63/18:
smiles_list
for smiles in smiles_list:
    mol = Chem.MolFromSmiles(smiles)
    mol_list.append(mol)
img = Draw.MolsToGridImage(mol_list)
mol_list
63/19:
smiles_list
for smiles in smiles_list:
    mol = Chem.MolFromSmiles(smiles)
    mol_list.append(mol)
img = Draw.MolsToGridImage(mol_list)
img
63/20:
smiles_list
for smiles in smiles_list:
    mol = Chem.MolFromSmiles(smiles)
    mol_list.append(mol)
img = Draw.MolsToGridImage(mol_list)
img
mw = Descriptors.MolWt(mol_list)
63/21:
smiles_list
for smiles in smiles_list:
    mol = Chem.MolFromSmiles(smiles)
    mol_list.append(mol)
img = Draw.MolsToGridImage(mol_list)
img
mw = Descriptors.MolWt(mol)
63/22: mw
63/23:
smiles_list
for smiles in smiles_list:
    mol = Chem.MolFromSmiles(smiles)
    mol_list.append(mol)
img = Draw.MolsToGridImage(mol_list)
img
mw = Descriptors.MolWt(smiles_list)
63/24:
smiles_list
for smiles in smiles_list:
    mol = Chem.MolFromSmiles(smiles)
    mol_list.append(mol)
img = Draw.MolsToGridImage(mol_list)
img
63/25:
mol_list = list(dict.fromkeys(mol_list))
print(mol_list)
63/26:
mol_list = list(dict.fromkeys(mol_list))
print(mol_list)
63/27: mol_list
63/28:
smiles_list
for smiles in smiles_list:
    mol = Chem.MolFromSmiles(smiles)
    mol_list.append(mol)
img = Draw.MolsToGridImage(mol_list,)
img
63/29:
smiles_list
for smiles in smiles_list:
    mol = Chem.MolFromSmiles(smiles)
    mols.append(mol)
img = Draw.MolsToGridImage(mols)
img
63/30:
smiles_list
for smiles in smiles_list:
    mol = Chem.MolFromSmiles(smiles)
    mols.append(mol)
img = Draw.MolsToGridImage(mols)
img
63/31:
smiles_list
for smiles in smiles_list:
    mol = Chem.MolFromSmiles(smiles)
    mols.append(mol)
img = Draw.MolsToGridImage(mols)
img
63/32:
smiles_list
for smiles in smiles_list:
    mol = Chem.MolFromSmiles(smiles)
    mols.append(mol)
img = Draw.MolsToGridImage(mols)
img
63/33:
smiles_list
for smiles in smiles_list:
    mol = Chem.MolFromSmiles(smiles)
    
    mol_list.append(mol)
img = Draw.MolsToGridImage(mols)
img
63/34:
smiles_list
for smiles in smiles_list:
    mol = Chem.MolFromSmiles(smiles)
    
    mol_list.append(mol)
img = Draw.MolsToGridImage(mol_list)
img
63/35: smiles_list = ['OC1=CC=C2OCOC2=C1', 'C1C2C(COC2C3=CC4=C(C=C3)OCO4)C(O1)C5=CC6=C(C=C5)OCO6', 'C1C2C(COC2OC3=CC4=C(C=C3)OCO4)C(O1)C5=CC6=C(C=C5)OCO6']
63/36:
for smiles in smiles_list:
    mol = Chem.MolFromSmiles(smiles)
    mol_list.append(mol)
img = Draw.MolsToGridImage(mol_list)
img
63/37:
for smiles in smiles_list:
    mol = Chem.MolFromSmiles(smiles)
    mols_list.append(mol)
img = Draw.MolsToGridImage(mols_list)
img
63/38:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
63/39:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
63/40:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
63/41: df = pdf.read_csv('rawcorrelation.csv')
63/42: df = pd.read_csv('rawcorrelation.csv')
63/43: df
63/44: df2 = df.drop([1])
63/45: df2
63/46: df2 = df.drop([0][1])
63/47: df2 = df.drop([:1][0])
63/48: df2 = df.drop([1][0])
63/49: df2
63/50: df2
63/51: df2 = df.drop(columns=['Unnamed: 0'])
63/52: df2
63/53: scipy.stats.linregress(df2)
63/54: M = df2['CM_SK_37'].to_numpy()
63/55: M
63/56: X = df2['CM_SK_37', 'CM_SK_4 '].to_numpy()
63/57: X = df2['CM_SK_37', 'CM_SK_4'].to_numpy()
63/58: X = df2['CM_SK_37','CM_SK_4'].to_numpy()
63/59: df2.corr(method='pearson')
63/60:
Res_pearson = df2.corr(method='pearson')
Res_pearson
63/61:
from scipy.stats import pearsonr
Res_pearson = df2.corr(method='pearson')
Res_pearson
rho = df2.corr()
pval = df2.corr(method=lambda x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)
p = pval.applymap(lambda x: ''.join(['*' for t in [0.01,0.05,0.1] if x<=t]))
rho.round(2).astype(str) + p
63/62:
from scipy.stats import pearsonr
Res_pearson = df2.corr(method='pearson')
Res_pearson
rho = df2.corr()
pval = df2.corr(method=lambda x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)
p = pval.applymap(lambda x: ''.join(['*' for t in [0.01,0.05,0.1] if x<=t]))
rho.round(4).astype(str) + p
63/63:
from scipy.stats import pearsonr
Res_pearson = df2.corr(method='pearson')
Res_pearson
rho = df2.corr()
pval = df2.corr(method=lambda x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)
p = pval.applymap(lambda x: ''.join(['*' for t in [0.01,0.05,0.1] if x<=t]))
rho.round(3).astype(str) + p
63/64:
from scipy.stats import pearsonr
rho = df2.corr()
pval = df2.corr(method=lambda x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)
p = pval.applymap(lambda x: ''.join(['*' for t in [0.01,0.05,0.1] if x<=t]))
rho.round(3).astype(str) + p
63/65:
Res_pearson = df2.corr(method='pearson')
Res_pearson
63/66:
Res_pearson = df2.corr(method='pearson')
Res_pearson.round(3)
63/67:
from scipy.stats import pearsonr
rho = df2.corr()
pval = df2.corr(method=lambda x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)
p = pval.applymap(lambda x: ''.join(['*' for t in [0.01,0.05,0.1] if x<=t]))
Res_corre_sig = rho.round(3).astype(str) + p
Res_corre_sig
63/68:
Res_pearson = df2.corr(method='pearson').round(3)
Res_pearson
63/69:
from scipy.stats import pearsonr
rho = df2.corr()
pval = df2.corr(method=lambda x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)
p = pval.applymap(lambda x: ''.join(['*' for t in [0.01,0.05,0.1] if x<=t]))
Res_pearson_sig = rho.round(3).astype(str) + p
Res_pearson_sig
Res_pearson_sig.to_csv('Res_pearson_sig.csv')
63/70:
from scipy.stats import pearsonr
rho = df2.corr()
pval = df2.corr(method=lambda x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)
p = pval.applymap(lambda x: ''.join(['*' for t in [0.01,0.05,0.1] if x<=t]))
Res_pearson_sig = rho.round(3).astype(str) + p
Res_pearson_sig.to_csv('Res_pearson_sig.csv')
Res_pearson_sig
63/71:
Res_pearson = df2.corr(method='pearson').round(3)
Res_pearson.to_csv('Res_pearson.csv')
Res_pearson
63/72: sns.heatmap(Res_pearson, annot=True, cmap='YlGnBu')
63/73: sns.heatmap(Res_pearson, annot=True, cmap='YlGnBu', fmt='.2f')
63/74: sns.heatmap(Res_pearson, annot=True, cmap='coolwarm', fmt='.2f')
63/75:
fig, ax = plt.subplots(figsize=(10,10)) 
ax = sns.heatmap(Res_pearson, annot=True, cmap='coolwarm', fmt='.2f')
63/76:
fig, ax = plt.subplots(figsize=(10,10)) 
ax = sns.heatmap(Res_pearson, annot=True, cmap='coolwarm', fmt='.3f')
63/77: df = pd.read_csv('rawcorrelation.csv')
63/78: df
63/79: df2 = df.drop(columns=['Unnamed: 0'])
63/80: df2
63/81:
from scipy.stats import pearsonr
rho = df2.corr()
pval = df2.corr(method=lambda x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)
p = pval.applymap(lambda x: ''.join(['*' for t in [0.01,0.05,0.1] if x<=t]))
Res_pearson_sig = rho.round(3).astype(str) + p
Res_pearson_sig.to_csv('Res_pearson_sig.csv')
Res_pearson_sig
63/82: df = pd.read_csv('rawcorrelation.csv')
63/83: df
63/84: df2 = df.drop(columns=['Unnamed: 0'])
63/85: df2
63/86:
from scipy.stats import pearsonr
rho = df2.corr()
pval = df2.corr(method=lambda x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)
p = pval.applymap(lambda x: ''.join(['*' for t in [0.01,0.05,0.1] if x<=t]))
Res_pearson_sig = rho.round(3).astype(str) + p
Res_pearson_sig.to_csv('Res_pearson_sig.csv')
Res_pearson_sig
63/87:
Res_pearson = df2.corr(method='pearson').round(3)
Res_pearson.to_csv('Res_pearson.csv')
Res_pearson
63/88:
fig, ax = plt.subplots(figsize=(10,10)) 
ax = sns.heatmap(Res_pearson, annot=True, cmap='coolwarm', fmt='.3f')
63/89:
fig, ax = plt.subplots(figsize=(10,10)) 
ax = sns.heatmap(Res_pearson, annot=True, cmap='coolwarm', fmt='.2f')
63/90:
fig, ax = plt.subplots(figsize=(10,10)) 
ax = sns.heatmap(Res_pearson, annot=False, cmap='coolwarm', fmt='.2f')
63/91:
fig, ax = plt.subplots(figsize=(10,10)) 
ax = sns.heatmap(Res_pearson, annot=True, fmt='.2f')
63/92:
fig, ax = plt.subplots(figsize=(10,10)) 
ax = sns.heatmap(Res_pearson, annot=True, cmap='coolwarm', fmt='.2f')
63/93:
fig, ax = plt.subplots(figsize=(10,10)) 
ax = sns.heatmap(Res_pearson_sig, annot=False, cmap='coolwarm', fmt='.2f')
63/94:
fig, ax = plt.subplots(figsize=(10,10)) 
ax = sns.heatmap(Res_pearson, annot=False, cmap='viridis', fmt='.2f')
63/95:
fig, ax = plt.subplots(figsize=(10,10)) 
ax = sns.heatmap(Res_pearson, annot=True, cmap='coolwarm', fmt='.2f')
plt.tight_layout()
plt.savefig('LAT1 plots-pIC50 distribution.pdf', dpi=300)
63/96:
fig, ax = plt.subplots(figsize=(10,10)) 
ax = sns.heatmap(Res_pearson, annot=True, cmap='coolwarm', fmt='.2f')
plt.tight_layout()
plt.savefig('corr_pearson_number.pdf', dpi=300)
63/97:
fig, ax = plt.subplots(figsize=(10,10)) 
ax = sns.heatmap(Res_pearson, annot=True, cmap='coolwarm', fmt='.2f')
plt.tight_layout()
plt.savefig('corr_pearson_number.pdf', dpi=300)
63/98:
fig, ax = plt.subplots(figsize=(10,10)) 
ax = sns.heatmap(Res_pearson, annot=False, cmap='coolwarm', fmt='.2f')
plt.tight_layout()
plt.savefig('corr_pearson.pdf', dpi=300)
63/99:
fig, ax = plt.subplots(figsize=(10,10)) 
ax = sns.heatmap(Res_pearson, annot=False, cmap='icefire', fmt='.2f')
63/100:
fig, ax = plt.subplots(figsize=(10,10)) 
ax = sns.heatmap(Res_pearson, annot=False, cmap='dark', fmt='.2f')
63/101:
fig, ax = plt.subplots(figsize=(10,10)) 
ax = sns.heatmap(Res_pearson, annot=False, cmap='dark', fmt='.2f')
63/102:
fig, ax = plt.subplots(figsize=(10,10)) 
ax = sns.heatmap(Res_pearson, annot=False, cmap='Spectral', fmt='.2f')
63/103:
fig, ax = plt.subplots(figsize=(10,10)) 
ax = sns.heatmap(Res_pearson, annot=False, cmap='Spectral', fmt='.2f')
plt.tight_layout()
plt.savefig('corr_pearson_spectral.pdf', dpi=300)
63/104:
fig, ax = plt.subplots(figsize=(10,10)) 
ax = sns.heatmap(Res_pearson, annot=True, cmap='Spectral', fmt='.2f')
plt.tight_layout()
plt.savefig('corr_pearson_spectral.pdf', dpi=300)
63/105:
fig, ax = plt.subplots(figsize=(10,10)) 
ax = sns.heatmap(Res_pearson, annot=True, cmap='Spectral', fmt='.2f')
plt.tight_layout()
plt.savefig('corr_pearson_spectral_number.pdf', dpi=300)
63/106:
fig, ax = plt.subplots(figsize=(10,10)) 
ax = sns.heatmap(Res_pearson, annot=False, cmap='Spectral', fmt='.2f')
plt.tight_layout()
plt.savefig('corr_pearson_spectral.pdf', dpi=300)
65/1:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
65/2:
#Nonparametic test
from scipy import stats
sesamol = [78.2, 75.3, 72.4]
sesamin = [60.8, 87.5, 84.9]
sesamolin = [66.4, 64.1, 67.1]

stats.kruskal(sesamol, sesamin, sesamolin)
65/3:
#Nonparametic test
from scipy import stats
sesamol = [78.2, 75.3, 72.4]
sesamin = [60.8, 87.5, 84.9]
sesamolin = [66.4, 64.1, 67.1]

stats.shapiro(sesamol, sesamin, sesamolin)
65/4:
#Nonparametic test
from scipy import stats
sesamol = [78.2, 75.3, 72.4]
sesamin = [60.8, 87.5, 84.9]
sesamolin = [66.4, 64.1, 67.1]

stats.shapiro(sesamol)
65/5: stats.shapiro(sesamin)
65/6: stats.shapiro(sesamolin)
65/7:
from scipy import f_oneway
f_oneway(sesamol,sesamin,sesamolin)
65/8:
from scipy.stats import f_oneway
f_oneway(sesamol,sesamin,sesamolin)
69/1:
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import curve_fit
69/2:
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import minimize
69/3:
def func(s, v_max, k_m, p_d):
    return (v_max * s) / (k_m + s) + p_d*s
69/4:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.optimize import curve_fit
69/5: df = pd.read_csv('Papp_SM_Ve_37.csv')
69/6:
df = pd.read_csv('Papp_SM_Ve_37.csv')
df
69/7:
x_train = df['S']
y_train = df['n1']
69/8:
x_train = df['S']
y_train = df['n1']
69/9: x_train
69/10:
popt, pcov = curve_fit(func, x_train, y_train)
popt
69/11:
plt.plot(xdata, func(x_train, *popt), 'r-',

         label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))
69/12:
plt.plot(x_train, func(x_train, *popt), 'r-',

         label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))
69/13:
def loss(theta):
    v_max, k_m = theta
    v_pred = v(x_train, v_max, k_m)
    return np.sum((y_train - y_pred)**2)
69/14:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.optimize import minimize
69/15:
res = minimize(loss, [1, 1])
res.x
69/16:
def loss(theta):
    v_max, k_m = theta
    y_pred = func(x_train, v_max, k_m)
    return np.sum((y_train - y_pred)**2)
69/17:
res = minimize(loss, [1, 1])
res.x
69/18:
def loss(theta):
    v_max, k_m = theta
    y_pred = func(x_train, v_max, k_m, p_d)
    return np.sum((y_train - y_pred)**2)
69/19:
res = minimize(loss, [1, 1])
res.x
69/20:
def loss(theta):
    v_max, k_m, p_d = theta
    y_pred = func(x_train, v_max, k_m, p_d)
    return np.sum((y_train - y_pred)**2)
69/21:
res = minimize(loss, [1, 1])
res.x
69/22:
res = minimize(loss, [1, 1, 1])
res.x
69/23: plt.scatter(x_train, y_train)
69/24:
x_train = df['S']
y_train = df['Sesamol']
69/25:
def loss(theta):
    v_max, k_m, p_d = theta
    y_pred = func(x_train, v_max, k_m, p_d)
    return np.sum((y_train - y_pred)**2)
69/26:
res = minimize(loss, [1, 1, 1])
res.x
69/27: plt.scatter(x_train, y_train)
69/28:
x_train = df['S']
y_train = df['n1']
69/29:
def loss(theta):
    v_max, k_m, p_d = theta
    y_pred = func(x_train, v_max, k_m, p_d)
    return np.sum((y_train - y_pred)**2)
69/30:
popt, pcov = curve_fit(func, x_train, y_train)
popt
69/31: plt.scatter(x_train, y_train)
69/32:
plt.scatter(x_train, y_train)
plt.plot(x_train, func(x_train, *popt)
69/33:
plt.scatter(x_train, y_train)
plt.plot(x_train, func(x_train, *popt), 'r-',

         label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))
69/34:
plt.scatter(x_train, y_train)
plt.plot(x_train, func(x_train, *popt))
69/35:
plt.scatter(x_train, y_train)
plt.plot(x_train, func(x_train, *popt), 'r-',

         label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))
69/36:
popt, pcov = curve_fit(func, x_train, y_train, bounds=(0, [3., 1., 0.5]))
popt
69/37:
plt.plot(x_train, func(x_train, *popt), 'r-',

         label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))
69/38:
plt.xlabel('x')

plt.ylabel('y')

plt.legend()

plt.show()
69/39:
plt.plot(x_train, func(x_train, *popt), 'r-',

         label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))
plt.xlabel('x')

plt.ylabel('y')

plt.legend()

plt.show()
69/40:
plt.scatter(x_train, y_train)
plt.plot(x_train, func(x_train, *popt), 'r-',

         label='fit: a=%5.3f, b=%5.3f, c=%5.3f' % tuple(popt))
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.show()
70/1:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
70/2: df = pd.read_csv('rawdataqsar.csv')
70/3: df
70/4: df2 =df.notna()
70/5: df2
70/6: df = pd.read_csv('rawdataqsar.csv')
70/7: df
70/8: df2 =df[kpu.notna()]
70/9: df2
70/10: df2 =df['kpu'.notna()]
70/11: df2 =df[df.kpu.notna()]
70/12: df2
70/13: df2.notna()
70/14: df2.middle
70/15: df2.tail
70/16: df.dropna(subset = ["kpu"], inplace=True)
70/17: df2 =df.dropna(subset = ["kpu"], inplace=True)
70/18: df2
70/19: df2
70/20: df2 = df.dropna(subset = ["kpu"], inplace=True)
70/21: df2
70/22: df3 = df.dropna(subset = ["kpu"], inplace=True)
70/23: df3
70/24: df2|
70/25: df2 =df[df.kpu.notna()]
70/26: df2
70/27: df = pd.read_csv('rawdataqsar.csv')
70/28: df
70/29: df2 =df[df.kpu.notna()]
70/30: df2
70/31: df3 =df2[df.kp.notna()]
70/32: df3
70/33: df3.shape
70/34: df3.na()
70/35: df3.isnull()
70/36: df3.reset_index
70/37: df4=df3.reset_index()
70/38: df4
70/39: df4=df3.reset_index(drop=True)
70/40: df4
70/41:
from rdkit import Chem
from rdkit.Chem import Descriptors, Lipinski
70/42:
def lipinski(smiles, verbose=False):

    moldata= []
    for elem in smiles:
        mol=Chem.MolFromSmiles(elem) 
        moldata.append(mol)
       
    baseData= np.arange(1,1)
    i=0  
    for mol in moldata:        
       
        desc_MolWt = Descriptors.MolWt(mol)
        desc_MolLogP = Descriptors.MolLogP(mol)
        desc_NumHDonors = Lipinski.NumHDonors(mol)
        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)
        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)
           
        row = np.array([desc_MolWt,
                        desc_MolLogP,
                        desc_NumHDonors,
                        desc_NumHAcceptors,
                        desc_NumRotatableBonds])   
    
        if(i==0):
            baseData=row
        else:
            baseData=np.vstack([baseData, row])
        i=i+1      
    
    columnNames=["MW","LogP","NumHDonors","NumHAcceptors", "NumRotatableBonds"]   
    descriptors = pd.DataFrame(data=baseData,columns=columnNames)
    
    return descriptors
70/43: print(df4['canonical_smiles'].unique())
70/44: print(df4['smiles'].unique())
70/45: df5 = df4.drop_duplicates(subset=['smiles'])
70/46: df5
70/47:
df_lipinski = lipinski(df5.canonical_smiles)
df_lipinski
70/48:
df_lipinski = lipinski(df5.smiles)
df_lipinski
70/49:
list_Fp = ['Fingerprinter.xml', 'ExtendedFingerprinter.xml', 'EStateFingerprinter.xml',
           'GraphOnlyFingerprinter.xml', 'MACCSFingerprinter.xml', 'PubchemFingerprinter.xml',
           'SubstructureFingerprinter.xml', 'KlekotaRothFingerprinter.xml', 'AtomPairs2DFingerprinter.xml']
70/50:
#create smi files
selection = ['smiles','compound']
df_selection = df4[selection]
df_selection.to_csv('molecule.smi', sep='\t', index=False, header=False)
70/51: !cat molecule.smi
70/52:
list_Fp = ['Fingerprinter.xml', 'ExtendedFingerprinter.xml', 'EStateFingerprinter.xml',
           'GraphOnlyFingerprinter.xml', 'MACCSFingerprinter.xml', 'PubchemFingerprinter.xml',
           'SubstructureFingerprinter.xml', 'KlekotaRothFingerprinter.xml', 'AtomPairs2DFingerprinter.xml']
70/53:
#create smi files
selection = ['smiles','compound']
df_selection = df4[selection]
df_selection.to_csv('all_molecule.smi', sep='\t', index=False, header=False)
70/54:
list_Fp = ['Fingerprinter.xml', 'ExtendedFingerprinter.xml', 'EStateFingerprinter.xml',
           'GraphOnlyFingerprinter.xml', 'MACCSFingerprinter.xml', 'PubchemFingerprinter.xml',
           'SubstructureFingerprinter.xml', 'KlekotaRothFingerprinter.xml', 'AtomPairs2DFingerprinter.xml']
70/55:
def Fingerprint(name , Fp):
    
    group = name
    group = [name.replace(".smi", ".csv") for name in group]
    result = ', '.join(group)
    
    Fp_name = Fp
    Fp_name = Fp.replace(".xml", "_")
    
    print name
    print result
    print Fp
    print Fp_name

    ! java -jar -Xms64m -Xmx200m /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp \
    -dir /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name \
    -file /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result
70/56:
def Fingerprint(name , Fp):
    
    group = name
    group = [name.replace(".smi", ".csv") for name in group]
    result = ', '.join(group)
    
    Fp_name = Fp
    Fp_name = Fp.replace(".xml", "_")
    
    print (name)
    print result
    print Fp
    print Fp_name

    ! java -jar -Xms64m -Xmx200m /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp \
    -dir /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name \
    -file /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result
70/57:
def Fingerprint(name , Fp):
    
    group = name
    group = [name.replace(".smi", ".csv") for name in group]
    result = ', '.join(group)
    
    Fp_name = Fp
    Fp_name = Fp.replace(".xml", "_")
    
    print (name)
    print (result)
    print (Fp)
    print (Fp_name)

    ! java -jar -Xms64m -Xmx200m /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp \
    -dir /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name \
    -file /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result
70/58:
import os
import glob

path =  r'smiles/'
for f in glob.glob(path + '*.smi'):
    names = [os.path.basename(f)]
    for Fp in (list_Fp):
        Fingerprint(names, Fp)
70/59:
#create smi files
selection = ['smiles']
df_selection = df4[selection]
df_selection.to_csv(r'smile/all_molecule.smi', sep='\t', index=False, header=False)
70/60:
#create smi files
selection = ['smiles']
df_selection = df4[selection]
df_selection.to_csv(r'smiles/all_molecule.smi', sep='\t', index=False, header=False)
70/61:
list_Fp = ['Fingerprinter.xml', 'ExtendedFingerprinter.xml', 'EStateFingerprinter.xml',
           'GraphOnlyFingerprinter.xml', 'MACCSFingerprinter.xml', 'PubchemFingerprinter.xml',
           'SubstructureFingerprinter.xml', 'KlekotaRothFingerprinter.xml', 'AtomPairs2DFingerprinter.xml']
70/62:
def Fingerprint(name , Fp):
    
    group = name
    group = [name.replace(".smi", ".csv") for name in group]
    result = ', '.join(group)
    
    Fp_name = Fp
    Fp_name = Fp.replace(".xml", "_")
    
    print (name)
    print (result)
    print (Fp)
    print (Fp_name)

    ! java -jar -Xms64m -Xmx200m /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp \
    -dir /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name \
    -file /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result
70/63:
import os
import glob

path =  r'smiles/'
for f in glob.glob(path + '*.smi'):
    names = [os.path.basename(f)]
    for Fp in (list_Fp):
        Fingerprint(names, Fp)
70/64:
list_Fp = ['Fingerprinter.xml', 'ExtendedFingerprinter.xml', 'EStateFingerprinter.xml',
           'GraphOnlyFingerprinter.xml', 'MACCSFingerprinter.xml', 'PubchemFingerprinter.xml',
           'SubstructureFingerprinter.xml', 'KlekotaRothFingerprinter.xml', 'AtomPairs2DFingerprinter.xml']
70/65:
def Fingerprint(name , Fp):
    
    group = name
    group = [name.replace(".smi", ".csv") for name in group]
    result = ', '.join(group)
    
    Fp_name = Fp
    Fp_name = Fp.replace(".xml", "_")
    
    print (name)
    print (result)
    print (Fp)
    print (Fp_name)

    ! java -jar -Xms64m -Xmx200m /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp \
    -dir /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name \
    -file /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result
70/66:
import os
import glob

path =  r'smiles/'
for f in glob.glob(path + '*.smi'):
    names = [os.path.basename(f)]
    for Fp in (list_Fp):
        Fingerprint(names, Fp)
70/67:
def Fingerprint(name , Fp):
    
    group = name
    group = [name.replace(".smi", ".csv") for name in group]
    result = ', '.join(group)
    
    Fp_name = Fp
    Fp_name = Fp.replace(".xml", "_")
    
    print (name)
    print (result)
    print (Fp)
    print (Fp_name)

    ! java -jar -Xms64m -Xmx200m /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp \
    -dir /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name \
    -file /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result
70/68:
import os
import glob

path =  r'smiles/'
for f in glob.glob(path + '*.smi'):
    names = [os.path.basename(f)]
    for Fp in (list_Fp):
        Fingerprint(names, Fp)
70/69:
import os
import glob
70/70:
def Fingerprint(name , Fp):
    
    group = name
    group = [name.replace(".smi", ".csv") for name in group]
    result = ', '.join(group)
    
    Fp_name = Fp
    Fp_name = Fp.replace(".xml", "_")
    
    print (name)
    print (result)
    print (Fp)
    print (Fp_name)

    ! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp \
    -dir /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name \
    -file /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result
70/71:
import os
import glob

path =  r'smiles/'
for f in glob.glob(path + '*.smi'):
    names = [os.path.basename(f)]
    for Fp in (list_Fp):
        Fingerprint(names, Fp)
70/72:
def Fingerprint(name , Fp):
    
    group = name
    group = [name.replace(".smi", ".csv") for name in group]
    result = ', '.join(group)
    
    Fp_name = Fp
    Fp_name = Fp.replace(".xml", "_")
    
    print (name)
    print (result)
    print (Fp)
    print (Fp_name)

    ! java -jar -Xms64m -Xmx200m /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp \
    -dir r'smiles/$name' \
    -file r'Fingerprint/$Fp_name$result'
70/73:
import os
import glob

path =  r'smiles/'
for f in glob.glob(path + '*.smi'):
    names = [os.path.basename(f)]
    for Fp in (list_Fp):
        Fingerprint(names, Fp)
70/74:
def Fingerprint(name , Fp):
    
    group = name
    group = [name.replace(".smi", ".csv") for name in group]
    result = ', '.join(group)
    
    Fp_name = Fp
    Fp_name = Fp.replace(".xml", "_")
    
    print (name)
    print (result)
    print (Fp)
    print (Fp_name)

    ! java -jar -Xms64m -Xmx200m /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar \
    -removesalt -standardizenitro  \
    -fingerprints r'PaDEL-Descriptor/$Fp' \
    -dir r'smiles/$name' \
    -file r'Fingerprint/$Fp_name$result'
70/75:
import os
import glob

path =  r'smiles/'
for f in glob.glob(path + '*.smi'):
    names = [os.path.basename(f)]
    for Fp in (list_Fp):
        Fingerprint(names, Fp)
70/76:
def Fingerprint(name , Fp):
    
    group = name
    group = [name.replace(".smi", ".csv") for name in group]
    result = ', '.join(group)
    
    Fp_name = Fp
    Fp_name = Fp.replace(".xml", "_")
    
    print (name)
    print (result)
    print (Fp)
    print (Fp_name)

    ! java -jar -Xms64m -Xmx200m /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar \
    -removesalt -standardizenitro  \
    -fingerprints PaDEL-Descriptor/$Fp' \
    -dir smiles/$name \
    -file Fingerprint/$Fp_name$result
70/77:
import os
import glob

path =  r'smiles/'
for f in glob.glob(path + '*.smi'):
    names = [os.path.basename(f)]
    for Fp in (list_Fp):
        Fingerprint(names, Fp)
70/78:
def Fingerprint(name , Fp):
    
    group = name
    group = [name.replace(".smi", ".csv") for name in group]
    result = ', '.join(group)
    
    Fp_name = Fp
    Fp_name = Fp.replace(".xml", "_")
    
    print (name)
    print (result)
    print (Fp)
    print (Fp_name)

    ! java -jar -Xms64m -Xmx200m /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar \
    -removesalt -standardizenitro  \
    -fingerprints PaDEL-Descriptor/$Fp \
    -dir smiles/$name \
    -file Fingerprint/$Fp_name$result
70/79:
import os
import glob

path =  r'smiles/'
for f in glob.glob(path + '*.smi'):
    names = [os.path.basename(f)]
    for Fp in (list_Fp):
        Fingerprint(names, Fp)
70/80:
def Fingerprint(name , Fp):
    
    group = name
    group = [name.replace(".smi", ".csv") for name in group]
    result = ', '.join(group)
    
    Fp_name = Fp
    Fp_name = Fp.replace(".xml", "_")
    
    print (name)
    print (result)
    print (Fp)
    print (Fp_name)

    ! java -jar -Xms64m -Xmx200m /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar \
    -removesalt -standardizenitro  \
    -fingerprints ./PaDEL-Descriptor/$Fp \
    -dir ./smiles/$name \
    -file ./Fingerprint/$Fp_name$result
70/81:
import os
import glob

path =  r'smiles/'
for f in glob.glob(path + '*.smi'):
    names = [os.path.basename(f)]
    for Fp in (list_Fp):
        Fingerprint(names, Fp)
70/82: java -Xms1G -Xmx1G -Djava.awt.headless=true -jar ./PaDEL-Descriptor/PaDEL-Descriptor.jar -removesalt -standardizenitro -fingerprints -descriptortypes ./PaDEL-Descriptor/PubchemFingerprinter.xml -dir ./ -file descriptors_output.csv
70/83: java -Xms1G -Xmx1G -Djava.awt.headless=true -jar PaDEL-Descriptor/PaDEL-Descriptor.jar -removesalt -standardizenitro -fingerprints -descriptortypes ./PaDEL-Descriptor/PubchemFingerprinter.xml -dir ./ -file descriptors_output.csv
70/84:
def Fingerprint(name , Fp):
    
    group = name
    group = [name.replace(".smi", ".csv") for name in group]
    result = ', '.join(group)
    
    Fp_name = Fp
    Fp_name = Fp.replace(".xml", "_")
    
    print (name)
    print (result)
    print (Fp)
    print (Fp_name)

    ! java -jar -Xms64m -Xmx200m /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp \
    -dir /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name \
    -file /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result
70/85:
import os
import glob

path =  r'smiles/'
for f in glob.glob(path + '*.smi'):
    names = [os.path.basename(f)]
    for Fp in (list_Fp):
        Fingerprint(names, Fp)
70/86: Fingerprint('all_molecules.smi', Fingerprinter)
70/87: Fingerprint('all_molecules.smi', Fingerprinter.xml)
70/88: Fingerprint('all_molecules.smi')
70/89: Fingerprint('all_molecules.smi', 'Fingerprinter.xml')
70/90:
import os
import glob

path =  r'smiles/'
for f in glob.glob(path + '*.smi'):
    names = [os.path.basename(f)]
    for Fp in (list_Fp):
        Fingerprint(names, Fp)
70/91:
#create smi files
selection = ['smiles']
df_selection = df4[selection]
df_selection.to_csv(r'smiles/molecule.smi', sep='\t', index=False, header=False)
70/92:
list_Fp = ['Fingerprinter.xml', 'ExtendedFingerprinter.xml', 'EStateFingerprinter.xml',
           'GraphOnlyFingerprinter.xml', 'MACCSFingerprinter.xml', 'PubchemFingerprinter.xml',
           'SubstructureFingerprinter.xml', 'KlekotaRothFingerprinter.xml', 'AtomPairs2DFingerprinter.xml']
70/93:
def Fingerprint(name , Fp):
    
    group = name
    group = [name.replace(".smi", ".csv") for name in group]
    result = ', '.join(group)
    
    Fp_name = Fp
    Fp_name = Fp.replace(".xml", "_")
    
    print (name)
    print (result)
    print (Fp)
    print (Fp_name)

    ! java -jar -Xms64m -Xmx200m /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp \
    -dir /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name \
    -file /Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result
70/94:
import os
import glob

path =  r'smiles/'
for f in glob.glob(path + '*.smi'):
    names = [os.path.basename(f)]
    for Fp in (list_Fp):
        Fingerprint(names, Fp)
73/1: pwd
73/2:
list_Fp = ['Fingerprinter.xml', 'ExtendedFingerprinter.xml', 'EStateFingerprinter.xml',
           'GraphOnlyFingerprinter.xml', 'MACCSFingerprinter.xml', 'PubchemFingerprinter.xml',
           'SubstructureFingerprinter.xml', 'KlekotaRothFingerprinter.xml', 'AtomPairs2DFingerprinter.xml']
73/3:
def Fingerprint(name , Fp):
    
    group = name
    group = [name.replace(".smi", ".csv") for name in group]
    result = ', '.join(group)
    
    Fp_name = Fp
    Fp_name = Fp.replace(".xml", "_")
    
    print name
    print result
    print Fp
    print Fp_name

    ! java -jar -Xms64m -Xmx200m /Users/zeromtmu/Desktop/Thesis/PaDEL-Descriptor/PaDEL-Descriptor.jar \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes /Users/zeromtmu/Desktop/Thesis/PaDEL-Descriptor/$Fp \
    -dir /Users/zeromtmu/Desktop/Thesis/ER_alpha_Regression/smiles/$name \
    -file /Users/zeromtmu/Desktop/Thesis/ER_alpha_Regression/Fingerprint/$Fp_name$result
73/4:
list_Fp = ['Fingerprinter.xml', 'ExtendedFingerprinter.xml', 'EStateFingerprinter.xml',
           'GraphOnlyFingerprinter.xml', 'MACCSFingerprinter.xml', 'PubchemFingerprinter.xml',
           'SubstructureFingerprinter.xml', 'KlekotaRothFingerprinter.xml', 'AtomPairs2DFingerprinter.xml']
73/5:
def Fingerprint(name , Fp):
    
    group = name
    group = [name.replace(".smi", ".csv") for name in group]
    result = ', '.join(group)
    
    Fp_name = Fp
    Fp_name = Fp.replace(".xml", "_")
    
    print (name)
    print (result)
    print (Fp)
    print (Fp_name)

    ! java -jar -Xms64m -Xmx200m /Users/zeromtmu/Desktop/Thesis/PaDEL-Descriptor/PaDEL-Descriptor.jar \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes /Users/zeromtmu/Desktop/Thesis/PaDEL-Descriptor/$Fp \
    -dir /Users/zeromtmu/Desktop/Thesis/ER_alpha_Regression/smiles/$name \
    -file /Users/zeromtmu/Desktop/Thesis/ER_alpha_Regression/Fingerprint/$Fp_name$result
73/6:
import os
import glob

path =  r'smiles/'
for f in glob.glob(path + '*.smi'):
    names = [os.path.basename(f)]
    for Fp in (list_Fp):
        Fingerprint(names, Fp)
73/7:
def Fingerprint(name , Fp):
    
    group = name
    group = [name.replace(".smi", ".csv") for name in group]
    result = ', '.join(group)
    
    Fp_name = Fp
    Fp_name = Fp.replace(".xml", "_")
    
    print (name)
    print (result)
    print (Fp)
    print (Fp_name)

    ! java -jar -Xms64m -Xmx200m /Users/tarapongsrisongkram/Desktop/estrogen-receptor-alpha-qsar-master/PaDEL-Descriptor/PaDEL-Descriptor.jar \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes /Users/tarapongsrisongkram/Desktop/estrogen-receptor-alpha-qsar-master/PaDEL-Descriptor/$Fp \
    -dir /Users/tarapongsrisongkram/Desktop/estrogen-receptor-alpha-qsar-master/smiles/$name \
    -file /Users/tarapongsrisongkram/Desktop/estrogen-receptor-alpha-qsar-master/Fingerprint/$Fp_name$result
73/8:
import os
import glob

path =  r'smiles/'
for f in glob.glob(path + '*.smi'):
    names = [os.path.basename(f)]
    for Fp in (list_Fp):
        Fingerprint(names, Fp)
73/9:
def Fingerprint(name , Fp):
    
    group = name
    group = [name.replace(".smi", ".csv") for name in group]
    result = ', '.join(group)
    
    Fp_name = Fp
    Fp_name = Fp.replace(".xml", "_")
    
    print (name)
    print (result)
    print (Fp)
    print (Fp_name)

    ! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/Desktop/estrogen-receptor-alpha-qsar-master/PaDEL-Descriptor/PaDEL-Descriptor.jar' \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/Desktop/estrogen-receptor-alpha-qsar-master/PaDEL-Descriptor/$Fp' \
    -dir '/Users/tarapongsrisongkram/Desktop/estrogen-receptor-alpha-qsar-master/smiles/$name' \
    -file '/Users/tarapongsrisongkram/Desktop/estrogen-receptor-alpha-qsar-master/Fingerprint/$Fp_name$result'
73/10:
import os
import glob

path =  r'smiles/'
for f in glob.glob(path + '*.smi'):
    names = [os.path.basename(f)]
    for Fp in (list_Fp):
        Fingerprint(names, Fp)
70/95:
def Fingerprint(name , Fp):
    
    group = name
    group = [name.replace(".smi", ".csv") for name in group]
    result = ', '.join(group)
    
    Fp_name = Fp
    Fp_name = Fp.replace(".xml", "_")
    
    print (name)
    print (result)
    print (Fp)
    print (Fp_name)

    ! java -jar -Xms64m -Xmx200m "/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar" \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes "/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp" \
    -dir "/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name" \
    -file "/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result"
70/96:
import os
import glob

path =  r'smiles/'
for f in glob.glob(path + '*.smi'):
    names = [os.path.basename(f)]
    for Fp in (list_Fp):
        Fingerprint(names, Fp)
70/97: !cat all_smile.smi
70/98: !cat r'smiles/molecule.smi'
70/99:
#create smi files
selection = ['smiles', 'compound']
df_selection = df4[selection]
df_selection.to_csv(r'smiles/molecule.smi', sep='\t', index=False, header=False)
70/100: !cat r'smiles/molecule.smi'
70/101:
list_Fp = ['Fingerprinter.xml', 'ExtendedFingerprinter.xml', 'EStateFingerprinter.xml',
           'GraphOnlyFingerprinter.xml', 'MACCSFingerprinter.xml', 'PubchemFingerprinter.xml',
           'SubstructureFingerprinter.xml', 'KlekotaRothFingerprinter.xml', 'AtomPairs2DFingerprinter.xml']
70/102:
def Fingerprint(name , Fp):
    
    group = name
    group = [name.replace(".smi", ".csv") for name in group]
    result = ', '.join(group)
    
    Fp_name = Fp
    Fp_name = Fp.replace(".xml", "_")
    
    print (name)
    print (result)
    print (Fp)
    print (Fp_name)

    ! java -jar -Xms64m -Xmx200m "/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar" \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes "/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp" \
    -dir "/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name" \
    -file "/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result"
70/103:
import os
import glob

path =  r'smiles/'
for f in glob.glob(path + '*.smi'):
    names = [os.path.basename(f)]
    for Fp in (list_Fp):
        Fingerprint(names, Fp)
70/104:
def Fingerprint(name , Fp):
    
    group = name
    group = [name.replace(".smi", ".csv") for name in group]
    result = ', '.join(group)
    
    Fp_name = Fp
    Fp_name = Fp.replace(".xml", "_")
    
    print (name)
    print (result)
    print (Fp)
    print (Fp_name)

    ! java -jar -Xms64m -Xmx200m "/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar" \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes "/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp" \
    -dir "/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name.xmi" \
    -file "/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result"
70/105:
def Fingerprint(name , Fp):
    
    group = name
    group = [name.replace(".smi", ".csv") for name in group]
    result = ', '.join(group)
    
    Fp_name = Fp
    Fp_name = Fp.replace(".xml", "_")
    
    print (name)
    print (result)
    print (Fp)
    print (Fp_name)

    ! java -jar -Xms64m -Xmx200m "/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar" \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes "/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp" \
    -dir "/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name.smi" \
    -file "/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result"
70/106:
import os
import glob

path =  r'smiles/'
for f in glob.glob(path + '*.smi'):
    names = [os.path.basename(f)]
    for Fp in (list_Fp):
        Fingerprint(names, Fp)
73/11:
def Fingerprint(name , Fp):
    
    group = name
    group = [name.replace(".smi", ".csv") for name in group]
    result = ', '.join(group)
    
    Fp_name = Fp
    Fp_name = Fp.replace(".xml", "_")
    
    print (name)
    print (result)
    print (Fp)
    print (Fp_name)

    ! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/Desktop/estrogen-receptor-alpha-qsar-master/PaDEL-Descriptor/PaDEL-Descriptor.jar' \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/Desktop/estrogen-receptor-alpha-qsar-master/PaDEL-Descriptor/$Fp' \
    -dir '/Users/tarapongsrisongkram/Desktop/estrogen-receptor-alpha-qsar-master/smiles/$name' \
    -file '/Users/tarapongsrisongkram/Desktop/estrogen-receptor-alpha-qsar-master/Fingerprint/$Fp_name$result'
73/12:
import os
import glob

path =  r'smiles/'
for f in glob.glob(path + '*.smi'):
    names = [os.path.basename(f)]
    for Fp in (list_Fp):
        Fingerprint(names, Fp)
70/107:
#create smi files
selection = ['smiles', 'compound']
df_selection = df4[selection]
df_selection.to_csv(r'smiles/molecule.smi', sep='\t', index=False, header=False)
70/108: !cat r'smiles/molecule.smi'
70/109:
list_Fp = ['Fingerprinter.xml', 'ExtendedFingerprinter.xml', 'EStateFingerprinter.xml',
           'GraphOnlyFingerprinter.xml', 'MACCSFingerprinter.xml', 'PubchemFingerprinter.xml',
           'SubstructureFingerprinter.xml', 'KlekotaRothFingerprinter.xml', 'AtomPairs2DFingerprinter.xml']
70/110:
def Fingerprint(name , Fp):
    
    group = name
    group = [name.replace(".smi", ".csv") for name in group]
    result = ', '.join(group)
    
    Fp_name = Fp
    Fp_name = Fp.replace(".xml", "_")
    
    print (name)
    print (result)
    print (Fp)
    print (Fp_name)

    ! java -jar -Xms64m -Xmx200m "/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar" \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes "/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp" \
    -dir "/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name" \
    -file "/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result"
70/111:
import os
import glob

path =  r'smiles/'
for f in glob.glob(path + '*.smi'):
    names = [os.path.basename(f)]
    for Fp in (list_Fp):
        Fingerprint(names, Fp)
70/112:
def Fingerprint(name , Fp):
    
    group = name
    group = [name.replace(".smi", ".csv") for name in group]
    result = ', '.join(group)
    
    Fp_name = Fp
    Fp_name = Fp.replace(".xml", "_")
    
    print (name)
    print (result)
    print (Fp)
    print (Fp_name)

    ! java -jar -Xms64m -Xmx200m "/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar" \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes "/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp" \
    -dir "/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/name" \
    -file "/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result"
70/113:
import os
import glob

path =  r'smiles/'
for f in glob.glob(path + '*.smi'):
    names = [os.path.basename(f)]
    for Fp in (list_Fp):
        Fingerprint(names, Fp)
70/114:
def Fingerprint(name , Fp):
    
    group = name
    group = [name.replace(".smi", ".csv") for name in group]
    result = ', '.join(group)
    
    Fp_name = Fp
    Fp_name = Fp.replace(".xml", "_")
    
    print (name)
    print (result)
    print (Fp)
    print (Fp_name)

    ! java -jar -Xms64m -Xmx200m "/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar" \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes "/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp" \
    -dir "/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name" \
    -file "/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result"
70/115:
import os
import glob

path =  r'smiles/'
for f in glob.glob(path + '*.smi'):
    names = [os.path.basename(f)]
    for Fp in (list_Fp):
        Fingerprint(names, Fp)
70/116:
def Fingerprint(name , Fp):
    
    group = name
    group = [name.replace(".smi", ".csv") for name in group]
    result = ', '.join(group)
    
    Fp_name = Fp
    Fp_name = Fp.replace(".xml", "_")
    
    print (name)
    print (result)
    print (Fp)
    print (Fp_name)

    ! java -Xms1G -Xmx1G -Djava.awt.headless=true -jar  "/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar" \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes "/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp" \
    -dir "/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name" \
    -file "/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result"
70/117:
import os
import glob

path =  r'smiles/'
for f in glob.glob(path + '*.smi'):
    names = [os.path.basename(f)]
    for Fp in (list_Fp):
        Fingerprint(names, Fp)
70/118:
def Fingerprint(name , Fp):
    
    group = name
    group = [name.replace(".smi", ".csv") for name in group]
    result = ', '.join(group)
    
    Fp_name = Fp
    Fp_name = Fp.replace(".xml", "_")
    
    print (name)
    print (result)
    print (Fp)
    print (Fp_name)

    ! java -jar -Xms64m -Xmx200m "/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar" \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes "/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp" \
    -dir "/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name" \
    -file "/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result"
70/119:
import os
import glob

path =  r'smiles/'
for f in glob.glob(path + '*.smi'):
    names = [os.path.basename(f)]
    for Fp in (list_Fp):
        Fingerprint(names, Fp)
70/120:
#create smi files
selection = ['smiles', 'compound']
df_selection = df4[selection]
df_selection.to_csv('molecule.smi', sep='\t', index=False, header=False)
70/121: !cat r'smiles/molecule.smi'
70/122: !cat 'molecule.smi'
70/123: !cat padel.sh
70/124: !cat padel.sh
70/125: !bash padel.sh
82/1: Analytical Data
82/2:
| Stretch/Untouched | ProbDistribution | Accuracy |
| --- | --- | --- |
| Stretched | Gaussian | .843 |
82/3:
| Stretch/Untouched | ProbDistribution | Accuracy |
| --- | --- | --- |
| Stretched | Gaussian | .843 |
84/1:
| Stretch/Untouched | ProbDistribution | Accuracy |
| --- | --- | --- |
| Stretched | Gaussian | .843 |
84/2:
|  | A | B |
| --- | --- | --- |
| Weighing bottle + KHP (before) | 36.1558 | 35.5405 |
| Weighing bottle + KHP (after) | 31.1707 | 30.3714 |
90/1:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
90/2: df = pd.read_csv('rawdataqsar.csv')
90/3: pwd
90/4: df = pd.read_csv('rawdataqsar.csv')
90/5: df
90/6: df2 =df[df.kpu.notna()]
90/7: df3 =df2[df.kp.notna()]
90/8: df3
90/9: df3.shape
90/10: df3.isnull()
90/11: df4=df3.reset_index(drop=True)
90/12: df4
90/13:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
90/14: pwd
90/15: df = pd.read_csv('rawdataqsar.csv')
90/16: df
90/17: df2 =df[df.kpu.notna()]
90/18: df3 =df2[df.kp.notna()]
90/19: df3
90/20: df3.shape
90/21: df3.isnull()
90/22: df4=df3.reset_index(drop=True)
90/23: df4
90/24:
from rdkit import Chem
from rdkit.Chem import Descriptors, Lipinski
90/25:
from rdkit import Chem
from rdkit.Chem import Descriptors, Lipinski
90/26:
def lipinski(smiles, verbose=False):

    moldata= []
    for elem in smiles:
        mol=Chem.MolFromSmiles(elem) 
        moldata.append(mol)
       
    baseData= np.arange(1,1)
    i=0  
    for mol in moldata:        
       
        desc_MolWt = Descriptors.MolWt(mol)
        desc_MolLogP = Descriptors.MolLogP(mol)
        desc_NumHDonors = Lipinski.NumHDonors(mol)
        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)
        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)
           
        row = np.array([desc_MolWt,
                        desc_MolLogP,
                        desc_NumHDonors,
                        desc_NumHAcceptors,
                        desc_NumRotatableBonds])   
    
        if(i==0):
            baseData=row
        else:
            baseData=np.vstack([baseData, row])
        i=i+1      
    
    columnNames=["MW","LogP","NumHDonors","NumHAcceptors", "NumRotatableBonds"]   
    descriptors = pd.DataFrame(data=baseData,columns=columnNames)
    
    return descriptors
90/27: print(df4['smiles'].unique())
90/28: df5 = df4.drop_duplicates(subset=['smiles'])
90/29: df5
90/30:
df_lipinski = lipinski(df5.smiles)
df_lipinski
90/31:
#create smi files
selection = ['smiles', 'compound']
df_selection = df4[selection]
df_selection.to_csv(r'smiles/molecule.smi', sep='\t', index=False, header=False)
90/32: df_6 = pd.concat([df5,df_lipinski], axis=1)
90/33:
df6 = pd.concat([df5,df_lipinski], axis=1)
df
90/34:
df6 = pd.concat([df5,df_lipinski], axis=1)
df6
90/35: df6.tail(6)
90/36:
data = pd.concat([df5,df_lipinski], axis=1)
data
90/37: data.tail(6)
90/38:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)
plt2.set_xlabel("LogP", fontsize=16, fontweight='bold')
plt2.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)
plt2.set_ylim(0, 50)
plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)


#Histogram for NumHDonors
plt3.hist(data['NumHDonors'], density=False, bins= 30, color='#FF9A00', edgecolor='black', linewidth=0.5)
plt3.set_xlabel("Number of Hydrogen Bond Donors", fontsize=16, fontweight='bold')
plt3.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)
plt3.set_ylim(0, 50)
plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 30, color='#FF165D', edgecolor='black', linewidth=0.5)
plt4.set_xlabel("Number of Hydrogen Bond Acceptors", fontsize=16, fontweight='bold')
plt4.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)
plt4.set_ylim(0, 50)
plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 30, color='#52006A', edgecolor='black', linewidth=0.5)
plt5.set_xlabel("Number of RotatableBonds", fontsize=16, fontweight='bold')
plt5.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)
plt5.set_ylim(0, 50)
plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#save file
plt.tight_layout()
plt.savefig('Kpuu plots-basic descriptors.pdf', dpi=300)
90/39:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)
plt2.set_xlabel("LogP", fontsize=16, fontweight='bold')
plt2.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)
plt2.set_ylim(0, 50)
plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)


#Histogram for NumHDonors
plt3.hist(data['NumHDonors'], density=False, bins= 30, color='#FF9A00', edgecolor='black', linewidth=0.5)
plt3.set_xlabel("Number of Hydrogen Bond Donors", fontsize=16, fontweight='bold')
plt3.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)
plt3.set_ylim(0, 50)
plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 30, color='#FF165D', edgecolor='black', linewidth=0.5)
plt4.set_xlabel("Number of Hydrogen Bond Acceptors", fontsize=16, fontweight='bold')
plt4.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)
plt4.set_ylim(0, 50)
plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 20, color='#52006A', edgecolor='black', linewidth=0.5)
plt5.set_xlabel("Number of RotatableBonds", fontsize=16, fontweight='bold')
plt5.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)
plt5.set_ylim(0, 50)
plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#save file
plt.tight_layout()
plt.savefig('Kpuu plots-basic descriptors.pdf', dpi=300)
90/40:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)
plt2.set_xlabel("LogP", fontsize=16, fontweight='bold')
plt2.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)
plt2.set_ylim(0, 50)
plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)


#Histogram for NumHDonors
plt3.hist(data['NumHDonors'], density=False, bins= 30, color='#FF9A00', edgecolor='black', linewidth=0.5)
plt3.set_xlabel("Number of Hydrogen Bond Donors", fontsize=16, fontweight='bold')
plt3.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)
plt3.set_ylim(0, 50)
plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt4.hist(data['NumHAcceptors'], density=False, bins= 30, color='#FF165D', edgecolor='black', linewidth=0.5)
plt4.set_xlabel("Number of Hydrogen Bond Acceptors", fontsize=16, fontweight='bold')
plt4.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)
plt4.set_ylim(0, 50)
plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt5.hist(data['NumRotatableBonds'], density=False, bins= 30, color='#52006A', edgecolor='black', linewidth=0.5)
plt5.set_xlabel("Number of RotatableBonds", fontsize=16, fontweight='bold')
plt5.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)
plt5.set_ylim(0, 50)
plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#save file
plt.tight_layout()
plt.savefig('Kpuu plots-basic descriptors.pdf', dpi=300)
90/41:
#Visuallize kpuu
plt.hist(data['kpuu'], density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('Kp\u208u\u208u', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 50)
#plt1.grid(True)
plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#save file
plt.tight_layout()
plt.savefig('LAT1 plots-pIC50 distribution.pdf', dpi=300)
90/42:
#Visuallize kpuu
plt.hist(data['kpuu'], density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('Kp$uu$', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 50)
#plt1.grid(True)
plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#save file
plt.tight_layout()
plt.savefig('LAT1 plots-pIC50 distribution.pdf', dpi=300)
90/43:
#Visuallize kpuu
plt.hist(data['kpu'], density=False, bins= 30, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('Kp$uu$', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 50)
#plt1.grid(True)
plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#save file
plt.tight_layout()
plt.savefig('LAT1 plots-pIC50 distribution.pdf', dpi=300)
90/44:
#Visuallize kpuu
plt.hist(data['kpu'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('Kp$uu$', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 50)
#plt1.grid(True)
plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#save file
plt.tight_layout()
plt.savefig('LAT1 plots-pIC50 distribution.pdf', dpi=300)
90/45:
#Visuallize kpuu
plt.hist(data['kpu'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('$Kp{uu}$', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 50)
#plt1.grid(True)
plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#save file
plt.tight_layout()
plt.savefig('LAT1 plots-pIC50 distribution.pdf', dpi=300)
90/46:
#Visuallize kpuu
plt.hist(data['kpu'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('$Kp_{uu}$', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 50)
#plt1.grid(True)
plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#save file
plt.tight_layout()
plt.savefig('LAT1 plots-pIC50 distribution.pdf', dpi=300)
90/47:
#Visuallize kpuu
plt.hist(data['kpu'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('$Kp_{uu}$', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=9)
plt.ylim(0, 50)
#plt1.grid(True)
plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#save file
plt.tight_layout()
plt.savefig('LAT1 plots-pIC50 distribution.pdf', dpi=300)
90/48:
#Visuallize kpuu
plt.hist(data['kpu'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('$Kp_{uu}$', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 50)
#plt1.grid(True)
plt.axvline(6, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
plt.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#save file
plt.tight_layout()
plt.savefig('Kpuu distribution.pdf', dpi=300)
90/49:
#Visuallize kpuu
plt.hist(data['kpu'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('$Kp_{uu}$', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 50)

#save file
plt.tight_layout()
plt.savefig('Kpuu distribution.pdf', dpi=300)
90/50:
#Visuallize kpuu
logbins = np.logspace(np.log10(bins[0]),np.log10(bins[-1]),len(bins))
plt.hist(data['kpu'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)

plt.xlabel('$Kp_{uu}$', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 50)

#save file
plt.tight_layout()
plt.savefig('Kpuu distribution.pdf', dpi=300)
90/51:
#Visuallize kpuu

plt.hist(data['kpu'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)
logbins = np.logspace(np.log10(bins[0]),np.log10(bins[-1]),len(bins))
plt.xlabel('$Kp_{uu}$', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 50)

#save file
plt.tight_layout()
plt.savefig('Kpuu distribution.pdf', dpi=300)
90/52:
#Visuallize kpuu

plt.hist(data['kpu'], density=False, bins= 10, color='#FF7600', edgecolor='black', linewidth=0.5)
plt.xlabel('$Kp_{uu}$', fontsize=16, fontweight='bold')
plt.ylabel("Frequency", fontsize=16, fontweight='bold')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(0, 50)

#save file
plt.tight_layout()
plt.savefig('Kpuu distribution.pdf', dpi=300)
90/53:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt



figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

x1 = data['MW']
x2 = data['LogP']
x3 = data['NumHDonors']
x4 = data['NumHAcceptors']
x5 = data['NumRotatableBonds']
y = data['kpu']
colors = {'active':'red', 'inactive':'blue', 'intermediate':'green'}
area = (30 * data['pIC50'])  # 0 to 15 point radii

#MW
plt1.scatter(x1, y, s=area, c=data['bioactivity_class'].map(colors), alpha=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)

#LogP
plt2.scatter(x2, y, s=area,c=data['bioactivity_class'].map(colors), alpha=0.5)
plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')
plt2.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)

#NumHbondDonors
plt3.scatter(x3, y, s=area, c=data['bioactivity_class'].map(colors), alpha=0.5)
plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')
plt3.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt4.scatter(x4, y, s=area, c=data['bioactivity_class'].map(colors), alpha=0.5)
plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')
plt4.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt5.scatter(x5, y, s=area, c=data['bioactivity_class'].map(colors), alpha=0.5)
plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')
plt5.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)


#save file
plt.tight_layout()
plt.savefig('RO5_and_Kpuu.pdf', dpi=300)
90/54:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt



figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

x1 = data['MW']
x2 = data['LogP']
x3 = data['NumHDonors']
x4 = data['NumHAcceptors']
x5 = data['NumRotatableBonds']
y = data['kpu']
colors = {'active':'red', 'inactive':'blue', 'intermediate':'green'}
area = (30 * data['kpu'])  # 0 to 15 point radii

#MW
plt1.scatter(x1, y, s=area, c=data['bioactivity_class'].map(colors), alpha=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)

#LogP
plt2.scatter(x2, y, s=area,c=data['bioactivity_class'].map(colors), alpha=0.5)
plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')
plt2.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)

#NumHbondDonors
plt3.scatter(x3, y, s=area, c=data['bioactivity_class'].map(colors), alpha=0.5)
plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')
plt3.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt4.scatter(x4, y, s=area, c=data['bioactivity_class'].map(colors), alpha=0.5)
plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')
plt4.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt5.scatter(x5, y, s=area, c=data['bioactivity_class'].map(colors), alpha=0.5)
plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')
plt5.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)


#save file
plt.tight_layout()
plt.savefig('RO5_and_Kpuu.pdf', dpi=300)
90/55:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt



figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

x1 = data['MW']
x2 = data['LogP']
x3 = data['NumHDonors']
x4 = data['NumHAcceptors']
x5 = data['NumRotatableBonds']
y = data['kpu']
area = (30 * data['kpu'])  # 0 to 15 point radii

#MW
plt1.scatter(x1, y, s=area, alpha=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)

#LogP
plt2.scatter(x2, y, s=area, alpha=0.5)
plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')
plt2.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)

#NumHbondDonors
plt3.scatter(x3, y, s=area, alpha=0.5)
plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')
plt3.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt4.scatter(x4, y, s=area, alpha=0.5)
plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')
plt4.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt5.scatter(x5, y, s=area, alpha=0.5)
plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')
plt5.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)


#save file
plt.tight_layout()
plt.savefig('RO5_and_Kpuu.pdf', dpi=300)
90/56:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt



figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

x1 = data['MW']
x2 = data['LogP']
x3 = data['NumHDonors']
x4 = data['NumHAcceptors']
x5 = data['NumRotatableBonds']
y = data['kpu']
area = (30 * data['kpu'])  # 0 to 15 point radii

#MW
plt1.scatter(x1, y, s=area, alpha=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)

#LogP
plt2.scatter(x2, y, s=area, alpha=0.5)
plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')
plt2.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)

#NumHbondDonors
plt3.scatter(x3, y, s=area, alpha=0.5)
plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')
plt3.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt4.scatter(x4, y, s=area, alpha=0.5)
plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')
plt4.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt5.scatter(x5, y, s=area, alpha=0.5)
plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')
plt5.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)


#save file
plt.tight_layout()
plt.savefig('RO5_and_Kpuu.pdf', dpi=300)
90/57:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt



figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

x1 = data['MW']
x2 = data['LogP']
x3 = data['NumHDonors']
x4 = data['NumHAcceptors']
x5 = data['NumRotatableBonds']
y = data['kpu']
area = (30 * data['kpu'])  # 0 to 15 point radii

#MW
plt1.scatter(x1, y, alpha=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)

#LogP
plt2.scatter(x2, y, s=area, alpha=0.5)
plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')
plt2.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)

#NumHbondDonors
plt3.scatter(x3, y, s=area, alpha=0.5)
plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')
plt3.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt4.scatter(x4, y, s=area, alpha=0.5)
plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')
plt4.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt5.scatter(x5, y, s=area, alpha=0.5)
plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')
plt5.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)


#save file
plt.tight_layout()
plt.savefig('RO5_and_Kpuu.pdf', dpi=300)
90/58:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt



figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

x1 = data['MW']
x2 = data['LogP']
x3 = data['NumHDonors']
x4 = data['NumHAcceptors']
x5 = data['NumRotatableBonds']
y = data['kpu']
area = (30 * data['kpu'])  # 0 to 15 point radii

#MW
plt1.scatter(x1, y, alpha=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)

#LogP
plt2.scatter(x2, y, alpha=0.5)
plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')
plt2.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)

#NumHbondDonors
plt3.scatter(x3, y, alpha=0.5)
plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')
plt3.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt4.scatter(x4, y,  alpha=0.5)
plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')
plt4.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt5.scatter(x5, y, alpha=0.5)
plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')
plt5.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)


#save file
plt.tight_layout()
plt.savefig('RO5_and_Kpuu.pdf', dpi=300)
90/59:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt



figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

x1 = data['MW']
x2 = data['LogP']
x3 = data['NumHDonors']
x4 = data['NumHAcceptors']
x5 = data['NumRotatableBonds']
y = data['kpu']
area = (30 * data['kpu'])  # 0 to 15 point radii

#MW
plt1.scatter(x1, y, alpha=0.5)
plt1.set_yscale('log')
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)

#LogP
plt2.scatter(x2, y, alpha=0.5)
plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')
plt2.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)

#NumHbondDonors
plt3.scatter(x3, y, alpha=0.5)
plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')
plt3.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt4.scatter(x4, y,  alpha=0.5)
plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')
plt4.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt5.scatter(x5, y, alpha=0.5)
plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')
plt5.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)


#save file
plt.tight_layout()
plt.savefig('RO5_and_Kpuu.pdf', dpi=300)
90/60:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt



figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

x1 = data['MW']
x2 = data['LogP']
x3 = data['NumHDonors']
x4 = data['NumHAcceptors']
x5 = data['NumRotatableBonds']
y = data['kpu']
area = (30 * data['kpu'])  # 0 to 15 point radii

#MW
plt1.scatter(x1, y, alpha=0.5)
plt1.set_yscale('log')
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)

#LogP
plt2.scatter(x2, y, alpha=0.5)
plt1.set_yscale('log')
plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')
plt2.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)

#NumHbondDonors
plt3.scatter(x3, y, alpha=0.5)
plt1.set_yscale('log')
plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')
plt3.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt4.scatter(x4, y,  alpha=0.5)
plt1.set_yscale('log')
plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')
plt4.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt5.scatter(x5, y, alpha=0.5)
plt1.set_yscale('log')
plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')
plt5.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)


#save file
plt.tight_layout()
plt.savefig('RO5_and_Kpuu.pdf', dpi=300)
90/61:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt



figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

x1 = data['MW']
x2 = data['LogP']
x3 = data['NumHDonors']
x4 = data['NumHAcceptors']
x5 = data['NumRotatableBonds']
y = data['kpu']
area = (30 * data['kpu'])  # 0 to 15 point radii

#MW
plt1.scatter(x1, y, alpha=0.5)
plt1.set_yscale('log')
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)

#LogP
plt2.scatter(x2, y, alpha=0.5)
plt1.set_yscale('log')
plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')
plt2.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)

#NumHbondDonors
plt3.scatter(x3, y, alpha=0.5)
plt1.set_yscale('log')
plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')
plt3.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt4.scatter(x4, y,  alpha=0.5)
plt1.set_yscale('log')
plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')
plt4.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt5.scatter(x5, y, alpha=0.5)
plt1.set_yscale('log')
plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')
plt5.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)


#save file
plt.tight_layout()
plt.savefig('RO5_and_Kpuu.pdf', dpi=300)
90/62:
data = pd.concat([df5,df_lipinski], axis=1)
data.to_csv('data_lipinski_kpu.csv')
data
90/63: print(df5['kpu'].unique())
90/64: pwd
90/65: df = pd.read_csv('rawdataqsar.csv')
90/66: df
90/67: df2 =df[df.kpu.notna()]
90/68: df3 =df2[df.kp.notna()]
90/69: df3
90/70: df3.shape
90/71: df3.isnull()
90/72: df4=df3.reset_index(drop=True)
90/73: df4
90/74:
from rdkit import Chem
from rdkit.Chem import Descriptors, Lipinski
90/75:
def lipinski(smiles, verbose=False):

    moldata= []
    for elem in smiles:
        mol=Chem.MolFromSmiles(elem) 
        moldata.append(mol)
       
    baseData= np.arange(1,1)
    i=0  
    for mol in moldata:        
       
        desc_MolWt = Descriptors.MolWt(mol)
        desc_MolLogP = Descriptors.MolLogP(mol)
        desc_NumHDonors = Lipinski.NumHDonors(mol)
        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)
        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)
           
        row = np.array([desc_MolWt,
                        desc_MolLogP,
                        desc_NumHDonors,
                        desc_NumHAcceptors,
                        desc_NumRotatableBonds])   
    
        if(i==0):
            baseData=row
        else:
            baseData=np.vstack([baseData, row])
        i=i+1      
    
    columnNames=["MW","LogP","NumHDonors","NumHAcceptors", "NumRotatableBonds"]   
    descriptors = pd.DataFrame(data=baseData,columns=columnNames)
    
    return descriptors
90/76: print(df4['smiles'].unique())
90/77: df5 = df4.drop_duplicates(subset=['smiles'])
90/78: df5
90/79: print(df5['kpu'].unique())
90/80:
df_lipinski = lipinski(df5.smiles)
df_lipinski
90/81:
#create smi files
selection = ['smiles', 'compound']
df_selection = df4[selection]
df_selection.to_csv(r'smiles/molecule.smi', sep='\t', index=False, header=False)
90/82:
#create smi files
selection = ['smiles', 'compound']
df_selection = df4[selection]
df_selection.to_csv('molecule.smi', sep='\t', index=False, header=False)
90/83:
data = pd.concat([df5,df_lipinski], axis=1)
data.to_csv('data_lipinski_kpu.csv')
data
90/84:
df_lipinski = lipinski(df5.smiles)
df_lipinski
90/85: df_lipinski.shape
90/86: df4
90/87: df5.shape
90/88: df6 = df5.reset_index(drop=True))
90/89:
df6 = df5.reset_index(drop=True)
df6
90/90:
df6_lipinski = df_lipinski.reset_index(drop=True)
df6
90/91:
df6_lipinski = df_lipinski.reset_index(drop=True)
df6_lipinski
90/92:
data = pd.concat([df6,df6_lipinski], axis=1)
data.to_csv('data_lipinski_kpu.csv')
data
89/1: pwd
89/2:
list_Fp = ['Fingerprinter.xml', 'ExtendedFingerprinter.xml', 'EStateFingerprinter.xml',
           'GraphOnlyFingerprinter.xml', 'MACCSFingerprinter.xml', 'PubchemFingerprinter.xml',
           'SubstructureFingerprinter.xml', 'KlekotaRothFingerprinter.xml', 'AtomPairs2DFingerprinter.xml']
95/1:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
95/2: pwd
95/3: df = pd.read_csv('rawdataqsar.csv')
95/4: df
95/5: df2 =df[df.kpu.notna()]
95/6: df3 =df2[df.kp.notna()]
95/7: df3
95/8: df3.shape
95/9: df3.isnull()
95/10: df4=df3.reset_index(drop=True)
95/11: df4
95/12:
from rdkit import Chem
from rdkit.Chem import Descriptors, Lipinski
95/13:
def lipinski(smiles, verbose=False):

    moldata= []
    for elem in smiles:
        mol=Chem.MolFromSmiles(elem) 
        moldata.append(mol)
       
    baseData= np.arange(1,1)
    i=0  
    for mol in moldata:        
       
        desc_MolWt = Descriptors.MolWt(mol)
        desc_MolLogP = Descriptors.MolLogP(mol)
        desc_NumHDonors = Lipinski.NumHDonors(mol)
        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)
        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)
           
        row = np.array([desc_MolWt,
                        desc_MolLogP,
                        desc_NumHDonors,
                        desc_NumHAcceptors,
                        desc_NumRotatableBonds])   
    
        if(i==0):
            baseData=row
        else:
            baseData=np.vstack([baseData, row])
        i=i+1      
    
    columnNames=["MW","LogP","NumHDonors","NumHAcceptors", "NumRotatableBonds"]   
    descriptors = pd.DataFrame(data=baseData,columns=columnNames)
    
    return descriptors
95/14:
def lipinski(smiles, verbose=False):

    moldata= []
    for elem in smiles:
        mol=Chem.MolFromSmiles(elem) 
        moldata.append(mol)
       
    baseData= np.arange(1,1)
    i=0  
    for mol in moldata:        
       
        desc_MolWt = Descriptors.MolWt(mol)
        desc_TPSA = Descriptors.TPSA(mol)
        desc_MolLogP = Descriptors.MolLogP(mol)
        desc_NumHDonors = Lipinski.NumHDonors(mol)
        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)
        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)
           
        row = np.array([desc_MolWt,
                        desc_TPSA,
                        desc_MolLogP,
                        desc_NumHDonors,
                        desc_NumHAcceptors,
                        desc_NumRotatableBonds])   
    
        if(i==0):
            baseData=row
        else:
            baseData=np.vstack([baseData, row])
        i=i+1      
    
    columnNames=["MW","TPSA", "LogP","NumHDonors","NumHAcceptors", "NumRotatableBonds"]   
    descriptors = pd.DataFrame(data=baseData,columns=columnNames)
    
    return descriptors
95/15: print(df4['smiles'].unique())
95/16:
dfall_lipinski = lipinski(df4.smiles)
dfall_lipinski
95/17: print(df4['kpu'].unique())
95/18:
df4_all = df4.reset_index(drop=True)
df4_all
95/19:
df4_all = df4.reset_index(drop=True)
df4_all_lipinski = dfall_lipinski.reset_index(drop=True)
data2 = pd.concat([df4_all,df4_all_lipinski], axis=1)
data2.to_csv('data2_lipinski_kpu.csv')
data2
93/1:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
93/2: pwd
93/3: df = pd.read_csv('data2_all_lipinski.csv')
93/4: df = pd.read_csv('data2_all_lipinski_pku.csv')
93/5: df = pd.read_csv('data2_all_lipinski_kpu.csv')
93/6: df = pd.read_csv('data2_lipinski_kpu.csv')
93/7:
df = pd.read_csv('data2_lipinski_kpu.csv')
df
93/8:
df = pd.read_csv('data2_lipinski_kpu.csv')
df1 =df.drop(['Unnamed'], axis =1)
93/9:
df = pd.read_csv('data2_lipinski_kpu.csv')
df1 =df.drop(['Unnamed'], axis =1)
93/10: df = pd.read_csv('data2_lipinski_kpu.csv')
93/11:
df = pd.read_csv('data2_lipinski_kpu.csv')
df
93/12: df1 =df.drop(['Unnamed:'], axis =1)
93/13: df1 =df.drop(['Unnamed: 0'], axis =1)
93/14:
df1 =df.drop(['Unnamed: 0'], axis =1)
df1
93/15:
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42)
93/16: X = df1.drop(['smiles', 'kp', 'Cells'])
93/17: X = df1.drop(c['smiles', 'kp', 'Cells'])
93/18: X = df1.drop(['smiles', 'kp', 'Cells'])
93/19: X = df1.drop(['smiles', 'kp', 'Cells'], axis =1)
93/20: X
93/21: X = df1.drop(['compound','smiles', 'kp', 'Cells'], axis =1)
93/22: X
100/1: X = df1.drop(['compound','smiles', 'kp','kpu' 'Cells'], axis =1)
100/2:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
100/3: pwd
100/4:
df = pd.read_csv('data2_lipinski_kpu.csv')
df
100/5:
df1 =df.drop(['Unnamed: 0'], axis =1)
df1
100/6: X = df1.drop(['compound','smiles', 'kp','kpu' 'Cells'], axis =1)
100/7: X = df1.drop(['compound','smiles', 'kp','kpu', 'Cells'], axis =1)
100/8: X
100/9: Y = df1['kp']
100/10:
Y = df1['kp']
Y
100/11: df2 = pd.concat([X, Y], 1)
100/12: df2
100/13: X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)
100/14: X_train.shape, Y_train.shape
100/15: X_test.shape, Y_test.shape
100/16:
model = RandomForestRegressor(n_estimators=100)
model.fit(X_train, Y_train)
r2 = model.score(X_test, Y_test)
r2
100/17:
rom sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(n_estimators=100)
model.fit(X_train, Y_train)
r2 = model.score(X_test, Y_test)
r2
100/18:
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(n_estimators=100)
model.fit(X_train, Y_train)
r2 = model.score(X_test, Y_test)
r2
100/19: Y_pred = model.predict(X_test)
100/20:
import matplotlib.pyplot as plt

sns.set(color_codes=True)
sns.set_style("white")

ax = sns.regplot(Y_test, Y_pred, scatter_kws={'alpha':0.4})
ax.set_xlabel('Experimental pIC50', fontsize='large', fontweight='bold')
ax.set_ylabel('Predicted pIC50', fontsize='large', fontweight='bold')
ax.set_xlim(0, 12)
ax.set_ylim(0, 12)
ax.figure.set_size_inches(5, 5)
plt.show
100/21:
import matplotlib.pyplot as plt

sns.set(color_codes=True)
sns.set_style("white")

ax = sns.regplot(Y_test, Y_pred, scatter_kws={'alpha':0.4})
ax.set_xlabel('Experimental pIC50', fontsize='large', fontweight='bold')
ax.set_ylabel('Predicted pIC50', fontsize='large', fontweight='bold')
ax.figure.set_size_inches(5, 5)
plt.show
100/22:
Y = df1['kpu']
Y
100/23: df2 = pd.concat([X, Y], 1)
100/24: df2
100/25: X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)
100/26: X_train.shape, Y_train.shape
100/27: X_test.shape, Y_test.shape
100/28:
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(n_estimators=100)
model.fit(X_train, Y_train)
r2 = model.score(X_test, Y_test)
r2
100/29: Y_pred = model.predict(X_test)
100/30:
import matplotlib.pyplot as plt

sns.set(color_codes=True)
sns.set_style("white")

ax = sns.regplot(Y_test, Y_pred, scatter_kws={'alpha':0.4})
ax.set_xlabel('Experimental pIC50', fontsize='large', fontweight='bold')
ax.set_ylabel('Predicted pIC50', fontsize='large', fontweight='bold')
ax.figure.set_size_inches(5, 5)
plt.show
100/31:
import matplotlib.pyplot as plt

sns.set(color_codes=True)
sns.set_style("white")

ax = sns.regplot(Y_test, Y_pred, scatter_kws={'alpha':0.4})
ax.set_xlabel('Experimental kpuu', fontsize='large', fontweight='bold')
ax.set_ylabel('Predicted pkuu', fontsize='large', fontweight='bold')

ax.figure.set_size_inches(5, 5)
plt.show
100/32:
from rdkit import Chem
from mordred import Calculator, all_descriptors

# create descriptor calculator with all descriptors
calc = Calculator(all_descriptors())

# calculate and print descriptors
for desc, value in calc(Chem.MolFromSmiles('c1ccccc1O')):
   print('{}\t{}'.format(desc, value))
107/1:
#create smi files 
#re-think
selection = ['smiles', 'compound']
df_selection = data2[selection]
df_selection.to_csv('molecules.smi', sep='\t', index=False, header=False)
df_selection
107/2:
df4_all = df4.reset_index(drop=True)
df4_all_lipinski = dfall_lipinski.reset_index(drop=True)
data2 = pd.concat([df4_all,df4_all_lipinski], axis=1)
data2.to_csv('data2_lipinski_kpu.csv')
data2
107/3:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
107/4: pwd
107/5: df = pd.read_csv('rawdataqsar.csv')
107/6: df
107/7: df2 =df[df.kpu.notna()]
107/8: df3 =df2[df.kp.notna()]
107/9: df3
107/10: df3.shape
107/11: df3.isnull()
107/12: df4=df3.reset_index(drop=True)
107/13: df4
107/14:
from rdkit import Chem
from rdkit.Chem import Descriptors, Lipinski
107/15:
def lipinski(smiles, verbose=False):

    moldata= []
    for elem in smiles:
        mol=Chem.MolFromSmiles(elem) 
        moldata.append(mol)
       
    baseData= np.arange(1,1)
    i=0  
    for mol in moldata:        
       
        desc_MolWt = Descriptors.MolWt(mol)
        desc_TPSA = Descriptors.TPSA(mol)
        desc_MolLogP = Descriptors.MolLogP(mol)
        desc_NumHDonors = Lipinski.NumHDonors(mol)
        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)
        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)
           
        row = np.array([desc_MolWt,
                        desc_TPSA,
                        desc_MolLogP,
                        desc_NumHDonors,
                        desc_NumHAcceptors,
                        desc_NumRotatableBonds])   
    
        if(i==0):
            baseData=row
        else:
            baseData=np.vstack([baseData, row])
        i=i+1      
    
    columnNames=["MW","TPSA", "LogP","NumHDonors","NumHAcceptors", "NumRotatableBonds"]   
    descriptors = pd.DataFrame(data=baseData,columns=columnNames)
    
    return descriptors
107/16: print(df4['smiles'].unique())
107/17:
dfall_lipinski = lipinski(df4.smiles)
dfall_lipinski
107/18: print(df4['kpu'].unique())
107/19:
df4_all = df4.reset_index(drop=True)
df4_all_lipinski = dfall_lipinski.reset_index(drop=True)
data2 = pd.concat([df4_all,df4_all_lipinski], axis=1)
data2.to_csv('data2_lipinski_kpu.csv')
data2
107/20:
#create smi files 
#re-think
selection = ['smiles', 'compound']
df_selection = data2[selection]
df_selection.to_csv('molecules.smi', sep='\t', index=False, header=False)
df_selection
107/21:
#create smi files 
#re-think
selection = ['smiles', 'compound']
df_selection = data2[selection]
df_selection.to_csv('molecules.smi', sep='\t', index=False, header=False)
! cat moleculkes.smi
107/22:
#create smi files 
#re-think
selection = ['smiles', 'compound']
df_selection = data2[selection]
df_selection.to_csv('molecules.smi', sep='\t', index=False, header=False)
! cat molecules.smi
105/1:
>>> from rdkit import Chem
>>> from mordred import Calculator, descriptors

# create descriptor calculator with all descriptors
>>> calc = Calculator(descriptors, ignore_3D=True)

>>> len(calc.descriptors)
1613

>>> len(Calculator(descriptors, ignore_3D=True, version="1.0.0"))
1612

# calculate single molecule
>>> mol = Chem.MolFromSmiles('c1ccccc1')
>>> calc(mol)[:3]
[4.242640687119286, 3.9999999999999996, 0]

# calculate multiple molecule
>>> mols = [Chem.MolFromSmiles(smi) for smi in ['c1ccccc1Cl', 'c1ccccc1O', 'c1ccccc1N']]

# as pandas
>>> df = calc.pandas(mols)
>>> df['SLogP']
0    2.3400
1    1.3922
2    1.2688
Name: SLogP, dtype: float64
105/2:
from rdkit import Chem
from mordred import Calculator, descriptors

# create descriptor calculator with all descriptors
>>> calc = Calculator(descriptors, ignore_3D=True)

>>> len(calc.descriptors)
1613

>>> len(Calculator(descriptors, ignore_3D=True, version="1.0.0"))
1612

# calculate single molecule
>>> mol = Chem.MolFromSmiles('c1ccccc1')
>>> calc(mol)[:3]
[4.242640687119286, 3.9999999999999996, 0]

# calculate multiple molecule
>>> mols = [Chem.MolFromSmiles(smi) for smi in ['c1ccccc1Cl', 'c1ccccc1O', 'c1ccccc1N']]

# as pandas
>>> df = calc.pandas(mols)
>>> df['SLogP']
0    2.3400
1    1.3922
2    1.2688
Name: SLogP, dtype: float64
105/3:
from rdkit import Chem
from mordred import Calculator, descriptors

# create descriptor calculator with all descriptors
calc = Calculator(descriptors, ignore_3D=True)
105/4: len(calc)
100/33:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
100/34: pwd
100/35:
df = pd.read_csv('data2_lipinski_kpu.csv')
df
100/36:
df1 =df.drop(['Unnamed: 0'], axis =1)
df1
100/37: X = df1.drop(['compound','smiles', 'kp','kpu', 'Cells'], axis =1)
100/38: X
100/39:
Y = df1['kpu']
Y
100/40: df2 = pd.concat([X, Y], 1)
100/41: df2
100/42: X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)
100/43: df2
100/44: X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)
100/45: X_train.shape, Y_train.shape
100/46: X_test.shape, Y_test.shape
100/47:
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(n_estimators=100)
model.fit(X_train, Y_train)
r2 = model.score(X_test, Y_test)
r2
100/48: Y_pred = model.predict(X_test)
100/49:
import matplotlib.pyplot as plt

sns.set(color_codes=True)
sns.set_style("white")
ax = sns.regplot(Y_test, Y_pred, scatter_kws={'alpha':0.4})
ax.set_xlabel('Experimental kpuu', fontsize='large', fontweight='bold')
ax.set_ylabel('Predicted pkuu', fontsize='large', fontweight='bold')
ax.figure.set_size_inches(5, 5)
plt.show
100/50:
from rdkit import Chem
from mordred import Calculator, all_descriptors

# create descriptor calculator with all descriptors
calc = Calculator(all_descriptors())
100/51:
from rdkit import Chem
from mordred import Calculator, all_descriptors

# create descriptor calculator with all descriptors
calc = Calculator(descriptors, ignore_3D=True)
100/52:
from rdkit import Chem
from mordred import Calculator

# create descriptor calculator with all descriptors
calc = Calculator(descriptors, ignore_3D=True)
100/53:
from rdkit import Chem
from mordred import Calculator, descriptors

# create descriptor calculator with all descriptors
calc = Calculator(descriptors, ignore_3D=True)
100/54: len(calc)
100/55: df3 = pd.read_csv('molecules.csv')
100/56: df3 = pd.read_csv('molecules3.csv')
100/57: python -m mordred -i molecules.smi -o molecules.csv
100/58: !python -m mordred -i molecules.smi -o molecules.csv
100/59: !python -m mordred -t smi /molecules.smi -o molecules.csv
100/60: pwd
100/61: !python -m mordred -t smi \molecules.smi -o molecules.csv
100/62: df3 = pd.read_csv('molecules.csv')
100/63: df3
100/64: df3.shape
100/65: df4.dropna()
100/66: df4 = df3.dropna()
100/67: df4
100/68: df4 = df3.dropna(axis='columns', how='all')
100/69: df4
100/70: df4 = df3.dropna(axis='columns', how='any')
100/71: df4
100/72: df4.column()
100/73: df4.column.unique()
100/74: df4.unique()
100/75: print(df4.column)
100/76: print(df4.columns)
100/77: print(df4.columns.unqiue)
100/78: print(df4.columns.unique)
100/79: df4.columns.unique
100/80:
df4.columns.unique
df4.to_csv('Fingerprint.csv')
100/81:
Y = df1['kpu']
Y
100/82: df5 = pd.concat([df4, Y], axis =1)
100/83: df5
100/84: X = df4
100/85: df5 = pd.concat([X, Y], axis =1)
100/86: df5
100/87: df6 = df5.drop('name')
100/88: df6 = df5.drop(['name'], axis=1)
100/89: df6
100/90: df5 = pd.concat([df4, Y], axis =1)
100/91: df5
100/92: df6 = df5.drop(['name'], axis=1)
100/93: df6
100/94: X = df6.drop(['kpu', axis =1])
100/95: X = df6.drop(['kpu'], , axis=1)
100/96: X = df6.drop(['kpu'],axis=1)
100/97: X = df6.drop(['kpu'], axis=1)
100/98: X,shape
100/99: X.shape
100/100: Y.shape
100/101:
from sklearn.model_selection import KFold
kf = KFold(n_splits=10)
kf
100/102:
from sklearn.model_selection import KFold
kf = KFold(n_splits=10, shuffle=True)
kf
100/103:
from sklearn.model_selection import cross_val_score
model = GaussianNB()
cvs = cross_val_score(model, X, Y, CV=10)
100/104:
from sklearn.model_selection import cross_val_score
from sklearn.naive_bayes import GaussianNB
model = GaussianNB()
cvs = cross_val_score(model, X, Y, CV=10)
100/105:
from sklearn.model_selection import cross_val_score
from sklearn.naive_bayes import GaussianNB
model = GaussianNB()
cvs = cross_val_score(model, X, Y, cv=10)
100/106: csv
100/107: cvs
100/108: y = df6(['kpu'], axis = 1)
100/109: y = df6['kpu']
100/110:
y = df6['kpu']
y
100/111:
from sklearn.model_selection import cross_val_score
from sklearn.naive_bayes import GaussianNB
model = GaussianNB()
cvs = cross_val_score(model, X, y, cv=10)
100/112:
from sklearn.model_selection import cross_val_score
from sklearn.naive_bayes import GaussianNB
model = GaussianNB()
100/113: X_train, X_test, y_train, y_test = train_set_split(X, y, test_size = 0.2)
100/114:
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_set_split(X, y, test_size = 0.2)
100/115:
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_set_split(X, y, test_size = 0.2)
100/116:
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)
100/117:
from sklearn.naive_bayes import GaussianNB
model = GaussianNB()
model.fit(X_train,y_train)
model.score(X_test,y_train)
100/118:
from sklearn.naive_bayes import GaussianNB
model = GaussianNB()
model.fit(X_train,y_train)
model.score(X_train,y_train)
100/119: X.type()
100/120: type(X)
100/121: X.dtypes
100/122: X.dtypes
100/123: Y.dtypes
100/124: y.dtypes
100/125: X.dtypes.unique()
100/126: X = X.astype('int')
100/127:
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)
100/128:
from sklearn.naive_bayes import GaussianNB
model = GaussianNB()
model.fit(X_train,y_train)
model.score(X_train,y_train)
100/129:
from sklearn.naive_bayes import RandomForestRegressor
model = RandomForestRegressor()
model.fit(X_train,y_train)
model.score(X_train,y_train)
100/130:
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor()
model.fit(X_train,y_train)
model.score(X_train,y_train)
100/131:
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor()
model.fit(X_train,y_train)
model.score(X_train,y_train)
100/132:
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor()
model.fit(X_train,y_train)
r2 = model.score(X_test, Y_test)
r2
100/133:
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor()
model.fit(X_train,y_train)
r2 = model.score(X_test, Y_test)
r2
100/134: code issues!
100/135:
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor()
model.fit(X_train,y_train)
r2 = model.score(X_test, Y_test)
r2
100/136:
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor()
model.fit(X_train,y_train)
r2 = model.score(X_train,y_train)
r2
100/137:
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor()
model.fit(X_train,y_train)
r2 = model.score(X_train,y_train)
r2
100/138:
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor()
model.fit(X_train,y_train)
r2 = model.score(X_train,y_train)
r2
100/139:
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor()
model.fit(X_train,y_train)
r2 = model.score(X_train,y_train)
r2
100/140:
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor()
model.fit(X_train,y_train)
r2 = model.score(X_train,y_train)
r2
100/141:
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor()
model.fit(X_train,y_train)
r2 = model.score(X_train,y_train)
r2
100/142: Y_pred = model.predict(X_test)
100/143: y_pred = model.predict(X_test)
100/144:
import matplotlib.pyplot as plt

sns.set(color_codes=True)
sns.set_style("white")

ax = sns.regplot(y_test, y_pred, scatter_kws={'alpha':0.4})
ax.set_xlabel('Experimental Kpu', fontsize='large', fontweight='bold')
ax.set_ylabel('Predicted Kpu', fontsize='large', fontweight='bold')
ax.set_xlim(0, 12)
ax.set_ylim(0, 12)
ax.figure.set_size_inches(5, 5)
plt.show
100/145:
import matplotlib.pyplot as plt

sns.set(color_codes=True)
sns.set_style("white")

ax = sns.regplot(y_test, y_pred, scatter_kws={'alpha':0.4})
ax.set_xlabel('Experimental Kpu', fontsize='large', fontweight='bold')
ax.set_ylabel('Predicted Kpu', fontsize='large', fontweight='bold')
ax.set_xlim(0, 12)
ax.set_xscale('log')
ax.set_yscale('log')
ax.set_ylim(0, 12)
ax.figure.set_size_inches(5, 5)
plt.show
100/146:
import matplotlib.pyplot as plt

sns.set(color_codes=True)
sns.set_style("white")

ax = sns.regplot(y_test, y_pred, scatter_kws={'alpha':0.4})
ax.set_xlabel('Experimental Kpu', fontsize='large', fontweight='bold')
ax.set_ylabel('Predicted Kpu', fontsize='large', fontweight='bold')

ax.set_xscale('log')
ax.set_yscale('log')

ax.figure.set_size_inches(5, 5)
plt.show
100/147:
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score
model2 = RandomForestRegressor()
model2.fit()
cvs = cross_val_score(model2, X, y, cv=10)
print('cross val scores {}'.format(cvs))
print('mean {:4f}'.format(cvs.mean()), 'sd {:4}'.format(cvs.sd()))
100/148:
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score
model2 = RandomForestRegressor()
cvs = cross_val_score(model2, X, y, cv=10)
print('cross val scores {}'.format(cvs))
print('mean {:4f}'.format(cvs.mean()), 'sd {:4}'.format(cvs.sd()))
100/149:
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score
model2 = RandomForestRegressor()
cvs = cross_val_score(model2, X, y, cv=10)
print('cross val scores {}'.format(cvs))
print('mean {:4f}'.format(cvs.mean()))
100/150:
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score
model = RandomForestRegressor()
cvs = cross_val_score(model, X, y, cv=10)
print('cross val scores {}'.format(cvs))
print('mean {:4f}'.format(cvs.mean()))
109/1:
import matplotlib.pyplot as plt

sns.set(color_codes=True)
sns.set_style("white")

ax = sns.regplot(y_test, y_pred, scatter_kws={'alpha':0.4})
ax.set_xlabel('Experimental Kpu', fontsize='large', fontweight='bold')
ax.set_ylabel('Predicted Kpu', fontsize='large', fontweight='bold')


ax.figure.set_size_inches(5, 5)
plt.show
109/2:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
109/3: pwd
110/1: pwd
110/2: df = pd.read_csv('Fingerprint.csv')
110/3:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
110/4: df = pd.read_csv('Fingerprint.csv')
110/5: df
110/6: df1 = df.drop('Unnamed: 0')
110/7: df1 = df.drop('Unnamed: 0', axis =1)
110/8: df1
110/9: df2 = pd.read_csv('data2_lipinski_kpu.csv')
110/10: df2
110/11:
df3 = df2.drop(['smiles','MW','TPSA','LogP'])
df3
110/12:
df3 = df2.drop(['smiles','MW','TPSA','LogP'], axis=1)
df3
110/13:
df3 = df2.drop(['Unnamed: 0','smiles','MW','TPSA','LogP'], axis=1)
df3
110/14:
df3 = df2.drop(['Unnamed: 0','compound','smiles','MW','TPSA','LogP'], axis=1)
df3
110/15: df4 =pd.concat([df1,df3], axis=1)
110/16: df4
110/17: df4
110/18:
from sklearn import preprocessing
min_max_scaler = preprocessing.MinMaxScaler()
np_scaled = min_max_scaler.fit_transform()
110/19: X = df4.drop(['name', 'kp', 'kpu','Cells'], axis=1)
110/20:
X = df4.drop(['name', 'kp', 'kpu','Cells'], axis=1)
y = df4('kp')
110/21:
X = df4.drop(['name', 'kp', 'kpu','Cells'], axis=1)
y = df4['kp']
110/22:
from sklearn import preprocessing
min_max_scaler = preprocessing.MinMaxScaler()
np_scaled = min_max_scaler.fit_transform(X)
110/23: np_scaled
110/24:
from sklearn import preprocessing
min_max_scaler = preprocessing.MinMaxScaler()
np_scaled = min_max_scaler.fit_transform(X)
Fp_normalized = pd.DataFrame(np_scaled)
Fp_normalized
110/25:
from sklearn import preprocessing
min_max_scaler = preprocessing.MinMaxScaler()
np_scaled = min_max_scaler.fit_transform(X)
Fp_normalized = pd.DataFrame(np_scaled, columns=X.columns)
110/26:
from sklearn import preprocessing
min_max_scaler = preprocessing.MinMaxScaler()
np_scaled = min_max_scaler.fit_transform(X)
Fp_normalized = pd.DataFrame(np_scaled, columns=X.columns)
Fp_normalized
110/27:
from sklearn import preprocessing
min_max_scaler = preprocessing.MinMaxScaler()
np_scaled = min_max_scaler.fit_transform(X)
Fp_normalized = pd.DataFrame(np_scaled, columns=X.columns)
Fp_normalized.to_csv('Fp_normalized.csv')
Fp_normalized
110/28:
from sklearn.cross_validation import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.cross_validation import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from sklearn import cross_validation
from collections import defaultdict
110/29:
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from sklearn import cross_validation
from collections import defaultdict
110/30:
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from sklearn.model_selection import cross_validaton
from sklearn.model_selection import cross_val_score
from collections import defaultdict
110/31:
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from sklearn.model_selection import cross_validate
from sklearn.model_selection import cross_val_score
from collections import defaultdict
110/32:
X_internal, X_external, y_internal, y_external = train_test_split(X,
                                                        y, test_size=0.2,
                                                        random_state=seed)
110/33:
X_internal, X_external, y_internal, y_external = train_test_split(X,
                                                        y, test_size=0.2,
                                                        random_state=5)
110/34:
X_train, X_test, y_train, y_test = train_test_split(X,
                                                        y, test_size=0.2,
                                                        random_state=5)
110/35:
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor()
model.fit(X_train,y_train)
r2 = model.score(X_train,y_train)
r2
110/36: y_pred = model.predict(X_train)
110/37: y_pred = model.predict(X_train)
110/38:
import matplotlib.pyplot as plt

ax = plt.scatter(y_test, y_pred, scatter_kws={'alpha':0.4})
ax.set_xlabel('Experimental Kpu', fontsize='large', fontweight='bold')
ax.set_ylabel('Predicted Kpu', fontsize='large', fontweight='bold')
#log
ax.set_xscale('log')
ax.set_yscale('log')

ax.figure.set_size_inches(5, 5)
plt.show
110/39:
import matplotlib.pyplot as plt

ax = plt.scatter(y_train, y_pred, scatter_kws={'alpha':0.4})
ax.set_xlabel('Experimental Kpu', fontsize='large', fontweight='bold')
ax.set_ylabel('Predicted Kpu', fontsize='large', fontweight='bold')
#log
ax.set_xscale('log')
ax.set_yscale('log')

ax.figure.set_size_inches(5, 5)
plt.show
110/40:
import matplotlib.pyplot as plt

ax = plt.scatter(y_train, y_pred, alpha=0.4)
ax.set_xlabel('Experimental Kpu', fontsize='large', fontweight='bold')
ax.set_ylabel('Predicted Kpu', fontsize='large', fontweight='bold')
#log
ax.set_xscale('log')
ax.set_yscale('log')

ax.figure.set_size_inches(5, 5)
plt.show
110/41:
import matplotlib.pyplot as plt

ax = plt.scatter(y_train, y_pred, alpha=0.4)
ax.xlabel('Experimental Kpu', fontsize='large', fontweight='bold')
ax.ylabel('Predicted Kpu', fontsize='large', fontweight='bold')
ax.xscale('log')
ax.yscale('log')

ax.figure.set_size_inches(5, 5)
plt.show
110/42:
import matplotlib.pyplot as plt

ax.scatter(y_train, y_pred, alpha=0.4)
ax.xlabel('Experimental Kpu', fontsize='large', fontweight='bold')
ax.ylabel('Predicted Kpu', fontsize='large', fontweight='bold')
ax.xscale('log')
ax.yscale('log')

ax.figure.set_size_inches(5, 5)
plt.show
110/43:
import matplotlib.pyplot as plt

plt.scatter(y_train, y_pred, alpha=0.4)
ax.xlabel('Experimental Kpu', fontsize='large', fontweight='bold')
ax.ylabel('Predicted Kpu', fontsize='large', fontweight='bold')
ax.xscale('log')
ax.yscale('log')

ax.figure.set_size_inches(5, 5)
plt.show
110/44:
import matplotlib.pyplot as plt

plt.scatter(y_train, y_pred, alpha=0.4)
plt.xlabel('Experimental Kpu', fontsize='large', fontweight='bold')
plt.ylabel('Predicted Kpu', fontsize='large', fontweight='bold')
plt.xscale('log')
plt.yscale('log')

ax.figure.set_size_inches(5, 5)
plt.show
110/45:
import matplotlib.pyplot as plt

plt.scatter(y_train, y_pred, alpha=0.5)
plt.xlabel('Experimental Kpu', fontsize='large', fontweight='bold')
plt.ylabel('Predicted Kpu', fontsize='large', fontweight='bold')
plt.xscale('log')
plt.yscale('log')

ax.figure.set_size_inches(5, 5)
plt.show
110/46:
import matplotlib.pyplot as plt

plt.scatter(y_train, y_pred, alpha=1)
plt.xlabel('Experimental Kpu', fontsize='large', fontweight='bold')
plt.ylabel('Predicted Kpu', fontsize='large', fontweight='bold')
plt.xscale('log')
plt.yscale('log')

ax.figure.set_size_inches(5, 5)
plt.show
110/47:
import matplotlib.pyplot as plt

plt.scatter(y_train, y_pred, alpha=0.4)
plt.xlabel('Experimental Kpu', fontsize='large', fontweight='bold')
plt.ylabel('Predicted Kpu', fontsize='large', fontweight='bold')
plt.xscale('log')
plt.yscale('log')

ax.figure.set_size_inches(5, 5)
plt.show
110/48:
y1_pred = model.predict(X_train)
y2_pred = model.predict(X_test)
110/49:
import matplotlib.pyplot as plt
color 
plt.scatter(y_train, y1_pred, alpha=0.4, c='coral')
plt.scatter(y_test, y2_pred, alpha=0.4, c='lightblue')
plt.xlabel('Experimental Kpu', fontsize='large', fontweight='bold')
plt.ylabel('Predicted Kpu', fontsize='large', fontweight='bold')
plt.xscale('log')
plt.yscale('log')

ax.figure.set_size_inches(5, 5)
plt.show
110/50:
import matplotlib.pyplot as plt
plt.scatter(y_train, y1_pred, alpha=0.4, c='coral')
plt.scatter(y_test, y2_pred, alpha=0.4, c='lightblue')
plt.xlabel('Experimental Kpu', fontsize='large', fontweight='bold')
plt.ylabel('Predicted Kpu', fontsize='large', fontweight='bold')
plt.xscale('log')
plt.yscale('log')

ax.figure.set_size_inches(5, 5)
plt.show
110/51:
import matplotlib.pyplot as plt
plt.scatter(y_train, y1_pred, alpha=0.7, c='coral')
plt.scatter(y_test, y2_pred, alpha=0.7, c='lightblue')
plt.xlabel('Experimental Kpu', fontsize='large', fontweight='bold')
plt.ylabel('Predicted Kpu', fontsize='large', fontweight='bold')
plt.xscale('log')
plt.yscale('log')

ax.figure.set_size_inches(5, 5)
plt.show
110/52:
import matplotlib.pyplot as plt
plt.scatter(y_train, y1_pred, alpha=0.7, c='coral')
plt.scatter(y_test, y2_pred, alpha=0.7, c='lightblue')
plt.xlabel('Experimental Kpu', fontsize='large', fontweight='bold')
plt.ylabel('Predicted Kpu', fontsize='large', fontweight='bold')
plt.xscale('log')
plt.yscale('log')
plt.legend(loc'upper right')
ax.figure.set_size_inches(5, 5)
plt.show
110/53:
import matplotlib.pyplot as plt
plt.scatter(y_train, y1_pred, alpha=0.7, c='coral')
plt.scatter(y_test, y2_pred, alpha=0.7, c='lightblue')
plt.xlabel('Experimental Kpu', fontsize='large', fontweight='bold')
plt.ylabel('Predicted Kpu', fontsize='large', fontweight='bold')
plt.xscale('log')
plt.yscale('log')
plt.legend(loc='upper right')
ax.figure.set_size_inches(5, 5)
plt.show
110/54:
import matplotlib.pyplot as plt
plt.scatter(y_train, y1_pred, alpha=0.7, c='coral')
plt.scatter(y_test, y2_pred, alpha=0.7, c='lightblue')
plt.xlabel('Experimental Kpu', fontsize='large', fontweight='bold')
plt.ylabel('Predicted Kpu', fontsize='large', fontweight='bold')
plt.xscale('log')
plt.yscale('log')
plt.legend()
ax.figure.set_size_inches(5, 5)
plt.show
110/55:
import matplotlib.pyplot as plt
plt.scatter(y_train, y1_pred, alpha=0.7, c='coral', label='Inline label')
plt.scatter(y_test, y2_pred, alpha=0.7, c='lightblue', label='Inline label')
plt.xlabel('Experimental Kpu', fontsize='large', fontweight='bold')
plt.ylabel('Predicted Kpu', fontsize='large', fontweight='bold')
plt.xscale('log')
plt.yscale('log')
plt.legend()
ax.figure.set_size_inches(5, 5)
plt.show
110/56:
import matplotlib.pyplot as plt
plt.scatter(y_train, y1_pred, alpha=0.7, c='coral', label='Traing set')
plt.scatter(y_test, y2_pred, alpha=0.7, c='lightblue', label='Test set')
plt.xlabel('Experimental Kpu', fontsize='large', fontweight='bold')
plt.ylabel('Predicted Kpu', fontsize='large', fontweight='bold')
plt.xscale('log')
plt.yscale('log')
plt.legend()
ax.figure.set_size_inches(5, 5)
plt.show
110/57:
import matplotlib.pyplot as plt
plt.scatter(y_train, y1_pred, alpha=0.7, c='coral', label='Traing set')
plt.scatter(y_test, y2_pred, alpha=0.7, c='lightblue', label='Test set')
plt.xlabel('Experimental Kpu', fontsize='large', fontweight='bold')
plt.ylabel('Predicted Kpu', fontsize='large', fontweight='bold')
plt.xscale('log')
plt.yscale('log')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.legend()
ax.figure.set_size_inches(5, 5)
plt.show
110/58:
import matplotlib.pyplot as plt
plt.scatter(y_train, y1_pred, alpha=0.7, c='coral', label='Traing set')
plt.scatter(y_test, y2_pred, alpha=0.7, c='lightblue', label='Test set')
plt.xlabel('Experimental Kpu', fontsize='16', fontweight='bold')
plt.ylabel('Predicted Kpu', fontsize='16', fontweight='bold')
plt.xscale('log')
plt.yscale('log')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.legend()
ax.figure.set_size_inches(5, 5)
plt.show
110/59:
import matplotlib.pyplot as plt
plt.scatter(y_train, y1_pred, alpha=0.7, c='coral', label='Traing set')
plt.scatter(y_test, y2_pred, alpha=0.7, c='lightblue', label='Test set')
plt.xlabel('Experimental Kp', fontsize='16', fontweight='bold')
plt.ylabel('Predicted Kp', fontsize='16', fontweight='bold')
plt.xscale('log')
plt.yscale('log')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.legend()
ax.figure.set_size_inches(5, 5)
plt.show
110/60:
import matplotlib.pyplot as plt
plt.scatter(y_train, y1_pred, alpha=0.7, c='coral', label='Traing set')
plt.scatter(y_test, y2_pred, alpha=0.7, c='lightblue', label='Test set')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.xscale('log')
plt.yscale('log')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.legend()
ax.figure.set_size_inches(5, 5)
plt.show
110/61:
import matplotlib.pyplot as plt
plt.scatter(y_train, y1_pred, alpha=0.7, c='coral', label='Traing set')
plt.scatter(y_test, y2_pred, alpha=0.7, c='blue', label='Test set')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.xscale('log')
plt.yscale('log')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.legend()
ax.figure.set_size_inches(5, 5)
plt.show
110/62:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y_train, y1_pred, alpha=0.7, c='coral', label='Traing set')
plt.scatter(y_test, y2_pred, alpha=0.7, c='blue', label='Test set')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.legend()
ax.figure.set_size_inches(5, 5)
plt.show
110/63:
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score
model = RandomForestRegressor()
cvs = cross_val_score(model, X, y, cv=10)
110/64: cvs
110/65:
from sklearn.model_selection import cross_val_score
cvs = cross_val_score(model, X, y, cv=5)
110/66:
from sklearn.model_selection import cross_val_score
scores= cross_val_score(model, X, y, cv=5)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
109/4:
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score

kf =KFold(n_splits=5, shuffle=True, random_state=42)
110/67:
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score
kf =KFold(n_splits=10, shuffle=True, random_state=5)
110/68:
cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1
110/69:
def rmse(score):
    rmse = np.sqrt(-score)
    print(f'rmse= {"{:.2f}".format(rmse)}')
110/70:
score = cross_val_score(model, X, y, cv= kf, scoring="neg_mean_squared_error")
print(f'Scores for each fold: {score}')
rmse(score.mean())
110/71:
score = cross_val_score(model, X, y, cv= kf)
print(f'Scores for each fold: {score}')
rmse(score.mean())
110/72:
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score
kf =KFold(n_splits=10, shuffle=True, random_state=40)
110/73:
cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1
110/74:
def rmse(score):
    rmse = np.sqrt(-score)
    print(f'rmse= {"{:.2f}".format(rmse)}')
110/75:
score = cross_val_score(model, X, y, cv= kf)
print(f'Scores for each fold: {score}')
rmse(score.mean())
110/76:
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score
kf =KFold(n_splits=10, shuffle=True, random_state=5)
110/77:
cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1
110/78:
def rmse(score):
    rmse = np.sqrt(-score)
    print(f'rmse= {"{:.2f}".format(rmse)}')
110/79:
score = cross_val_score(model, X, y, cv= kf)
print(f'Scores for each fold: {score}')
rmse(score.mean())
110/80:
from sklearn import LogisticRegression
clf = LogisticRegression(random_state=0).fit(X, y)
110/81:
from sklearn.linear_model import LogisticRegression
clf = LogisticRegression(random_state=0).fit(X, y)
110/82:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y_train, y1_pred, alpha=0.7, c='coral', label='Traing set')
plt.scatter(y_test, y2_pred, alpha=0.7, c='blue', label='Test set')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')

plt.tick_params(axis='both', which='major', labelsize=14)
plt.legend()
ax.figure.set_size_inches(5, 5)
plt.show
110/83:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y_train, y1_pred, alpha=0.7, c='coral', label='Traing set')
plt.scatter(y_test, y2_pred, alpha=0.7, c='blue', label='Test set')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.legend()
ax.figure.set_size_inches(5, 5)
plt.show
110/84:
from sklearn.svm import SVR
svr_rbf = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1)
svr_lin = SVR(kernel='linear', C=100, gamma='auto')
svr_poly = SVR(kernel='poly', C=100, gamma='auto', degree=3, epsilon=.1,
               coef0=1)
110/85:
# Look at the results
lw = 2

svrs = [svr_rbf, svr_lin, svr_poly]
kernel_label = ['RBF', 'Linear', 'Polynomial']
model_color = ['m', 'c', 'g']
110/86:
fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 10), sharey=True)
for ix, svr in enumerate(svrs):
    axes[ix].plot(X, svr.fit(X_train, y_train).predict(X_test), color=model_color[ix], lw=lw,
                  label='{} model'.format(kernel_label[ix]))
    axes[ix].scatter(X_train[svr.support_], y[svr.support_], facecolor="none",
                     edgecolor=model_color[ix], s=50,
                     label='{} support vectors'.format(kernel_label[ix]))
    axes[ix].scatter(X[np.setdiff1d(np.arange(len(X)), svr.support_)],
                     y[np.setdiff1d(np.arange(len(X)), svr.support_)],
                     facecolor="none", edgecolor="k", s=50,
                     label='other training data')
    axes[ix].legend(loc='upper center', bbox_to_anchor=(0.5, 1.1),
                    ncol=1, fancybox=True, shadow=True)

fig.text(0.5, 0.04, 'data', ha='center', va='center')
fig.text(0.06, 0.5, 'target', ha='center', va='center', rotation='vertical')
fig.suptitle("Support Vector Regression", fontsize=14)
plt.show()
110/87:
fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 10), sharey=True)
for ix, svr in enumerate(svrs):
    axes[ix].plot(X_train, svr.fit(X_train, y_train).predict(X_test), color=model_color[ix], lw=lw,
                  label='{} model'.format(kernel_label[ix]))
    axes[ix].scatter(X_train[svr.support_], y)train[svr.support_], facecolor="none",
                     edgecolor=model_color[ix], s=50,
                     label='{} support vectors'.format(kernel_label[ix]))
    axes[ix].scatter(X_train[np.setdiff1d(np.arange(len(X)), svr.support_)],
                     y_train[np.setdiff1d(np.arange(len(X)), svr.support_)],
                     facecolor="none", edgecolor="k", s=50,
                     label='other training data')
    axes[ix].legend(loc='upper center', bbox_to_anchor=(0.5, 1.1),
                    ncol=1, fancybox=True, shadow=True)

fig.text(0.5, 0.04, 'data', ha='center', va='center')
fig.text(0.06, 0.5, 'target', ha='center', va='center', rotation='vertical')
fig.suptitle("Support Vector Regression", fontsize=14)
plt.show()
110/89:
fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 10), sharey=True)
for ix, svr in enumerate(svrs):
    axes[ix].plot(X_train, svr.fit(X_train, y_train).predict(X_test), color=model_color[ix], lw=lw,
                  label='{} model'.format(kernel_label[ix]))
    axes[ix].scatter(X_train[svr.support_], y)train[svr.support_], facecolor="none",
                     edgecolor=model_color[ix], s=50,
                     label='{} support vectors'.format(kernel_label[ix]))
    axes[ix].scatter(X_train[np.setdiff1d(np.arange(len(X)), svr.support_)],
                     y_train[np.setdiff1d(np.arange(len(X)), svr.support_)],
                     facecolor="none", edgecolor="k", s=50,
                     label='other training data')
    axes[ix].legend(loc='upper center', bbox_to_anchor=(0.5, 1.1), ncol=1, fancybox=True, shadow=True)

fig.text(0.5, 0.04, 'data', ha='center', va='center')
fig.text(0.06, 0.5, 'target', ha='center', va='center', rotation='vertical')
fig.suptitle("Support Vector Regression", fontsize=14)
plt.show()
110/90:
fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 10), sharey=True)
for ix, svr in enumerate(svrs):
    axes[ix].plot(X_train, svr.fit(X_train, y_train).predict(X_test), color=model_color[ix], lw=lw,
                  label='{} model'.format(kernel_label[ix]))
    axes[ix].scatter(X_train[svr.support_], y_train[svr.support_], facecolor="none",
                     edgecolor=model_color[ix], s=50,
                     label='{} support vectors'.format(kernel_label[ix]))
    axes[ix].scatter(X_train[np.setdiff1d(np.arange(len(X)), svr.support_)],
                     y_train[np.setdiff1d(np.arange(len(X)), svr.support_)],
                     facecolor="none", edgecolor="k", s=50,
                     label='other training data')
    axes[ix].legend(loc='upper center', bbox_to_anchor=(0.5, 1.1), ncol=1, fancybox=True, shadow=True)

fig.text(0.5, 0.04, 'data', ha='center', va='center')
fig.text(0.06, 0.5, 'target', ha='center', va='center', rotation='vertical')
fig.suptitle("Support Vector Regression", fontsize=14)
plt.show()
110/91:
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score
kf =KFold(n_splits=5, shuffle=True, random_state=5)
110/92:
cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1
110/93:
score = cross_val_score(model, X, y, cv= kf)
print(f'Scores for each fold: {score}')
rmse(score.mean())
110/94:
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score
kf =KFold(n_splits=5, shuffle=True)
110/95:
cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1
110/96:
def rmse(score):
    rmse = np.sqrt(-score)
    print(f'rmse= {"{:.2f}".format(rmse)}')
110/97:
score = cross_val_score(model, X, y, cv= kf)
print(f'Scores for each fold: {score}')
rmse(score.mean())
110/98:
from sklearn.svm import SVC
model2 = SVC(kernel='polynomial')
core = cross_val_score(model2, X, y, cv= kf)
110/99:
from sklearn.svm import SVC
model2 = SVC(kernel='linear')
core = cross_val_score(model2, X, y, cv= kf)
110/100:
from sklearn.svm import SVR
model2 = SVR(kernel='linear')
score = cross_val_score(model2, X, y, cv= kf)
score
128/1: pwd
128/2:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
128/3: df = pd.read_csv('Fingerprint.csv')
128/4: df
128/5: df1 = df.drop('Unnamed: 0', axis =1)
128/6: df1
128/7: df2 = pd.read_csv('data2_lipinski_kpu.csv')
128/8: df2
128/9:
df3 = df2.drop(['Unnamed: 0','compound','smiles','MW','TPSA','LogP'], axis=1)
df3
128/10: df4 =pd.concat([df1,df3], axis=1)
128/11: df4
128/12:
X = df4.drop(['name', 'kp', 'kpu','Cells'], axis=1)
y = df4['kp']
128/13:
from sklearn import preprocessing
min_max_scaler = preprocessing.MinMaxScaler()
np_scaled = min_max_scaler.fit_transform(X)
Fp_normalized = pd.DataFrame(np_scaled, columns=X.columns)
Fp_normalized.to_csv('Fp_normalized.csv')
Fp_normalized
128/14:
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from sklearn.model_selection import cross_validate
from sklearn.model_selection import cross_val_score
from collections import defaultdict
128/15:
X_train, X_test, y_train, y_test = train_test_split(X,
                                                        y, test_size=0.2,
                                                        random_state=5)
128/16:
#rf = RandomForestRegressor(n_estimators=400, 
                               max_features='sqrt', 
                               min_samples_leaf = 1,
                               random_state = 13,
                               n_jobs=-1)
    
    rf.fit(X_internal,Y_internal)
    prediction = rf.predict(X_internal)
128/17:
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor()
model.fit(X_train,y_train)
r2 = model.score(X_train,y_train)
r2
128/18:
y1_pred = model.predict(X_train)
y2_pred = model.predict(X_test)
128/19:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y_train, y1_pred, alpha=0.7, c='coral', label='Traing set')
plt.scatter(y_test, y2_pred, alpha=0.7, c='blue', label='Test set')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.legend()
ax.figure.set_size_inches(5, 5)
plt.show
128/20:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y_train, y1_pred, alpha=0.7, c='coral', label='Traing set')
plt.scatter(y_test, y2_pred, alpha=0.7, c='blue', label='Test set')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.legend()
plt.figure.set_size_inches(5, 5)
plt.show
128/21:
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import ShuffleSplit
scores= cross_val_score(model, X, y, cv=5)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
128/22:
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score
kf =KFold(n_splits=5, shuffle=True)
128/23:
cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1
128/24:
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import ShuffleSplit
scores= cross_val_score(model, X, y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
128/25:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y_train, y1_pred, alpha=0.7, c='coral', label='Traing set')
plt.scatter(y_test, y2_pred, alpha=0.7, c='blue', label='Test set')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.legend()
#save file
plt.tight_layout()
plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)
128/26: scores
128/27:
from sklearn.svm import SVR
model2 = SVR(kernel='polynomial')
score = cross_val_score(model2, X, y, cv= kf)
score
128/28:
from sklearn.svm import SVR
model2 = SVR(kernel='linear')
score = cross_val_score(model2, X, y, cv= kf)
score
129/1:
df4 =pd.concat([df1,df3], axis=1)
df4.to_csv('Fn_normalized_kpu.csv')
129/2: pwd
129/3:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
129/4: df = pd.read_csv('Fingerprint.csv')
129/5: df
129/6: df1 = df.drop('Unnamed: 0', axis =1)
129/7: df1
129/8: df2 = pd.read_csv('data2_lipinski_kpu.csv')
129/9: df2
129/10: df2
129/11:
df3 = df2.drop(['Unnamed: 0','compound','smiles','MW','TPSA','LogP'], axis=1)
df3
129/12:
df4 =pd.concat([df1,df3], axis=1)
df4.to_csv('Fn_normalized_kpu.csv')
129/13: df4
129/14:
X = df4.drop(['name', 'kp', 'kpu','Cells'], axis=1)
y = df4['kp']
129/15:
from sklearn import preprocessing
min_max_scaler = preprocessing.MinMaxScaler()
np_scaled = min_max_scaler.fit_transform(X)
Fp_normalized = pd.DataFrame(np_scaled, columns=X.columns)
Fp_normalized.to_csv('Fp_normalized.csv')
Fp_normalized
129/16:
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from sklearn.model_selection import cross_validate
from sklearn.model_selection import cross_val_score
from collections import defaultdict
129/17:
X_train, X_test, y_train, y_test = train_test_split(X,
                                                        y, test_size=0.2,
                                                        random_state=5)
129/18:
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor()
model.fit(X_train,y_train)
r2 = model.score(X_train,y_train)
r2
129/19:
y1_pred = model.predict(X_train)
y2_pred = model.predict(X_test)
129/20:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y_train, y1_pred, alpha=0.7, c='coral', label='Traing set')
plt.scatter(y_test, y2_pred, alpha=0.7, c='blue', label='Test set')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.legend()
#save file
plt.tight_layout()
plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)
129/21:
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score
kf =KFold(n_splits=5, shuffle=True)
129/22:
cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1
129/23:
def rmse(score):
    rmse = np.sqrt(-score)
    print(f'rmse= {"{:.2f}".format(rmse)}')
129/24:
from sklearn.model_selection import cross_val_score
scores= cross_val_score(model, X, y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
130/1: from sklearn.model_selection import SVR
129/25:
from sklearn import datasets
from sklearn.model_selection import cross_val_predict
from sklearn import linear_model
import matplotlib.pyplot as plt
129/26:
lr = linear_model.LinearRegression()

# cross_val_predict returns an array of the same size as `y` where each entry
# is a prediction obtained by cross validation:
predicted = cross_val_predict(lr, X, y, cv=10)

fig, ax = plt.subplots()
ax.scatter(y, predicted, edgecolors=(0, 0, 0))
ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)
ax.set_xlabel('Measured')
ax.set_ylabel('Predicted')
plt.show()
129/27:
lr = linear_model.LinearRegression()

# cross_val_predict returns an array of the same size as `y` where each entry
# is a prediction obtained by cross validation:
predicted = cross_val_predict(lr, X, y, cv=10)

fig, ax = plt.subplots()
ax.scatter(y, predicted, edgecolors=(0, 0, 0))
ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)
ax.set_xlabel('Measured')
ax.set_ylabel('Predicted')
ax.xscale('log')
ax.yscale('log')
plt.show()
129/28:
lr = linear_model.LinearRegression()

# cross_val_predict returns an array of the same size as `y` where each entry
# is a prediction obtained by cross validation:
predicted = cross_val_predict(lr, X, y, cv=10)

fig, ax = plt.subplots()
ax.scatter(y, predicted, edgecolors=(0, 0, 0))
129/29: y
129/30:
lr = linear_model.LinearRegression()

# cross_val_predict returns an array of the same size as `y` where each entry
# is a prediction obtained by cross validation:
predicted = cross_val_predict(lr, X, y, cv=10)

fig, ax = plt.subplots()
ax.scatter(y, predicted, edgecolors=(0, 0,
129/31:
lr = linear_model.LinearRegression()

# cross_val_predict returns an array of the same size as `y` where each entry
# is a prediction obtained by cross validation:
predicted = cross_val_predict(lr, X, y, cv=10)

fig, ax = plt.subplots()
ax.scatter(y, predicte)
129/32:
lr = linear_model.LinearRegression()

# cross_val_predict returns an array of the same size as `y` where each entry
# is a prediction obtained by cross validation:
predicted = cross_val_predict(lr, X, y, cv=10)

fig, ax = plt.subplots()
ax.scatter(y, predicted)
129/33:
lr = linear_model.LinearRegression()

# cross_val_predict returns an array of the same size as `y` where each entry
# is a prediction obtained by cross validation:
predicted = cross_val_predict(lr, X, y, cv=10)

plt.subplots()
plt.scatter(y, predicted)
plt.xscale('log')
plt.yscale('log')
plt.show()
129/34:
lr = linear_model.LinearRegression()

# cross_val_predict returns an array of the same size as `y` where each entry
# is a prediction obtained by cross validation:
predicted = cross_val_predict(lr, X, y, cv=10)

plt.scatter(y, predicted)
plt.xscale('log')
plt.yscale('log')
plt.show()
129/35:
lr = linear_model.LinearRegression()

# cross_val_predict returns an array of the same size as `y` where each entry
# is a prediction obtained by cross validation:
predicted = cross_val_predict(lr, X, y, cv=10)

plt.scatter(y, predicted)
plt.xscale('log')
plt.yscale('log')
plt.show()
129/36:
lr = linear_model.LinearRegression()

# cross_val_predict returns an array of the same size as `y` where each entry
# is a prediction obtained by cross validation:
predicted = cross_val_predict(lr, X, y, cv=10)

plt.scatter(y, predicted)
plt.xscale('log')
plt.yscale('log')
plt.show()
129/37:
lr = linear_model.LinearRegression()

# cross_val_predict returns an array of the same size as `y` where each entry
# is a prediction obtained by cross validation:
predicted = cross_val_predict(lr, X, y, cv=10)

plt.scatter(y, predicted)
plt.xscale('log')
plt.yscale('log')
[lt]
129/38:
lr = linear_model.LinearRegression()

# cross_val_predict returns an array of the same size as `y` where each entry
# is a prediction obtained by cross validation:
predicted = cross_val_predict(lr, X, y, cv=10)

plt.scatter(y, predicted)
plt.xscale('log')
plt.yscale('log')
plt
129/39:
lr = linear_model.LinearRegression()

# cross_val_predict returns an array of the same size as `y` where each entry
# is a prediction obtained by cross validation:
predicted = cross_val_predict(lr, X, y, cv=10)

plt.scatter(y, predicted)

plt
129/40:
lr = linear_model.LinearRegression()

# cross_val_predict returns an array of the same size as `y` where each entry
# is a prediction obtained by cross validation:
predicted = cross_val_predict(lr, X, y, cv=10)

plt.scatter(y, predicted)

plt.shape()
129/41:
lr = linear_model.LinearRegression()

# cross_val_predict returns an array of the same size as `y` where each entry
# is a prediction obtained by cross validation:
predicted = cross_val_predict(lr, X, y, cv=10)
129/42:
#no CV
import matplotlib.pyplot as plt
plt.scatter(X, y, alpha=0.7, c='coral', label='Traing set')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.legend()
#save file
plt.tight_layout()
plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)
129/43:
#no CV
import matplotlib.pyplot as plt
plt.scatter(predicted, y, alpha=0.7, c='coral', label='Traing set')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.legend()
#save file
plt.tight_layout()
plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)
129/44:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y_train, y1_pred, alpha=0.7, c='coral', label='Traing set')
plt.scatter(y_test, y2_pred, alpha=0.7, c='blue', label='Test set')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.legend()
#save file
plt.tight_layout()
plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)
129/45:
#no CV
import matplotlib.pyplot as plt
plt.scatter(X, X, alpha=0.7, c='coral', label='Traing set')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.legend()
#save file
plt.tight_layout()
plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)
129/46: X
129/47:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, y, alpha=0.7, c='coral', label='Traing set')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.legend()
#save file
plt.tight_layout()
plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)
129/48:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, predicted, alpha=0.7, c='coral', label='Traing set')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.legend()
#save file
plt.tight_layout()
plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)
129/49:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y_train, y1_pred, alpha=0.7, c='coral', label='Traing set')
plt.scatter(y_test, y2_pred, alpha=0.7, c='blue', label='Test set')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.legend()
#save file
plt.tight_layout()
plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)
129/50:
lr = linear_model.LinearRegression()

# cross_val_predict returns an array of the same size as `y` where each entry
# is a prediction obtained by cross validation:
predicted = cross_val_predict(lr, X, y, cv=10)
predicted
129/51:
lr = linear_model.LinearRegression()

# cross_val_predict returns an array of the same size as `y` where each entry
# is a prediction obtained by cross validation:
predicted = cross_val_predict(lr, X, y, cv=10)
predicted.shape
129/52:
lr = linear_model.LinearRegression()

# cross_val_predict returns an array of the same size as `y` where each entry
# is a prediction obtained by cross validation:
predicted = cross_val_predict(lr, X, y, cv=10)
predicted.shape
y.shape
129/53: y.shape
129/54:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, predicted, alpha=0.7, c='coral', label='Traing set')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.legend()
#save file
plt.tight_layout()
plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)
129/55:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, predicted, alpha=0.7, c='coral', label='Traing set')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.xscale('log')
plt.yscale('log')
plt.legend()
#save file
plt.tight_layout()
plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)
129/56:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, predicted, alpha=0.7, c='coral', label='Traing set')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.xscale('log')
plt.yscale('log')
plt.ylim(0, 1000000)
plt.legend()
#save file
plt.tight_layout()
plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)
129/57:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, predicted, alpha=0.7, c='coral', label='Traing set')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.xscale('log')
plt.yscale('log')
plt.ylim(0, 10000)
plt.legend()
#save file
plt.tight_layout()
plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)
129/58:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, predicted, alpha=0.7, c='coral', label='Traing set')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.xscale('log')
plt.yscale('log')
plt.ylim(0, 1)
plt.legend()
#save file
plt.tight_layout()
plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)
129/59:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, predicted, alpha=0.7, c='coral', label='Traing set')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.xscale('log')
plt.yscale('log')
plt.ylim(0, 10)
plt.legend()
#save file
plt.tight_layout()
plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)
129/60:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, predicted, alpha=0.7, c='coral', label='Traing set')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.xscale('log')
plt.yscale('log')
plt.ylim(0, 100)
plt.legend()
#save file
plt.tight_layout()
plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)
129/61:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, predicted, alpha=0.7, c='coral', label='Traing set')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.xscale('log')
plt.yscale('log')
plt.ylim(-1, 100)
plt.legend()
#save file
plt.tight_layout()
plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)
129/62:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, predicted, alpha=0.7, c='coral', label='Traing set')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.xscale('log')
plt.yscale('log')
plt.ylim(-10, 100)
plt.legend()
#save file
plt.tight_layout()
plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)
129/63:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, predicted, alpha=0.7, c='coral', label='Traing set')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.xscale('log')
plt.yscale('log')
plt.ylim(-10, 10)
plt.legend()
#save file
plt.tight_layout()
plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)
129/64:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, predicted, alpha=0.7, c='coral', label='Traing set')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.xscale('log')
plt.yscale('log')
plt.ylim(-10, 1)
plt.legend()
#save file
plt.tight_layout()
plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)
129/65:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, predicted, alpha=0.7, c='coral', label='Traing set')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.ylim(-10, 1)
plt.legend()
#save file
plt.tight_layout()
plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)
129/66:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, predicted, alpha=0.7, c='coral', label='Traing set')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.tick_params(axis='both', which='major', labelsize=14)
plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)
plt.legend()
#save file
plt.tight_layout()
plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)
129/67:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, predicted, alpha=0.7, c='coral', label='Traing set')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)
plt.legend()
#save file
plt.tight_layout()
plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)
129/68:
lr = linear_model.LinearRegression()

# cross_val_predict returns an array of the same size as `y` where each entry
# is a prediction obtained by cross validation:
predicted = cross_val_predict(lr, X, y, cv=1kF)
predicted.shape
129/69:
lr = linear_model.LinearRegression()

# cross_val_predict returns an array of the same size as `y` where each entry
# is a prediction obtained by cross validation:
predicted = cross_val_predict(lr, X, y, cv=kf)
predicted.shape
129/70:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, predicted, alpha=0.7, c='coral', label='Traing set')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)
plt.legend()
#save file
plt.tight_layout()
plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)
129/71:
lr = linear_model.LinearRegression()

# cross_val_predict returns an array of the same size as `y` where each entry
# is a prediction obtained by cross validation:
predicted = cross_val_predict(lr, X, y, cv=k10
predicted.shape
129/72:
lr = linear_model.LinearRegression()

# cross_val_predict returns an array of the same size as `y` where each entry
# is a prediction obtained by cross validation:
predicted = cross_val_predict(lr, X, y, cv=10)
predicted.shape
129/73:
lr = linear_model.LinearRegression()

# cross_val_predict returns an array of the same size as `y` where each entry
# is a prediction obtained by cross validation:
cv = cross_val_score(ls, X, y, cv=kf)
cv
predicted = cross_val_predict(lr, X, y, cv=10)
predicted.shape
129/74:
lr = linear_model.LinearRegression()

# cross_val_predict returns an array of the same size as `y` where each entry
# is a prediction obtained by cross validation:
cv = cross_val_score(lr, X, y, cv=kf)
cv
129/75:
lr = linear_model.LinearRegression()

# cross_val_predict returns an array of the same size as `y` where each entry
# is a prediction obtained by cross validation:
predicted = cross_val_predict(lr, X, y, cv=10)
predicted.shape
129/76: predicted
129/77: predicted = abs(predicted)
129/78:
lr = linear_model.LinearRegression()

# cross_val_predict returns an array of the same size as `y` where each entry
# is a prediction obtained by cross validation:
reg = LinearRegression()
scores= cross_val_score(reg, X, y, cv=kf)
scores
129/79:
lr = linear_model.LinearRegression()

# cross_val_predict returns an array of the same size as `y` where each entry
# is a prediction obtained by cross validation:
scores= cross_val_score(reg, X, y, cv=kf)
scores
129/80:
lr = linear_model.LinearRegression()

# cross_val_predict returns an array of the same size as `y` where each entry
# is a prediction obtained by cross validation:
scores= cross_val_score(lr, X, y, cv=kf)
scores
129/81:
lr = linear_model.LinearRegression()

# cross_val_predict returns an array of the same size as `y` where each entry
# is a prediction obtained by cross validation:
scores = cross_val_score(lr, X, y, cv=kf)
scores
129/82:
lr = linear_model.LinearRegression()

# cross_val_predict returns an array of the same size as `y` where each entry
# is a prediction obtained by cross validation:
lr = linear_model.LinearRegression()
lr.fit(X_train,y_train)
r3 = lr.score(X_train,y_train)
r3
129/83: predicted = lr.predict(X_test)
129/84:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, predicted, alpha=0.7, c='coral', label='Traing set')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)
plt.legend()
#save file
plt.tight_layout()
plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)
129/85:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y_test, predicted, alpha=0.7, c='coral', label='Traing set')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)
plt.legend()
#save file
plt.tight_layout()
plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)
129/86: predicted = lr.predict(X)
129/87:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, predicted, alpha=0.7, c='coral', label='Traing set')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)
plt.legend()
#save file
plt.tight_layout()
plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)
129/88:
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(n_estimators=200)
model.fit(X_train,y_train)
r2 = model.score(X_train,y_train)
r2
129/89:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y_train, y1_pred, alpha=0.7, c='coral', label='Traing set')
plt.scatter(y_test, y2_pred, alpha=0.7, c='blue', label='Test set')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.legend()
#save file
plt.tight_layout()
plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)
129/90: predicted = lr.predict(X_train)
129/91:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, predicted, alpha=0.7, c='coral', label='Traing set')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)
plt.legend()
#save file
plt.tight_layout()
plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)
129/92:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y_train, predicted, alpha=0.7, c='coral', label='Traing set')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)
plt.legend()
#save file
plt.tight_layout()
plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)
129/93:
lr = linear_model.LinearRegression()

# cross_val_predict returns an array of the same size as `y` where each entry
# is a prediction obtained by cross validation:
predicted = cross_val_predict(lr, X_train, y_train, cv=10)
129/94:
lr = linear_model.LinearRegression()

# cross_val_predict returns an array of the same size as `y` where each entry
# is a prediction obtained by cross validation:
predicted = cross_val_predict(lr, X_train, y_train, cv=10)
predicted
129/95:
lr = linear_model.LinearRegression()

# cross_val_predict returns an array of the same size as `y` where each entry
# is a prediction obtained by cross validation:
predicted = cross_validation.cross_val_predict(lr, X_train, y_train, cv=10)
predicted
129/96:
lr = linear_model.LinearRegression()

# cross_val_predict returns an array of the same size as `y` where each entry
# is a prediction obtained by cross validation:
predicted = cross_val_predict(lr, X_train, y_train, cv=10)
predicted
129/97:
lr = linear_model.LinearRegression()

# cross_val_predict returns an array of the same size as `y` where each entry
# is a prediction obtained by cross validation:
predicted = cross_val_predict(lr, X_train, y_train, cv=10)
predicted
fig, ax = plt.subplots()
ax.scatter(y_train, predicted, edgecolors=(0, 0, 0))
129/98:
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score
kf =KFold(n_splits=5, shuffle=True)
129/99:
cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1
130/2:
import numopy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import SVM
130/3:
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import SVM
130/4:
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import svm
130/5: pwd
130/6: Fn = pd.read_csv('Fn_normalized_kpu.csv')
130/7: Fn
130/8: Fn = Fn.drop(['Unnamed: 0'])
130/9: Fn = Fn.drop('Unnamed: 0', axis=1)
130/10:
Fn = Fn.drop('Unnamed: 0', axis=1)
Fn
130/11:
Fn = Fn.drop('Unnamed: 0', axis=1)
Fn
130/12: Fn1 = Fn.drop('Unnamed: 0', axis=1)
130/13: Fn1 = Fn.drop('Unnamed:0', axis=1)
130/14: Fn = pd.read_csv('Fn_normalized_kpu.csv')
130/15: Fn1 = Fn.drop('Unnamed: 0', axis=1)
130/16: Fn1
130/17: Fn2 = pd.read_csv('Fp_nomalized.csv')
130/18: Fn2 = pd.read_csv('Fp_normalized.csv')
130/19: Fn2
129/100:
df4 =pd.concat([df1,df3], axis=1)
df4.to_csv('Fn_kpu.csv')
129/101: df4
130/20: Fn = pd.read_csv('Fn_kpu.csv')
130/21: Fn1 = Fn.drop('Unnamed: 0', axis=1)
130/22: Fn1
130/23: Fn1 = Fn(['kp', 'kpu', 'Cells'])
130/24: Fn1 = Fn['kp', 'kpu', 'Cells']
130/25: Fn1 = pd.Fn['kp', 'kpu', 'Cells']
130/26: Fn1 = Fn['kp', 'kpu', 'Cells']
130/27: Fn1 = Fn(['kp', 'kpu', 'Cells'])
130/28: Fn1 = Fn['kp', 'kpu', 'Cells'])
130/29: Fn1 = Fn['kp', 'kpu', 'Cells']
130/30: Fn1 = Fn[Fn.'kp', 'kpu', 'Cells']
130/31: Fn1 = Fn[Fn[]'kp', 'kpu', 'Cells']
130/32: Fn1
130/33:
Fn = pd.read_csv('Fn_kpu.csv')
Fn
130/34: Fn1 = Fn['kp', 'kpu', 'Cells']
130/35: Fn1 = Fn["kp", "kpu", "Cells"]
130/36: Fn1
130/37: df1 = Fn['kp']
130/38: df1 = Fn['kp', 'kpu']
130/39: df1 = Fn(['kp', 'kpu'], axis=1)
130/40: df1 = Fn['kp', 'kpu']
130/41: Fn1 = Fn[['kp', 'kpu', 'Cells']]
130/42: Fn3 = pd.concatZ['Fn2', 'Fn1'], axis=1)
130/43: Fn3 = pd.concat(['Fn2', 'Fn1'], axis=1)
130/44: Fn3 = pd.concat(['Fn2', 'Fn1'], axis=1)
129/102: data = pd.concat(['X', 'y'], axis =1)
129/103: data = pd.concat([X, y], axis =1)
129/104:
data = pd.concat([X, y], axis =1)
data.to_csv('datasetforbiuildmodel.csv')
130/45:
data = pd.read_csv('datasetforbiuildmodel.csv')
data
129/105:
X = df4.drop(['name', 'kp', 'kpu','Cells'], axis=1)
y = df4['kp']
y_all = df4[['kp', 'kpu', 'Cells']]
129/106:
data = pd.concat([X, y_all], axis =1)
data.to_csv('datasetforbiuildmodel.csv')
130/46:
data = pd.read_csv('datasetforbiuildmodel.csv')
data
130/47: Fn2 = pd.read_csv('Fingerprint')
130/48: Fn2 = pd.read_csv('Fingerprint.csv')
130/49: Fn2
129/107:
X = df4.drop(['name', 'kp', 'kpu','Cells'], axis=1)
y = df4['kp']
y_all = df4[['kp', 'kpu', 'Cells']]
name = df4['name']
129/108:
data = pd.concat([name, X, y_all], axis =1, set_index())
data.to_csv('datasetforbiuildmodel.csv')
129/109:
data = pd.concat([name, X, y_all], axis =1, set_index(name))
data.to_csv('datasetforbiuildmodel.csv')
129/110:
data = pd.concat([name, X, y_all], axis =1)
data.to_csv('datasetforbiuildmodel.csv')
129/111:
data = pd.concat([name, X, y_all], axis =1)
data.set_index(name)
data.to_csv('datasetforbiuildmodel.csv')
130/50:
data = pd.read_csv('datasetforbiuildmodel.csv')
data
129/112:
data = pd.concat([name, X, y_all], axis =1)
data2 = data.set_index(name)
data2.to_csv('datasetforbiuildmodel.csv')
130/51:
data = pd.read_csv('datasetforbiuildmodel.csv')
data
129/113:
data = pd.concat([name, X, y_all], axis =1)
data2 = data.set_index(name)
data2.drop('name.1')
data2.to_csv('datasetforbiuildmodel.csv')
129/114:
data = pd.concat([name, X, y_all], axis =1)
data2 = data.set_index(name)
data2.drop(['name.1'], axis=1)
data2.to_csv('datasetforbiuildmodel.csv')
129/115:
data = pd.concat([name, X, y_all], axis =1)
data2 = data.set_index(name)
data3 = data2.drop(['name.1 '], axis=1)
data2.to_csv('datasetforbiuildmodel.csv')
129/116:
data = pd.concat([name, X, y_all], axis =1)
data2 = data.set_index(name)
data3 = data2.drop(['name.1 '], axis=1)
data3.to_csv('datasetforbiuildmodel.csv')
129/117:
data = pd.concat([name, X, y_all], axis =1)
data2 = data.set_index(name)
data3 = data2.drop(['name.1'], axis=1)
data3.to_csv('datasetforbiuildmodel.csv')
129/118:
data = pd.concat([name, X, y_all], axis =1)
data2 = data.set_index(name)
data3 = data2.drop([name.1], axis=1)
data3.to_csv('datasetforbiuildmodel.csv')
129/119:
data = pd.concat([name, X, y_all], axis =1)
data2 = data.set_index(name)
data3 = data2.drop(['name.1'], axis=1)
data3.to_csv('datasetforbiuildmodel.csv')
129/120:
data = pd.concat([name, X, y_all], axis =1)
data2 = data.set_index(name, drop=True)
data3.to_csv('datasetforbiuildmodel.csv')
129/121:
data = pd.concat([name, X, y_all], axis =1)
data2 = data.set_index(name, drop=True)
data2.to_csv('datasetforbiuildmodel.csv')
130/52:
data = pd.read_csv('datasetforbiuildmodel.csv')
data
130/53: data2 = data.drop(['name.1'])
130/54: data2 = data.drop('name.1')
130/55: data2 = data.drop('name.1', axis=1)
130/56:
data2 = data.drop('name.1', axis=1)
data2
130/57: data3 = data2.set_index('name')
130/58: data3
130/59: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)
130/60: X = data3.drop(['kp', 'kpu', 'Cells'], axis=1)
130/61: y = data3['kpu']
130/62: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=5)
130/63: X_train.shape, X_test.shape
130/64: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=5)
130/65: X_train.shape, X_test.shape
130/66: y_train.shape, y_test.shape
130/67:
clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)
clf.score(X_test, y_test)
129/122: model.score
129/123:
r2_test = model.score(X_test,y_test)
r2_test
129/124:
from sklearn.metrics import mean_squared_error
rms = sqrt(mean_squared_error(y_train, y1_predict))
129/125:
from sklearn.metrics import mean_squared_error
rms1 = mean_squared_error(y_train, y1_predict)
129/126:
from sklearn.metrics import mean_squared_error
rms1 = mean_squared_error(y_train, y1_pred)
129/127:
from sklearn.metrics import mean_squared_error
rms1 = mean_squared_error(y_train, y1_pred)
rms2 = mean_squared_error(y_test, y2_pred)
129/128:
from sklearn.metrics import mean_squared_error
rms1 = mean_squared_error(y_train, y1_pred)
rms2 = mean_squared_error(y_test, y2_pred)
print(rms1, rms2)
129/129:
from sklearn.metrics import mean_squared_error
rms1 = mean_squared_error(y_train, y1_pred, squared=False)
rms2 = mean_squared_error(y_test, y2_pred, squared=False)
print(rms1, rms2)
130/68: from sklearn.linear_model import LinearRegression
129/130: X = Fp_normalized.drop(['name', 'kp', 'kpu','Cells'], axis=1)
129/131: X = Fp_normalized
129/132:
X = Fp_normalized
y = df4['kp']
129/133: X.shape
129/134: X.shape y.shape
129/135: X.shape, y.shape
129/136: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)
129/137:
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(n_estimators=200)
model.fit(X_train,y_train)
r2 = model.score(X_train,y_train)
r2
129/138:
y1_pred = model.predict(X_train)
y2_pred = model.predict(X_test)
129/139:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y_train, y1_pred, alpha=0.7, c='coral', label='Traing set')
plt.scatter(y_test, y2_pred, alpha=0.7, c='blue', label='Test set')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.legend()
#save file
plt.tight_layout()
plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)
129/140:
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score
kf =KFold(n_splits=5, shuffle=False)
129/141:
cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1
129/142:
cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {(train_index)}, Test set:{(test_index)}')
    cnt += 1
129/143:
cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {(train_index)}, Test set:{(test_index)}')
    cnt += 1
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1
129/144:
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score
kf =KFold(n_splits=10, shuffle=False)
129/145:
cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {(train_index)}, Test set:{(test_index)}')
    cnt += 1
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1
129/146:
from sklearn.model_selection import cross_val_score
scores= cross_val_score(model, X, y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
130/69:
data = pd.read_csv('data2_lipinski_kpu.csv')
data
130/70:
data2 = data.drop('Unnamed: 0', axis=1)
data2
129/147: scores
130/71: Fp = pd.read_csv('Fp_normalized.csv')
130/72:
Fp = pd.read_csv('Fp_normalized.csv')
Fp
130/73: X = Fp.drop['Unnamed: 0']
130/74: X = Fp.drop()'Unnamed: 0', axis=1)
130/75: X = Fp.drop('Unnamed: 0', axis=1)
130/76:
X = Fp.drop('Unnamed: 0', axis=1)
X.shape
130/77: y = data3['kpu']
130/78:
y = data3['kpu']
y.shape
130/79:
y = data3['kpu']
y.shape
130/80:
from sklearn.linear_model import LinearRegression
lr = linear_model.LinearRegression()
130/81:
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt
%matplotlib inline
130/82:

# 10-fold cross-validation with K=5 for KNN (the n_neighbors parameter)
knn = KNeighborsClassifier(n_neighbors=5)
scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')
print(scores)
130/83:
from sklearn.linear_model import LinearRegression
lr = LinearRegression()
scores = cross_val_score(lr, X, y, cv=10, scoring='accuracy')
print(scores)
130/84:
from sklearn.linear_model import LinearRegression
lr = LinearRegression()
scores = cross_val_score(lr, X, y, cv=10)
print(scores)
130/85:
lr = LinearRegression()
scores = cross_val_score(lr, X, y, cv=10)
print(scores)
130/86: from sklearn.model_selection import cross_val_score
130/87:
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score
kf =KFold(n_splits=10, shuffle=False)
130/88:
cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1
130/89:
from sklearn.linear_model import LinearRegression
lr = LinearRegression()
scores = cross_val_score(lr, X, y, cv=kF)
print(scores)
130/90:
from sklearn.linear_model import LinearRegression
lr = LinearRegression()
scores = cross_val_score(lr, X, y, cv=kf)
print(scores)
130/91:
lr = LinearRegression()
scores = cross_val_score(lr, X, y, cv=kf)
print(scores)
130/92:
from sklearn.linear_model import LinearRegression
lr = LinearRegression()
scores = cross_val_score(lr, X, y, cv=kf)
print(scores)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
130/93:
from sklearn.decomposition import PCA
pca = PCA(n_components=10)
130/94: scores = cross_val_score(pca, X, y, cv=kf)
130/95: scores
130/96:
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
130/97: scores = cross_val_score(pca, X, y, cv=kf)
130/98: scores
130/99:
scores = cross_val_score(pca, X, y, cv=kf)
print(scores)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
130/100:
from sklearn.cross_decomposition import PLSRegression
pls = PLSRegression(n_components=1)
130/101:
scores = cross_val_score(pls, X, y, cv=kf)
print(scores)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
130/102:
from sklearn.cross_decomposition import PLSRegression
pls = PLSRegression(n_components=2)
130/103:
scores = cross_val_score(pls, X, y, cv=kf)
print(scores)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
130/104:
print(f"PCR r-squared {pcr.score(X_test, y_test):.3f}")
print(f"PLS r-squared {pls.score(X_test, y_test):.3f}")
130/105:
print(f"PCA r-squared {pca.score(X_test, y_test):.3f}")
print(f"PLS r-squared {pls.score(X_test, y_test):.3f}")
130/106:
from sklearn.cross_decomposition import PLSRegression
pls2 = PLSRegression(n_components=2)
pls2.fit(X, y)
PLSRegression()
130/107:
from sklearn.cross_decomposition import PLSRegression
pls2 = PLSRegression(n_components=2)
pls2.fit(X, y)
r2 = pls2.score(X, y)
130/108:
from sklearn.cross_decomposition import PLSRegression
pls2 = PLSRegression(n_components=2)
pls2.fit(X, y)
r2 = pls2.score(X, y)
r2
130/109:
from sklearn.cross_decomposition import PLSRegression
pls2 = PLSRegression(n_components=2)
pls2.fit(X_train, y_train)
r2 = pls2.score(X_train, y_train)
r2
130/110:
from sklearn.cross_decomposition import PLSRegression
pls2 = PLSRegression(n_components=2)
scores = cross_val_score(pls2, X, y, cv=kf)
130/111:
from sklearn.cross_decomposition import PLSRegression
pls2 = PLSRegression(n_components=2)
scores = cross_val_score(pls2, X, y, cv=kf)
scores
129/148:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y_train, y1_pred, alpha=0.7, c='coral', label='Traing set')
plt.scatter(y_test, y2_pred, alpha=0.7, c='blue', label='Test set')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.legend()
#save file
plt.tight_layout()
plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)
129/149:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y_train, y1_pred, alpha=0.7, c='coral', label='Traing set')
plt.scatter(y_test, y2_pred, alpha=0.7, c='blue', label='Test set')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.legend()
#save file
plt.tight_layout()
plt.savefig('Regression_model_noCV_nocut.pdf', dpi=300)
130/112:
from scipy import stats
import numpy as np
z = np.abs(stats.zscore(Y))
print(z)
130/113:
from scipy import stats
import numpy as np
z = np.abs(stats.zscore(y))
print(z)
130/114:
threshold = 3
print(np.where(z > 3))
130/115:
data2 = data.drop('Unnamed: 0', axis=1)
selection = data2['kp','kpu']
data2
130/116:
data2 = data.drop('Unnamed: 0', axis=1)
selection = data['kp','kpu']
data2
130/117:
data2 = data.drop('Unnamed: 0', axis=1)
selection = data[['kp','kpu']]
data2
130/118: Xy = pd.concat([X,y], axis =1)
130/119: Xy = pd.concat([X, y], axis =1)
130/120: Xy = pd.concat([X, selection], axis =1)
130/121:
from scipy import stats
import numpy as np
z = np.abs(stats.zscore(Xy))
print(z)
130/122:
threshold = 3
print(np.where(z > 3))
130/123:
from scipy import stats
import numpy as np
z = np.abs(stats.zscore(y))
print(z)
130/124:
threshold = 3
print(np.where(z > 3))
130/125: Xy1 = Xy.drop(['72','84','85','86','87'])
130/126: Xy1 = Xy.drop(['72','84','85','86','87'],axis=0)
130/127: Xy1 = Xy.drop(['72','84','85','86','87'], axis=0)
130/128: Xy1 = Xy.drop(['72','84','85','86','87'], index=labels)
130/129: Xy1 = Xy.drop([72,84,85,86,87])
130/130: Xy1
130/131: X = Xy1.drop(['kp', 'kpu'], axis=1)
130/132: X
130/133: y = Xy1[['kp','kpu']]
130/134:
from sklearn.linear_model import LinearRegression
lr = LinearRegression()
scores = cross_val_score(lr, X, y, cv=kf)
print(scores)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
130/135:
from sklearn.cross_decomposition import PLSRegression
pls2 = PLSRegression(n_components=2)
scores = cross_val_score(pls2, X, y, cv=kf)
scores
130/136: print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
130/137:
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(n_estimators=200)
scores = cross_val_score(model, X, y, cv=kf)
scores
130/138: print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
130/139: y = Xy1[['kp']]
130/140: y = Xy1[['kp']]
130/141:
from sklearn.linear_model import LinearRegression
lr = LinearRegression()
scores = cross_val_score(lr, X, y, cv=kf)
print(scores)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
130/142:
from sklearn.cross_decomposition import PLSRegression
pls2 = PLSRegression(n_components=2)
scores = cross_val_score(pls2, X, y, cv=kf)
scores
130/143: print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
130/144:
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(n_estimators=200)
scores = cross_val_score(model, X, y, cv=kf)
scores
130/145: print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
130/146: y = Xy1[['kpu']]
130/147:
from sklearn.linear_model import LinearRegression
lr = LinearRegression()
scores = cross_val_score(lr, X, y, cv=kf)
print(scores)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
130/148:
from sklearn.cross_decomposition import PLSRegression
pls2 = PLSRegression(n_components=2)
scores = cross_val_score(pls2, X, y, cv=kf)
scores
130/149: print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
130/150:
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(n_estimators=200)
scores = cross_val_score(model, X, y, cv=kf)
scores
130/151: print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
130/152:
from sklearn.cross_decomposition import PLSRegression
pls2 = PLSRegression(n_components=7)
scores = cross_val_score(pls2, X, y, cv=kf)
scores
130/153:
from sklearn.cross_decomposition import PLSRegression
pls2 = PLSRegression(n_components=10)
scores = cross_val_score(pls2, X, y, cv=kf)
scores
130/154:
from sklearn.cross_decomposition import PLSRegression
pls2 = PLSRegression(n_components=10)
scores = cross_val_score(pls2, X, y, cv=kf)
scores
130/155:
from sklearn.model_selection import cross_val_predict
y_pred = cross_val_predict(pls2, X, y, cv=10)
130/156:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.legend()
#save file
plt.tight_layout()
plt.savefig('PLS_model_CV_cut.pdf', dpi=300)
130/157:
from sklearn.model_selection import cross_val_predict
y_pred = cross_val_predict(pls2, X, y, cv=10)
Y_pred
130/158:
from sklearn.model_selection import cross_val_predict
y_pred = cross_val_predict(pls2, X, y, cv=10)
y_pred
130/159:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.legend()
#save file
plt.tight_layout()
plt.savefig('PLS_model_CV_cut.pdf', dpi=300)
130/160:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.tick_params(axis='both', which='major', labelsize=14)
plt.legend()
#save file
plt.tight_layout()
plt.savefig('PLS_model_CV_cut.pdf', dpi=300)
130/161:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.tick_params(axis='both', which='major', labelsize=14)
#save file
plt.tight_layout()
plt.savefig('PLS_model_CV_cut.pdf', dpi=300)
130/162:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.ylimit(0.000001. 10)
plt.tick_params(axis='both', which='major', labelsize=14)
#save file
plt.tight_layout()
plt.savefig('PLS_model_CV_cut.pdf', dpi=300)
130/163:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.ylimit(0.000001, 10)
plt.tick_params(axis='both', which='major', labelsize=14)
#save file
plt.tight_layout()
plt.savefig('PLS_model_CV_cut.pdf', dpi=300)
130/164:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.ylim(0.000001, 10)
plt.tick_params(axis='both', which='major', labelsize=14)
#save file
plt.tight_layout()
plt.savefig('PLS_model_CV_cut.pdf', dpi=300)
130/165:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.ylim(0.0001, 10)
plt.tick_params(axis='both', which='major', labelsize=14)
#save file
plt.tight_layout()
plt.savefig('PLS_model_CV_cut.pdf', dpi=300)
130/166:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.ylim(0.01, 10)
plt.tick_params(axis='both', which='major', labelsize=14)
#save file
plt.tight_layout()
plt.savefig('PLS_model_CV_cut.pdf', dpi=300)
130/167:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.ylim(0.001, 10)
plt.tick_params(axis='both', which='major', labelsize=14)
#save file
plt.tight_layout()
plt.savefig('PLS_model_CV_cut.pdf', dpi=300)
130/168:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.ylim(0.01, 10)
plt.tick_params(axis='both', which='major', labelsize=14)
#save file
plt.tight_layout()
plt.savefig('PLS_model_CV_cut.pdf', dpi=300)
130/169:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.ylim(0.02, 10)
plt.tick_params(axis='both', which='major', labelsize=14)
#save file
plt.tight_layout()
plt.savefig('PLS_model_CV_cut.pdf', dpi=300)
130/170:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.ylim(0.0015, 10)
plt.tick_params(axis='both', which='major', labelsize=14)
#save file
plt.tight_layout()
plt.savefig('PLS_model_CV_cut.pdf', dpi=300)
130/171:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.ylim(0.0011, 10)
plt.tick_params(axis='both', which='major', labelsize=14)
#save file
plt.tight_layout()
plt.savefig('PLS_model_CV_cut.pdf', dpi=300)
130/172:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.ylim(0.0011, 10)
plt.xlim(0.0011, 10)
plt.tick_params(axis='both', which='major', labelsize=14)
#save file
plt.tight_layout()
plt.savefig('PLS_model_CV_cut.pdf', dpi=300)
130/173:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.ylim(0.0011, 100)
plt.xlim(0.0011, 100)
plt.tick_params(axis='both', which='major', labelsize=14)
#save file
plt.tight_layout()
plt.savefig('PLS_model_CV_cut.pdf', dpi=300)
130/174:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.ylim(0.0011, 50)
plt.xlim(0.0011, 50)
plt.tick_params(axis='both', which='major', labelsize=14)
#save file
plt.tight_layout()
plt.savefig('PLS_model_CV_cut.pdf', dpi=300)
134/1:
# Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>
#          Maria Telenczuk    <https://github.com/maikia>
# License: BSD 3 clause

print(__doc__)

from sklearn import set_config
set_config(display='diagram')
134/2:
import numpy as np

from sklearn.datasets import fetch_openml
from sklearn.utils import shuffle


def load_ames_housing():
    df = fetch_openml(name="house_prices", as_frame=True)
    X = df.data
    y = df.target

    features = ['YrSold', 'HeatingQC', 'Street', 'YearRemodAdd', 'Heating',
                'MasVnrType', 'BsmtUnfSF', 'Foundation', 'MasVnrArea',
                'MSSubClass', 'ExterQual', 'Condition2', 'GarageCars',
                'GarageType', 'OverallQual', 'TotalBsmtSF', 'BsmtFinSF1',
                'HouseStyle', 'MiscFeature', 'MoSold']

    X = X[features]
    X, y = shuffle(X, y, random_state=0)

    X = X[:600]
    y = y[:600]
    return X, np.log(y)


X, y = load_ames_housing()
134/3:
from sklearn.compose import make_column_selector

cat_selector = make_column_selector(dtype_include=object)
num_selector = make_column_selector(dtype_include=np.number)
cat_selector(X)
134/4: num_selector(X)
134/5:
from sklearn.compose import make_column_transformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import OrdinalEncoder

cat_tree_processor = OrdinalEncoder(
    handle_unknown="use_encoded_value", unknown_value=-1)
num_tree_processor = SimpleImputer(strategy="mean", add_indicator=True)

tree_preprocessor = make_column_transformer(
    (num_tree_processor, num_selector), (cat_tree_processor, cat_selector))
tree_preprocessor
134/6:
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler

cat_linear_processor = OneHotEncoder(handle_unknown="ignore")
num_linear_processor = make_pipeline(
    StandardScaler(), SimpleImputer(strategy="mean", add_indicator=True))

linear_preprocessor = make_column_transformer(
    (num_linear_processor, num_selector), (cat_linear_processor, cat_selector))
linear_preprocessor
130/175:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')
plot_regression_results(
        ax, y, y_pred,
        name,
        (r'$R^2={:.2f} \pm {:.2f}$' + '\n' + r'$MAE={:.2f} \pm {:.2f}$')
        .format(np.mean(score['test_r2']),
                np.std(score['test_r2']),
                -np.mean(score['test_neg_mean_absolute_error']),
                np.std(score['test_neg_mean_absolute_error'])),
        elapsed_time)
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.ylim(0.0011, 50)
plt.xlim(0.0011, 50)
plt.tick_params(axis='both', which='major', labelsize=14)
#save file
plt.tight_layout()
plt.savefig('PLS_model_CV_cut.pdf', dpi=300)
130/176:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.ylim(0.0011, 50)
plt.xlim(0.0011, 50)
plt.tick_params(axis='both', which='major', labelsize=14)
#save file
plt.tight_layout()
plt.savefig('PLS_model_CV_cut.pdf', dpi=300)
130/177:
import time
import matplotlib.pyplot as plt
from sklearn.model_selection import cross_validate, cross_val_predict


def plot_regression_results(ax, y_true, y_pred, title, scores, elapsed_time):
    """Scatter plot of the predicted vs true targets."""
    ax.plot([y_true.min(), y_true.max()],
            [y_true.min(), y_true.max()],
            '--r', linewidth=2)
    ax.scatter(y_true, y_pred, alpha=0.2)

    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.get_xaxis().tick_bottom()
    ax.get_yaxis().tick_left()
    ax.spines['left'].set_position(('outward', 10))
    ax.spines['bottom'].set_position(('outward', 10))
    ax.set_xlim([y_true.min(), y_true.max()])
    ax.set_ylim([y_true.min(), y_true.max()])
    ax.set_xlabel('Measured')
    ax.set_ylabel('Predicted')
    extra = plt.Rectangle((0, 0), 0, 0, fc="w", fill=False,
                          edgecolor='none', linewidth=0)
    ax.legend([extra], [scores], loc='upper left')
    title = title + '\n Evaluation in {:.2f} seconds'.format(elapsed_time)
    ax.set_title(title)


fig, axs = plt.subplots(2, 2, figsize=(9, 7))
axs = np.ravel(axs)

for ax, (name, est) in zip(axs, estimators + [('Stacking Regressor',
                                               stacking_regressor)]):
    start_time = time.time()
    score = cross_validate(est, X, y,
                           scoring=['r2', 'neg_mean_absolute_error'],
                           n_jobs=-1, verbose=0)
    elapsed_time = time.time() - start_time

    y_pred = cross_val_predict(est, X, y, n_jobs=-1, verbose=0)

    plot_regression_results(
        ax, y, y_pred,
        name,
        (r'$R^2={:.2f} \pm {:.2f}$' + '\n' + r'$MAE={:.2f} \pm {:.2f}$')
        .format(np.mean(score['test_r2']),
                np.std(score['test_r2']),
                -np.mean(score['test_neg_mean_absolute_error']),
                np.std(score['test_neg_mean_absolute_error'])),
        elapsed_time)

plt.suptitle('Single predictors versus stacked predictors')
plt.tight_layout()
plt.subplots_adjust(top=0.9)
plt.show()
130/178:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')

plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.ylim(0.0011, 50)
plt.xlim(0.0011, 50)
plt.tick_params(axis='both', which='major', labelsize=14)
#save file
plt.tight_layout()
plt.savefig('PLS_model_CV_cut.pdf', dpi=300)
130/179:
from sklearn.model_selection import cross_val_predict
y_pred = cross_val_predict(pls2, X, y, cv=10)
130/180: np.mean(score['test_r2']
130/181: np.mean(score['test_r2'])
130/182:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.ylim(0.0011, 50)
plt.xlim(0.0011, 50)
plt.tick_params(axis='both', which='major', labelsize=14)
plt.annotate('R2: ' + str(r2_score(y, y_pred)))
#save file
plt.tight_layout()
plt.savefig('PLS_model_CV_cut.pdf', dpi=300)
130/183:
from sklearn.metrics import r2_score
r2_score(y, y_pred)
130/184:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.ylim(0.0011, 50)
plt.xlim(0.0011, 50)
plt.tick_params(axis='both', which='major', labelsize=14)
plt.annotate('R2: ' + str(r2_score(y, y_pred)))
#save file
plt.tight_layout()
plt.savefig('PLS_model_CV_cut.pdf', dpi=300)
130/185:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.ylim(0.0011, 50)
plt.xlim(0.0011, 50)
plt.tick_params(axis='both', which='major', labelsize=14)
plt.annotate('R2: ' + str(r2_score(y, y_pred)), y, y_pred)
#save file
plt.tight_layout()
plt.savefig('PLS_model_CV_cut.pdf', dpi=300)
130/186:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.ylim(0.0011, 50)
plt.xlim(0.0011, 50)
plt.tick_params(axis='both', which='major', labelsize=14)
plt.annotate('R2: ' + str(r2_score))
#save file
plt.tight_layout()
plt.savefig('PLS_model_CV_cut.pdf', dpi=300)
130/187:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.ylim(0.0011, 50)
plt.xlim(0.0011, 50)
plt.tick_params(axis='both', which='major', labelsize=14)
plt.annotate('R2: ', str(r2_score))
#save file
plt.tight_layout()
plt.savefig('PLS_model_CV_cut.pdf', dpi=300)
130/188:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.ylim(0.0011, 50)
plt.xlim(0.0011, 50)
plt.tick_params(axis='both', which='major', labelsize=14)
plt.annotate(r2_score)
#save file
plt.tight_layout()
plt.savefig('PLS_model_CV_cut.pdf', dpi=300)
130/189:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.ylim(0.0011, 50)
plt.xlim(0.0011, 50)
plt.tick_params(axis='both', which='major', labelsize=14)
plt.annotate(r2_scoree(y, y_pred))
#save file
plt.tight_layout()
plt.savefig('PLS_model_CV_cut.pdf', dpi=300)
130/190:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.ylim(0.0011, 50)
plt.xlim(0.0011, 50)
plt.tick_params(axis='both', which='major', labelsize=14)
plt.annotate(r2_score(y, y_pred))
#save file
plt.tight_layout()
plt.savefig('PLS_model_CV_cut.pdf', dpi=300)
130/191:
from sklearn.model_selection import cross_val_predict
y_pred = cross_val_predict(model, X, y, cv=kf)
130/192:
#no CV
import matplotlib.pyplot as plt
plt.scatter(y, y_pred, alpha=0.7, c='coral', label='CV')
plt.xlabel('Experimental C/M', fontsize='16', fontweight='bold')
plt.ylabel('Predicted C/M', fontsize='16', fontweight='bold')
plt.title('')
plt.xscale('log')
plt.yscale('log')
plt.ylim(0.0011, 50)
plt.xlim(0.0011, 50)
plt.tick_params(axis='both', which='major', labelsize=14)
#save file
plt.tight_layout()
plt.savefig('PLS_model_CV_cut.pdf', dpi=300)
130/193: r2_score(y, y_pred)
129/150: r2_score(y_train, y1_pred)
129/151: r2_score(y_test, y2_pred)
129/152:
from sklearn.metrics import mean_squared_error
rms1 = mean_squared_error(y_train, y1_pred, squared=False)
rms2 = mean_squared_error(y_test, y2_pred, squared=False)
print(rms1, rms2)
130/194:
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(n_estimators=200)
scores = cross_val_score(model, X, y, cv=kf)
scores
124/1:
df = pd.read_csv('Test_compounds.csv')
df
124/2:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
124/3: pwd
124/4:
df = pd.read_csv('Test_compounds.csv')
df
124/5: df.to_csv('test_molecule.smi', sep='\t', index=False, header=False)
124/6: df.to_csv('test_molecules.smi', sep='\t', index=False, header=False)
124/7:
#calculate molecular descriptor
!python -m mordred -t smi \test_molecules.smi -o test_molecules.csv
124/8: df_test = pd.read_csv('test_molecules.csv')
124/9: df_test
135/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
135/2:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
135/3: df.read_csv('test_molecules.csv')
135/4: df = pd.read_csv('test_molecules.csv')
135/5: df.shape
135/6: df2 = pd.read_csv('molecules.csv')
135/7: df2.sha
135/8: df2.shape
135/9: df3 = pd.concat(df, df2)
135/10: df3 = pd.concat[df, df2]
135/11: df3 = pd.concat[[df, df2]]
135/12: df3 = pd.concat([df, df2])
135/13: df3
135/14: df4 =df3.reset_index()
135/15: df4
135/16: df4 =df3.reset_index(drop=Ture)
135/17: df4 =df3.reset_index(drop=True)
135/18: df4
135/19:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
136/1:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
136/2: pwd
136/3: df = pd.read_csv('rawdataqsar2.csv')
136/4: df = pd.read_csv('rawdataqsar_sm.csv')
136/5: df
136/6: df2 =df[df.logpk.notna()]
126/1: df2 =df[df.kpu.notna()]
136/7: df
136/8: df2 =df[df.logpk.notna()]
136/9: df2 =df[df.logkp.notna()]
136/10: df2
136/11: df2 =df[df.kp.notna()]
136/12: df2
136/13: df3 = df.drop[logkp]
136/14: df3 = df.drop['logkp']
136/15: df3 = df.drop(['logkp', 'kpu'])
136/16:
df3 = df.drop(['logkp', 'kpu'], axis=1
             )
136/17: df3
136/18:
df3 = df2.drop(['logkp', 'kpu'], axis=1
             )
136/19: df3
136/20:
def LogKp(input):
    LogKp = []

    for i in input['kp']:
        LogKp.append(np.log10(i))

    input['LogKp'] = LogKp
        
    return x
136/21: df4 = LogKp(df3)
136/22:
def LogKp(input):
    LogKp = []

    for i in input['kp']:
        LogKp.append(np.log10(i))

    input['LogKp'] = LogKp
        
    return x
136/23: df4 = LogKp(df3)
136/24:
def LogKp(input):
    LogKp = []

    for i in input['kp']:
        LogKp.append(np.log10(i))

    input['LogKp'] = LogKp
        
    return x
136/25: df4 = LogKp(df3)
136/26:
def LogKp(input):
    LogKp = []

    for i in input['kp']:
        LogKp.append(np.log10(i))

    input['LogKp'] = LogKp
        
    return LogKp
136/27:
def LogKp(input):
    LogKp = []

    for i in input['kp']:
        LogKp.append(np.log10(i))

    input['LogKp'] = LogKp
        
    return LogKp
136/28: df4 = LogKp(df3)
136/29: df4
136/30:
def LogKp(input):
    LogKp = []

    for i in input['kp']:
        LogKp.append(np.log10(i))

    input['LogKp'] = LogKp
    x = input.drop('standard_value_norm', 1)   
    return LogKp
136/31:
def LogKp(input):
    LogKp = []

    for i in input['kp']:
        LogKp.append(np.log10(i))

    input['LogKp'] = LogKp
    x = input.drop('standard_value_norm', 1)   
    return x
136/32: df4 = LogKp(df3)
136/33:
def LogKp(input):
    LogKp = []

    for i in input['kp']:
        LogKp.append(np.log10(i))

    input['LogKp'] = LogKp
    x = input.drop('LogKp', 1)   
    return x
136/34: df4 = LogKp(df3)
136/35: df4
136/36:
def LogKp(input):
    LogKp = []

    for i in input['kp']:
        LogKp.append(np.log10(i))

    input['LogKp'] = LogKp
    x = input.append('LogKp', 1)   
    return x
136/37: df4 = LogKp(df3)
136/38: df4
136/39:
def LogKp(input):
    LogKp = []

    for i in input['kp']:
        LogKp.append(np.log10(i))

    input['LogKp'] = LogKp
    x = input.drop('LogKp', 1)   
    return x
136/40: df4 = LogKp(df3)
136/41:
def LogKp(input):
    LogKp = []

    for i in input['kp']:
        LogKp.append(np.log10(i))

    input['LogKp'] = LogKp
    x = input.drop('kp', 1)   
    return x
136/42: df4 = LogKp(df3)
136/43: df4
136/44: selection = df4['smile', 'compound']
136/45: selection = df4[['smile', 'compound']]
136/46: selection = df4(['smile', 'compound'])
136/47: selection = df4[['smile', 'compound']]
136/48: selection = ['smile', 'compound']
136/49: df5 = df4[selection]
136/50: selection = ['smiles', 'compound']
136/51: df5 = df4[selection]
136/52: df5
136/53:
df5 = df4[selection]
df_selection.to_csv('allmolecules.smi', sep='\t', index=False, header=False)
136/54:
df5 = df4[selection]
df5.to_csv('allmolecules.smi', sep='\t', index=False, header=False)
136/55:
#calculate molecular descriptor
!python -m mordred -t smi \allmolecules.smi -o allmolecules.csv
136/56:
#calculate molecular fingerprint
from rdkit import Chem
from mordred import Calculator, descriptors
calc = Calculator(descriptors, ignore_3D=True)
136/57: len(calc)
136/58:

mols = [Chem.MolFromSmiles(smi) 
    for smi in input['smiles']
df = calc.pandas(mols)
136/59:
def lipinski(smiles, verbose=False):

    moldata= []
    for elem in smiles:
        mol=Chem.MolFromSmiles(elem) 
        moldata.append(mol)
       
    baseData= np.arange(1,1)
    i=0  
    for mol in moldata:        
       
        desc_MolWt = Descriptors.MolWt(mol)
        desc_TPSA = Descriptors.TPSA(mol)
        desc_MolLogP = Descriptors.MolLogP(mol)
        desc_NumHDonors = Lipinski.NumHDonors(mol)
        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)
        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)
           
        row = np.array([desc_MolWt,
                        desc_TPSA,
                        desc_MolLogP,
                        desc_NumHDonors,
                        desc_NumHAcceptors,
                        desc_NumRotatableBonds])   
    
        if(i==0):
            baseData=row
        else:
            baseData=np.vstack([baseData, row])
        i=i+1      
    
    columnNames=["MW","TPSA", "LogP","NumHDonors","NumHAcceptors", "NumRotatableBonds"]   
    descriptors = pd.DataFrame(data=baseData,columns=columnNames)
    
    return descriptors
136/60:
def lipinski(smiles, verbose=False):

    moldata= []
    for elem in smiles:
        mol=Chem.MolFromSmiles(elem) 
        moldata.append(mol)
       
    baseData= np.arange(1,1)
    i=0  
    for mol in moldata:        
       
        desc_MolWt = Descriptors.MolWt(mol)
        desc_MolLogP = Descriptors.MolLogP(mol)
        desc_NumHDonors = Lipinski.NumHDonors(mol)
        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)
        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)
           
        row = np.array([desc_MolWt,
                        desc_TPSA,
                        desc_MolLogP,
                        desc_NumHDonors,
                        desc_NumHAcceptors,
                        desc_NumRotatableBonds])   
    
        if(i==0):
            baseData=row
        else:
            baseData=np.vstack([baseData, row])
        i=i+1      
    
    columnNames=["MW", "LogP","NumHDonors","NumHAcceptors", "NumRotatableBonds"]   
    descriptors = pd.DataFrame(data=baseData,columns=columnNames)
    
    return descriptors
136/61:
def lipinski(smiles, verbose=False):

    moldata= []
    for elem in smiles:
        mol=Chem.MolFromSmiles(elem) 
        moldata.append(mol)
       
    baseData= np.arange(1,1)
    i=0  
    for mol in moldata:        
       
        desc_MolWt = Descriptors.MolWt(mol)
        desc_TPSA = Descriptors.TPSA(mol)
        desc_MolLogP = Descriptors.MolLogP(mol)
        desc_NumHDonors = Lipinski.NumHDonors(mol)
        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)
        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)
           
        row = np.array([desc_MolWt,
                        desc_TPSA,
                        desc_MolLogP,
                        desc_NumHDonors,
                        desc_NumHAcceptors,
                        desc_NumRotatableBonds])   
    
        if(i==0):
            baseData=row
        else:
            baseData=np.vstack([baseData, row])
        i=i+1      
    
    columnNames=["MW","TPSA", "LogP","NumHDonors","NumHAcceptors", "NumRotatableBonds"]   
    descriptors = pd.DataFrame(data=baseData,columns=columnNames)
    
    return descriptors
136/62: df5 = pd.read_csv('allmolecules.smi')
136/63:
df5 = pd.read_csv('allmolecules.smi')
df5
136/64:
df5 = pd.read_csv('allmolecules.csv')
df5
136/65:
from rdkit import Chem, DataStructs
from rdkit.Chem import PandasTools, AllChem
PandasTools.AddMoleculeColumnToFrame(data,'SMILES','Molecule')
data[["SMILES","Molecule"]].head(1)
136/66:
from rdkit import Chem, DataStructs
from rdkit.Chem import PandasTools, AllChem
PandasTools.AddMoleculeColumnToFrame(df4,'smiles','Molecule')
data[["smiles","Molecule"]].head(1)
136/67:
from rdkit import Chem, DataStructs
from rdkit.Chem import PandasTools, AllChem
PandasTools.AddMoleculeColumnToFrame(df4,'smiles','Molecule')
df4[["smiles","Molecule"]].head(1)
136/68:
from rdkit import Chem, DataStructs
from rdkit.Chem import PandasTools, AllChem
PandasTools.AddMoleculeColumnToFrame(df4,'smiles','Molecule')
df4[["smiles","Molecule"]]
136/69: data.Molecule.isna().sum()
136/70: df4.Molecule.isna().sum()
136/71:
def mol2fp(mol):
    fp = AllChem.GetHashedMorganFingerprint(mol, 2, nBits=4096)
    ar = np.zeros((1,), dtype=np.int8)
    DataStructs.ConvertToNumpyArray(fp, ar)
    return ar
     
fp =mol2fp(Chem.MolFromSmiles(df4.loc[1,"SMILES"]))
plt.matshow(fp.reshape((64,-1)) &amp;gt; 0)
136/72:
def mol2fp(mol):
    fp = AllChem.GetHashedMorganFingerprint(mol, 2, nBits=4096)
    ar = np.zeros((1,), dtype=np.int8)
    DataStructs.ConvertToNumpyArray(fp, ar)
    return ar
     
fp =mol2fp(Chem.MolFromSmiles(df4.loc[1,"smiles"]))
plt.matshow(fp.reshape((64,-1)) &amp;gt; 0)
136/73:
def mol2fp(mol):
    fp = AllChem.GetHashedMorganFingerprint(mol, 2, nBits=4096)
    ar = np.zeros((1,), dtype=np.int8)
    DataStructs.ConvertToNumpyArray(fp, ar)
    return ar
     
fp =mol2fp(Chem.MolFromSmiles(df4.loc[1,"smiles"]))
plt.matshow(fp.reshape((64,-1)))
136/74: data["FPs"] = df4.Molecule.apply(mol2fp)
136/75: df4["FPs"] = df4.Molecule.apply(mol2fp)
136/76: df6 = pd.concat([df5], [df4['LogKp']])
136/77: df6 = pd.concat([df5, df4.LogKp], axis=1)
136/78: df6
136/79: df7 = df6.name.drop_duplicates()
136/80: df7
136/81:
selection = [LogKp, Cells]
df6 = pd.concat([df5, df4[selection]], axis=1)
136/82: selection = [LogKp, Cells]
136/83:
selection = ['LogKp', 'Cells']
df6 = pd.concat([df5, df4[selection]], axis=1)
136/84: df6
136/85: df7 = df6.drop_duplicates(subset=['name'])
136/86: df7
136/87: df7 = df6.drop_duplicates()
136/88: df7
136/89: df6
136/90: df = pd.read_csv('rawdataqsar_sm.csv')
136/91: df
136/92: df2 =df[df.kp.notna()]
136/93: df2
136/94: df3 = df2.drop(['logkp', 'kpu'], axis=1)
136/95: df3
136/96:
def LogKp(input):
    LogKp = []

    for i in input['kp']:
        LogKp.append(np.log10(i))

    input['LogKp'] = LogKp
    x = input.drop('kp', 1)   
    return x
136/97: df4 = LogKp(df3)
136/98: df4
136/99: df5 = df4.drop_duplicates(subset=['smiles'])
136/100:
df5 = df4.drop_duplicates(subset=['smiles'])
df5
136/101: df6 = df5.reset_index(drop=True)
136/102:
df6 = df5.reset_index(drop=True)
df6
136/103:
# Calculate Molecular Descriptor
from rdkit import Chem
from rdkit.Chem import Descriptors, Lipinski

def lipinski(smiles, verbose=False):

    moldata= []
    for elem in smiles:
        mol=Chem.MolFromSmiles(elem) 
        moldata.append(mol)
       
    baseData= np.arange(1,1)
    i=0  
    for mol in moldata:        
       
        desc_MolWt = Descriptors.MolWt(mol)
        desc_TPSA = Descriptors.TPSA(mol)
        desc_MolLogP = Descriptors.MolLogP(mol)
        desc_NumHDonors = Lipinski.NumHDonors(mol)
        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)
        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)
           
        row = np.array([desc_MolWt,
                        desc_TPSA,
                        desc_MolLogP,
                        desc_NumHDonors,
                        desc_NumHAcceptors,
                        desc_NumRotatableBonds])   
    
        if(i==0):
            baseData=row
        else:
            baseData=np.vstack([baseData, row])
        i=i+1      
    
    columnNames=["MW","TPSA", "LogP","NumHDonors","NumHAcceptors", "NumRotatableBonds"]   
    descriptors = pd.DataFrame(data=baseData,columns=columnNames)
    
    return descriptors
136/104: df6_lipinski = lipinski(df6)
136/105: df6_lipinski = lipinski(df6.smiles)
136/106:
df6_lipinski = lipinski(df6.smiles)
df6_lipinski
136/107:
# Calculate Molecular Descriptor
from rdkit import Chem
from rdkit.Chem import Descriptors, Lipinski

def lipinski(smiles, verbose=False):

    moldata= []
    for elem in smiles:
        mol=Chem.MolFromSmiles(elem) 
        moldata.append(mol)
       
    baseData= np.arange(1,1)
    i=0  
    for mol in moldata:        
       
        desc_MolWt = Descriptors.MolWt(mol)
        desc_TPSA = Descriptors.TPSA(mol)
        desc_MolLogP = Descriptors.MolLogP(mol)
        desc_NumHDonors = Lipinski.NumHDonors(mol)
        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)
        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)
        desc_FpDensityMorgan1 = Descriptors.FpDensityMorgan1(mol)
        desc_FpDensityMorgan2 = Descriptors.FpDensityMorgan2(mol)
        desc_FpDensityMorgan3 = Descriptors.FpDensityMorgan3(mol)
        desc_NumRadicalElectrons = Descriptors.NumRadicalElectrons(mol)
        desc_NumValenceElectrons = Descriptors.NumValenceElectrons(mol)
           
        row = np.array([desc_MolWt,
                        desc_TPSA,
                        desc_MolLogP,
                        desc_NumHDonors,
                        desc_NumHAcceptors,
                        desc_NumRotatableBonds,
                        desc_FpDensityMorgan1,
                        desc_FpDensityMorgan2,
                        desc_FpDensityMorgan3,
                        desc_NumRadicalElectrons,
                        desc_NumValenceElectrons])   
    
        if(i==0):
            baseData=row
        else:
            baseData=np.vstack([baseData, row])
        i=i+1      
    
    columnNames=["MW","TPSA", "LogP","NumHDonors","NumHAcceptors", "NumRotatableBonds", 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'NumRadicalElectrons', 'NumValenceElectrons']   
    descriptors = pd.DataFrame(data=baseData,columns=columnNames)
    
    return descriptors
136/108:
df6_lipinski = lipinski(df6.smiles)
df6_lipinski
136/109: df7 = pd.concat([df6, df6_lipinski], axis=1)
136/110:
df7 = pd.concat([df6, df6_lipinski], axis=1)
df7
136/111:
data = pd.concat([df6, df6_lipinski], axis=1)
data
136/112:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)
plt2.set_xlabel("LogP", fontsize=16, fontweight='bold')
plt2.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)
plt2.set_ylim(0, 50)
plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)


#Histogram for NumHDonors
plt3.hist(data['NumHDonors'], density=False, bins= 30, color='#FF9A00', edgecolor='black', linewidth=0.5)
plt3.set_xlabel("Number of Hydrogen Bond Donors", fontsize=16, fontweight='bold')
plt3.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)
plt3.set_ylim(0, 50)
plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for NumHAcceptors
plt4.hist(data['NumHAcceptors'], density=False, bins= 30, color='#FF165D', edgecolor='black', linewidth=0.5)
plt4.set_xlabel("Number of Hydrogen Bond Acceptors", fontsize=16, fontweight='bold')
plt4.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)
plt4.set_ylim(0, 50)
plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for NumRotatableBonds
plt5.hist(data['NumRotatableBonds'], density=False, bins= 30, color='#52006A', edgecolor='black', linewidth=0.5)
plt5.set_xlabel("Number of RotatableBonds", fontsize=16, fontweight='bold')
plt5.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)
plt5.set_ylim(0, 50)
plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for TPSA
plt6.hist(data['TPSA'], density=False, bins= 30, color='#52006A', edgecolor='black', linewidth=0.5)
plt6.set_xlabel("Topolocial Plar Surface Area", fontsize=16, fontweight='bold')
plt6.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt6.tick_params(axis='both', which='major', labelsize=14)
plt6.set_ylim(0, 50)
plt6.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#save file
plt.tight_layout()
plt.savefig('Lipinski_basic descriptors.pdf', dpi=300)
136/113:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)
plt2.set_xlabel("LogP", fontsize=16, fontweight='bold')
plt2.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)
plt2.set_ylim(0, 50)
plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)


#Histogram for NumHDonors
plt3.hist(data['NumHDonors'], density=False, bins= 30, color='#FF9A00', edgecolor='black', linewidth=0.5)
plt3.set_xlabel("Number of Hydrogen Bond Donors", fontsize=16, fontweight='bold')
plt3.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)
plt3.set_ylim(0, 50)
plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for NumHAcceptors
plt4.hist(data['NumHAcceptors'], density=False, bins= 30, color='#FF165D', edgecolor='black', linewidth=0.5)
plt4.set_xlabel("Number of Hydrogen Bond Acceptors", fontsize=16, fontweight='bold')
plt4.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)
plt4.set_ylim(0, 50)
plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for NumRotatableBonds
plt5.hist(data['NumRotatableBonds'], density=False, bins= 30, color='#52006A', edgecolor='black', linewidth=0.5)
plt5.set_xlabel("Number of RotatableBonds", fontsize=16, fontweight='bold')
plt5.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)
plt5.set_ylim(0, 50)
plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for TPSA
plt6.hist(data['TPSA'], density=False, bins= 30, color='#52006A', edgecolor='black', linewidth=0.5)
plt6.set_xlabel("Topolocial Plar Surface Area", fontsize=16, fontweight='bold')
plt6.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt6.tick_params(axis='both', which='major', labelsize=14)
plt6.set_ylim(0, 50)
plt6.axvline(90, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#save file
plt.tight_layout()
plt.savefig('Lipinski_basic descriptors.pdf', dpi=300)
136/114:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)
plt2.set_xlabel("LogP", fontsize=16, fontweight='bold')
plt2.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)
plt2.set_ylim(0, 50)
plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)


#Histogram for NumHDonors
plt3.hist(data['NumHDonors'], density=False, bins= 30, color='#FF9A00', edgecolor='black', linewidth=0.5)
plt3.set_xlabel("Number of Hydrogen Bond Donors", fontsize=16, fontweight='bold')
plt3.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)
plt3.set_ylim(0, 50)
plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for NumHAcceptors
plt4.hist(data['NumHAcceptors'], density=False, bins= 30, color='#FF165D', edgecolor='black', linewidth=0.5)
plt4.set_xlabel("Number of Hydrogen Bond Acceptors", fontsize=16, fontweight='bold')
plt4.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)
plt4.set_ylim(0, 50)
plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for NumRotatableBonds
plt5.hist(data['NumRotatableBonds'], density=False, bins= 30, color='#52006A', edgecolor='black', linewidth=0.5)
plt5.set_xlabel("Number of RotatableBonds", fontsize=16, fontweight='bold')
plt5.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)
plt5.set_ylim(0, 50)
plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for TPSA
plt6.hist(data['TPSA'], density=False, bins= 30, color='#DF711B', edgecolor='black', linewidth=0.5)
plt6.set_xlabel("Topolocial Plar Surface Area", fontsize=16, fontweight='bold')
plt6.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt6.tick_params(axis='both', which='major', labelsize=14)
plt6.set_ylim(0, 50)
plt6.axvline(90, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#save file
plt.tight_layout()
plt.savefig('Lipinski_basic descriptors.pdf', dpi=300)
136/115:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)
plt2.set_xlabel("LogP", fontsize=16, fontweight='bold')
plt2.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)
plt2.set_ylim(0, 50)
plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)


#Histogram for NumHDonors
plt3.hist(data['NumHDonors'], density=False, bins= 30, color='#FF9A00', edgecolor='black', linewidth=0.5)
plt3.set_xlabel("Number of Hydrogen Bond Donors", fontsize=16, fontweight='bold')
plt3.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)
plt3.set_ylim(0, 50)
plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for NumHAcceptors
plt4.hist(data['NumHAcceptors'], density=False, bins= 30, color='#FF165D', edgecolor='black', linewidth=0.5)
plt4.set_xlabel("Number of Hydrogen Bond Acceptors", fontsize=16, fontweight='bold')
plt4.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)
plt4.set_ylim(0, 50)
plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for NumRotatableBonds
plt5.hist(data['NumRotatableBonds'], density=False, bins= 30, color='#52006A', edgecolor='black', linewidth=0.5)
plt5.set_xlabel("Number of RotatableBonds", fontsize=16, fontweight='bold')
plt5.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)
plt5.set_ylim(0, 50)
plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for TPSA
plt6.hist(data['TPSA'], density=False, bins= 30, color='#01937C', edgecolor='black', linewidth=0.5)
plt6.set_xlabel("Topolocial Plar Surface Area", fontsize=16, fontweight='bold')
plt6.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt6.tick_params(axis='both', which='major', labelsize=14)
plt6.set_ylim(0, 50)
plt6.axvline(90, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#save file
plt.tight_layout()
plt.savefig('Lipinski_basic descriptors.pdf', dpi=300)
136/116:
selection = ['smiles', 'compound']
smiles = data[selection]
smiles.to_csv('new_smiles.smi', sep='\t', index=False, header=False)
136/117:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt



figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

x1 = data['MW']
x2 = data['LogP']
x3 = data['NumHDonors']
x4 = data['NumHAcceptors']
x5 = data['NumRotatableBonds']
x6 = data['TPSA']
y = data['LogKp']
colors = {'HEK293':'red', 'SK-MEL-2':'blue', 'HL60':'green', 'SH-5Y':'lightblue', 'PBMC':'yellow'}

#MW
plt1.scatter(x1, y, alpha=0.5, c=data['Cells'].map(colors))
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)

#LogP
plt2.scatter(x2, y, alpha=0.5, c=data['Cells'].map(colors))
plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')
plt2.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)

#NumHbondDonors
plt3.scatter(x3, y, alpha=0.5, c=data['Cells'].map(colors))
plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')
plt3.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt4.scatter(x4, y,  alpha=0.5, c=data['Cells'].map(colors))
plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')
plt4.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt5.scatter(x5, y, alpha=0.5, c=data['Cells'].map(colors))
plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')
plt5.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)

#TPSA
plt5.scatter(x6, y, alpha=0.5, c=data['Cells'].map(colors))
plt5.set_xlabel('Topological Polar Surface Area', fontsize=16, fontweight='bold')
plt5.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)

#save file
plt.tight_layout()
plt.savefig('RO5_and_Kpuu.pdf', dpi=300)
136/118:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt



figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

x1 = data['MW']
x2 = data['LogP']
x3 = data['NumHDonors']
x4 = data['NumHAcceptors']
x5 = data['NumRotatableBonds']
x6 = data['TPSA']
y = data['LogKp']
colors = {'HEK293':'red', 'SK-MEL-2':'blue', 'HL60':'green', 'SH-5Y':'lightblue', 'PBMC':'yellow'}

#MW
plt1.scatter(x1, y, alpha=0.5, c=data['Cells'].map(colors))
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)

#LogP
plt2.scatter(x2, y, alpha=0.5, c=data['Cells'].map(colors))
plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')
plt2.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)

#NumHbondDonors
plt3.scatter(x3, y, alpha=0.5, c=data['Cells'].map(colors))
plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')
plt3.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt4.scatter(x4, y,  alpha=0.5, c=data['Cells'].map(colors))
plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')
plt4.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt5.scatter(x5, y, alpha=0.5, c=data['Cells'].map(colors))
plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')
plt5.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)

#TPSA
plt6.scatter(x6, y, alpha=0.5, c=data['Cells'].map(colors))
plt6.set_xlabel('Topological Polar Surface Area', fontsize=16, fontweight='bold')
plt6.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt6.tick_params(axis='both', which='major', labelsize=14)

#save file
plt.tight_layout()
plt.savefig('RO5_and_Kpuu.pdf', dpi=300)
136/119:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt



figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

x1 = data['MW']
x2 = data['LogP']
x3 = data['NumHDonors']
x4 = data['NumHAcceptors']
x5 = data['NumRotatableBonds']
x6 = data['TPSA']
y = data['LogKp']
colors = {'HEK293':'red', 'SK-MEL-2':'blue', 'HL60':'green', 'SH-5Y':'lightblue', 'PBMC':'yellow'}

#MW
plt1.scatter(x1, y, alpha=0.5, c=data['Cells'].map(colors))
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)

#LogP
plt2.scatter(x2, y, alpha=0.5, c=data['Cells'].map(colors))
plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')
plt2.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)

#NumHbondDonors
plt3.scatter(x3, y, alpha=0.5, c=data['Cells'].map(colors))
plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')
plt3.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt4.scatter(x4, y,  alpha=0.5, c=data['Cells'].map(colors))
plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')
plt4.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt5.scatter(x5, y, alpha=0.5, c=data['Cells'].map(colors))
plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')
plt5.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)

#TPSA
plt6.scatter(x6, y, alpha=0.5, c=data['Cells'].map(colors))
plt6.set_xlabel('Topological Polar Surface Area', fontsize=16, fontweight='bold')
plt6.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt6.tick_params(axis='both', which='major', labelsize=14)

#save file
plt.tight_layout()
plt.savefig('RO5_and_LogPk.pdf', dpi=300)
136/120:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt



figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

x1 = data['MW']
x2 = data['LogP']
x3 = data['NumHDonors']
x4 = data['NumHAcceptors']
x5 = data['NumRotatableBonds']
x6 = data['TPSA']
y = data['LogKp']
colors = {'HEK293':'red', 'SK-MEL-2':'blue', 'HL60':'green', 'SH-5Y':'lightblue', 'PBMC':'yellow'}

#MW
plt1.scatter(x1, y, alpha=0.5, c=data['Cells'].map(colors))
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)

#LogP
plt2.scatter(x2, y, alpha=0.5, c=data['Cells'].map(colors))
plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')
plt2.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)

#NumHbondDonors
plt3.scatter(x3, y, alpha=0.5, c=data['Cells'].map(colors))
plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')
plt3.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt4.scatter(x4, y,  alpha=0.5, c=data['Cells'].map(colors))
plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')
plt4.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt5.scatter(x5, y, alpha=0.5, c=data['Cells'].map(colors))
plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')
plt5.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)

#TPSA
plt6.scatter(x6, y, alpha=0.5, c=data['Cells'].map(colors))
plt6.set_xlabel('Topological Polar Surface Area', fontsize=16, fontweight='bold')
plt6.set_ylabel("$Kp_{uu}$", fontsize=16, fontweight='bold')
plt6.tick_params(axis='both', which='major', labelsize=14)

#save file
plt.tight_layout()
plt.savefig('RO5_and_LogKp.pdf', dpi=300)
136/121:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt



figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

x1 = data['MW']
x2 = data['LogP']
x3 = data['NumHDonors']
x4 = data['NumHAcceptors']
x5 = data['NumRotatableBonds']
x6 = data['TPSA']
y = data['LogKp']
colors = {'HEK293':'red', 'SK-MEL-2':'blue', 'HL60':'green', 'SH-5Y':'lightblue', 'PBMC':'yellow'}

#MW
plt1.scatter(x1, y, alpha=0.5, c=data['Cells'].map(colors))
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("log $Kp_{uu}$", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)

#LogP
plt2.scatter(x2, y, alpha=0.5, c=data['Cells'].map(colors))
plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')
plt2.set_ylabel("log $Kp_{uu}$", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)

#NumHbondDonors
plt3.scatter(x3, y, alpha=0.5, c=data['Cells'].map(colors))
plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')
plt3.set_ylabel("log $Kp_{uu}$", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt4.scatter(x4, y,  alpha=0.5, c=data['Cells'].map(colors))
plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')
plt4.set_ylabel("log $Kp_{uu}$", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt5.scatter(x5, y, alpha=0.5, c=data['Cells'].map(colors))
plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')
plt5.set_ylabel("log $Kp_{uu}$", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)

#TPSA
plt6.scatter(x6, y, alpha=0.5, c=data['Cells'].map(colors))
plt6.set_xlabel('Topological Polar Surface Area', fontsize=16, fontweight='bold')
plt6.set_ylabel("log $Kp_{uu}$", fontsize=16, fontweight='bold')
plt6.tick_params(axis='both', which='major', labelsize=14)

#save file
plt.tight_layout()
plt.savefig('RO5_and_LogKp.pdf', dpi=300)
136/122:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt



figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

x1 = data['MW']
x2 = data['LogP']
x3 = data['NumHDonors']
x4 = data['NumHAcceptors']
x5 = data['NumRotatableBonds']
x6 = data['TPSA']
y = data['LogKp']
colors = {'HEK293':'red', 'SK-MEL-2':'blue', 'HL60':'green', 'SH-5Y':'lightblue', 'PBMC':'yellow'}

#MW
plt1.scatter(x1, y, alpha=0.5, c=data['Cells'].map(colors))
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("log $Kp_{uu}$", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)

#LogP
plt2.scatter(x2, y, alpha=0.5, c=data['Cells'].map(colors))
plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')
plt2.set_ylabel("log $Kp_{uu}$", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)

#NumHbondDonors
plt3.scatter(x3, y, alpha=0.5, c=data['Cells'].map(colors))
plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')
plt3.set_ylabel("log $Kp_{uu}$", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt4.scatter(x4, y,  alpha=0.5, c=data['Cells'].map(colors))
plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')
plt4.set_ylabel("log $Kp_{uu}$", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt5.scatter(x5, y, alpha=0.5, c=data['Cells'].map(colors))
plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')
plt5.set_ylabel("log $Kp_{uu}$", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)

#TPSA
plt6.scatter(x6, y, alpha=0.5, c=data['Cells'].map(colors))
plt6.set_xlabel('Topological Polar Surface Area', fontsize=16, fontweight='bold')
plt6.set_ylabel("log Kp$_{uu}$", fontsize=16, fontweight='bold')
plt6.tick_params(axis='both', which='major', labelsize=14)

#save file
plt.tight_layout()
plt.savefig('RO5_and_LogKp.pdf', dpi=300)
136/123:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt



figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

x1 = data['MW']
x2 = data['LogP']
x3 = data['NumHDonors']
x4 = data['NumHAcceptors']
x5 = data['NumRotatableBonds']
x6 = data['TPSA']
y = data['LogKp']
colors = {'HEK293':'red', 'SK-MEL-2':'blue', 'HL60':'green', 'SH-5Y':'lightblue', 'PBMC':'yellow'}

#MW
plt1.scatter(x1, y, alpha=0.5, c=data['Cells'].map(colors))
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("log Kp", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)

#LogP
plt2.scatter(x2, y, alpha=0.5, c=data['Cells'].map(colors))
plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')
plt2.set_ylabel("log Kp", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)

#NumHbondDonors
plt3.scatter(x3, y, alpha=0.5, c=data['Cells'].map(colors))
plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')
plt3.set_ylabel("log Kp", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt4.scatter(x4, y,  alpha=0.5, c=data['Cells'].map(colors))
plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')
plt4.set_ylabel("log Kp", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt5.scatter(x5, y, alpha=0.5, c=data['Cells'].map(colors))
plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')
plt5.set_ylabel("log Kp", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)

#TPSA
plt6.scatter(x6, y, alpha=0.5, c=data['Cells'].map(colors))
plt6.set_xlabel('Topological Polar Surface Area', fontsize=16, fontweight='bold')
plt6.set_ylabel("log Kp", fontsize=16, fontweight='bold')
plt6.tick_params(axis='both', which='major', labelsize=14)

#save file
plt.tight_layout()
plt.savefig('RO5_and_LogKp.pdf', dpi=300)
136/124:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt



figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

x1 = data['MW']
x2 = data['LogP']
x3 = data['NumHDonors']
x4 = data['NumHAcceptors']
x5 = data['NumRotatableBonds']
x6 = data['TPSA']
y = data['LogKp']
colors = {'HEK293':'red', 'SK-MEL-2':'blue', 'HL60':'green', 'SH-5Y':'lightblue', 'PBMC':'yellow'}

#MW
plt1.scatter(x1, y, alpha=0.5, c=data['Cells'].map(colors))
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("log Kp", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)

#LogP
plt2.scatter(x2, y, alpha=0.5, c=data['Cells'].map(colors))
plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')
plt2.set_ylabel("log Kp", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)

#NumHbondDonors
plt3.scatter(x3, y, alpha=0.5, c=data['Cells'].map(colors))
plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')
plt3.set_ylabel("log Kp", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt4.scatter(x4, y,  alpha=0.5, c=data['Cells'].map(colors))
plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')
plt4.set_ylabel("log Kp", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt5.scatter(x5, y, alpha=0.5, c=data['Cells'].map(colors))
plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')
plt5.set_ylabel("log Kp", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)

#TPSA
plt6.scatter(x6, y, alpha=0.5, c=data['Cells'].map(colors))
plt6.set_xlabel('Topological Polar Surface Area', fontsize=16, fontweight='bold')
plt6.set_ylabel("log Kp", fontsize=16, fontweight='bold')
plt6.tick_params(axis='both', which='major', labelsize=14)

#save file
plt.legend(colors)
plt.tight_layout()
plt.savefig('RO5_and_LogKp.pdf', dpi=300)
124/10:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
124/11: pwd
124/12: df = pd.read_csv('new_molecules.smi')
124/13: df = pd.read_csv('new_smiles.smi.smi')
124/14: df = pd.read_csv('new_smiles.smi')
124/15:
df = pd.read_csv('new_smiles.smi')
df
124/16:
list_Fp = ['Fingerprinter.xml', 'ExtendedFingerprinter.xml', 'EStateFingerprinter.xml',
           'GraphOnlyFingerprinter.xml', 'MACCSFingerprinter.xml', 'PubchemFingerprinter.xml',
           'SubstructureFingerprinter.xml', 'KlekotaRothFingerprinter.xml', 'AtomPairs2DFingerprinter.xml']
124/17:
def Fingerprint(name , Fp):
    
    group = name
    group = [name.replace(".smi", ".csv") for name in group]
    result = ', '.join(group)
    
    Fp_name = Fp
    Fp_name = Fp.replace(".xml", "_")
    
    print (name)
    print (result)
    print (Fp)
    print (Fp_name)

    ! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/$Fp' \
    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/$name' \
    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result'
124/18:
path =  '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/'
for f in glob.glob(path + '*.smi'):
    names = [os.path.basename(f)]
    for Fp in (list_Fp):
        Fingerprint(names, Fp)
124/19:
import os
import glob
path =  '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/'
for f in glob.glob(path + '*.smi'):
    names = [os.path.basename(f)]
    for Fp in (list_Fp):
        Fingerprint(names, Fp)
124/20:
def Fingerprint(name , Fp):
    
    group = name
    group = [name.replace(".smi", ".csv") for name in group]
    result = ', '.join(group)
    
    Fp_name = Fp
    Fp_name = Fp.replace(".xml", "_")
    
    print (name)
    print (result)
    print (Fp)
    print (Fp_name)

    ! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp' \
    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name' \
    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result'
124/21:
import os
import glob
path =  '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/'
for f in glob.glob(path + '*.smi'):
    names = [os.path.basename(f)]
    for Fp in (list_Fp):
        Fingerprint(names, Fp)
136/125:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt



figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

x1 = data['MW']
x2 = data['LogP']
x3 = data['NumHDonors']
x4 = data['NumHAcceptors']
x5 = data['NumRotatableBonds']
x6 = data['TPSA']
y = data['LogKp']
colors = {'HEK293':'red', 'SK-MEL-2':'blue', 'HL60':'green', 'SH-5Y':'lightblue', 'PBMC':'yellow'}

#MW
plt1.scatter(x1, y, alpha=0.5, c=data['Cells'].map(colors))
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("log Kp", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)

#LogP
plt2.scatter(x2, y, alpha=0.5, c=data['Cells'].map(colors))
plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')
plt2.set_ylabel("log Kp", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)

#NumHbondDonors
plt3.scatter(x3, y, alpha=0.5, c=data['Cells'].map(colors))
plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')
plt3.set_ylabel("log Kp", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt4.scatter(x4, y,  alpha=0.5, c=data['Cells'].map(colors))
plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')
plt4.set_ylabel("log Kp", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt5.scatter(x5, y, alpha=0.5, c=data['Cells'].map(colors))
plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')
plt5.set_ylabel("log Kp", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)

#TPSA
plt6.scatter(x6, y, alpha=0.5, c=data['Cells'].map(colors))
plt6.set_xlabel('Topological Polar Surface Area', fontsize=16, fontweight='bold')
plt6.set_ylabel("log Kp", fontsize=16, fontweight='bold')
plt6.tick_params(axis='both', which='major', labelsize=14)

#save file
plt.legend()
plt.tight_layout()
plt.savefig('RO5_and_LogKp.pdf', dpi=300)
136/126:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt



figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

x1 = data['MW']
x2 = data['LogP']
x3 = data['NumHDonors']
x4 = data['NumHAcceptors']
x5 = data['NumRotatableBonds']
x6 = data['TPSA']
y = data['LogKp']
colors = {'HEK293':'red', 'SK-MEL-2':'blue', 'HL60':'green', 'SH-5Y':'lightblue', 'PBMC':'yellow'}

#MW
plt1.scatter(x1, y, alpha=0.5, c=data['Cells'].map(colors))
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("log Kp", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)

#LogP
plt2.scatter(x2, y, alpha=0.5, c=data['Cells'].map(colors))
plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')
plt2.set_ylabel("log Kp", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)

#NumHbondDonors
plt3.scatter(x3, y, alpha=0.5, c=data['Cells'].map(colors))
plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')
plt3.set_ylabel("log Kp", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt4.scatter(x4, y,  alpha=0.5, c=data['Cells'].map(colors))
plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')
plt4.set_ylabel("log Kp", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt5.scatter(x5, y, alpha=0.5, c=data['Cells'].map(colors))
plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')
plt5.set_ylabel("log Kp", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)

#TPSA
plt6.scatter(x6, y, alpha=0.5, c=data['Cells'].map(colors))
plt6.set_xlabel('Topological Polar Surface Area', fontsize=16, fontweight='bold')
plt6.set_ylabel("log Kp", fontsize=16, fontweight='bold')
plt6.tick_params(axis='both', which='major', labelsize=14)

#save file
plt1.legend()
plt.tight_layout()
plt.savefig('RO5_and_LogKp.pdf', dpi=300)
136/127:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt



figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

x1 = data['MW']
x2 = data['LogP']
x3 = data['NumHDonors']
x4 = data['NumHAcceptors']
x5 = data['NumRotatableBonds']
x6 = data['TPSA']
y = data['LogKp']
colors = {'HEK293':'red', 'SK-MEL-2':'blue', 'HL60':'green', 'SH-5Y':'lightblue', 'PBMC':'yellow'}

#MW
plt1.scatter(x1, y, alpha=0.5, c=data['Cells'].map(colors))
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("log Kp", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)

#LogP
plt2.scatter(x2, y, alpha=0.5, c=data['Cells'].map(colors))
plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')
plt2.set_ylabel("log Kp", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)

#NumHbondDonors
plt3.scatter(x3, y, alpha=0.5, c=data['Cells'].map(colors))
plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')
plt3.set_ylabel("log Kp", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt4.scatter(x4, y,  alpha=0.5, c=data['Cells'].map(colors))
plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')
plt4.set_ylabel("log Kp", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt5.scatter(x5, y, alpha=0.5, c=data['Cells'].map(colors))
plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')
plt5.set_ylabel("log Kp", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)

#TPSA
plt6.scatter(x6, y, alpha=0.5, c=data['Cells'].map(colors))
plt6.set_xlabel('Topological Polar Surface Area', fontsize=16, fontweight='bold')
plt6.set_ylabel("log Kp", fontsize=16, fontweight='bold')
plt6.tick_params(axis='both', which='major', labelsize=14)

#save file
plt.tight_layout()
plt.savefig('RO5_and_LogKp.pdf', dpi=300)
124/22:
import os
import glob
path =  '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/'
for f in glob.glob(path + '*.smi'):
    names = [os.path.basename(f)]
    for Fp in (list_Fp):
        Fingerprint(names, Fp)
124/23:
list_Fp = ['Descriptor.xml'. 'Fingerprinter.xml', 'ExtendedFingerprinter.xml', 'EStateFingerprinter.xml',
           'GraphOnlyFingerprinter.xml', 'MACCSFingerprinter.xml', 'PubchemFingerprinter.xml',
           'SubstructureFingerprinter.xml', 'KlekotaRothFingerprinter.xml', 'AtomPairs2DFingerprinter.xml']
124/24:
list_Fp = ['Descriptor.xml', 'Fingerprinter.xml', 'ExtendedFingerprinter.xml', 'EStateFingerprinter.xml',
           'GraphOnlyFingerprinter.xml', 'MACCSFingerprinter.xml', 'PubchemFingerprinter.xml',
           'SubstructureFingerprinter.xml', 'KlekotaRothFingerprinter.xml', 'AtomPairs2DFingerprinter.xml']
124/25:
def Fingerprint(name , Fp):
    
    group = name
    group = [name.replace(".smi", ".csv") for name in group]
    result = ', '.join(group)
    
    Fp_name = Fp
    Fp_name = Fp.replace(".xml", "_")
    
    print (name)
    print (result)
    print (Fp)
    print (Fp_name)

    ! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/$Fp' \
    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name' \
    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result'
124/26:
import os
import glob
path =  '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/'
for f in glob.glob(path + '*.smi'):
    names = [os.path.basename(f)]
    for Fp in (list_Fp):
        Fingerprint(names, Fp)
124/27:
! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/Fingerprinter.xml' \
    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name' \
    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result'
124/28:
! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/Fingerprinter.xml' \
    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \
    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/Fingerprinter.csv'
124/29:
! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/ExtendedFingerprinter.xml' \
    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \
    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/ExtendedFingerprinter.csv'
124/30:
! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/EStateFingerprinter.xml' \
    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \
    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/EStateFingerprinter.csv'
124/31:
! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/GraphOnlyFingerprinter.xml' \
    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \
    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/GraphOnlyFingerprinter.csv'
124/32:
! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/MACCSFingerprinter.xml' \
    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \
    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/MACCSFingerprinter.csv'
124/33:
! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PubchemFingerprinter.xml' \
    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \
    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PubchemFingerprinter.csv'
124/34:
! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/SubstructureFingerprinter.xml' \
    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \
    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/SubstructureFingerprinter.csv'
124/35:
! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/KlekotaRothFingerprinter.xml' \
    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \
    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/KlekotaRothFingerprinter.csv'
124/36:
! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/AtomPairs2DFingerprinter.xml' \
    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \
    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/AtomPairs2DFingerprinter.csv'
135/20:
path = r'Fingerprint/'

Fp1  = pd.read_csv(path + 'Train_ER_alpha-Fingerprinter.csv'               , header = 0)
Fp2  = pd.read_csv(path + 'Train_ER_alpha-ExtendedFingerprinter.csv'       , header = 0)
Fp3  = pd.read_csv(path + 'Train_ER_alpha-EStateFingerprinter.csv'         , header = 0)
Fp4  = pd.read_csv(path + 'Train_ER_alpha-GraphOnlyFingerprinter.csv'      , header = 0)
Fp5  = pd.read_csv(path + 'Train_ER_alpha-MACCSFingerprinter.csv'          , header = 0)
Fp6  = pd.read_csv(path + 'Train_ER_alpha-PubchemFingerprinter.csv'        , header = 0)
Fp7  = pd.read_csv(path + 'Train_ER_alpha-SubstructureFingerprinter.csv'   , header = 0)
Fp8  = pd.read_csv(path + 'Train_ER_alpha-SubstructureFingerprintCount.csv', header = 0)
Fp9  = pd.read_csv(path + 'Train_ER_alpha-KlekotaRothFingerprinter.csv'    , header = 0)
Fp10 = pd.read_csv(path + 'Train_ER_alpha-KlekotaRothFingerprintCount.csv' , header = 0)
Fp11 = pd.read_csv(path + 'Train_ER_alpha-AtomPairs2DFingerprinter.csv'    , header = 0)
Fp12 = pd.read_csv(path + 'Train_ER_alpha-AtomPairs2DFingerprintCount.csv' , header = 0)
135/21:
path = r'Fingerprint/'

Fp1  = pd.read_csv(path + 'Fingerprinter.csv'               , header = 0)
Fp2  = pd.read_csv(path + 'ExtendedFingerprinter.csv'       , header = 0)
Fp3  = pd.read_csv(path + 'EStateFingerprinter.csv'         , header = 0)
Fp4  = pd.read_csv(path + 'GraphOnlyFingerprinter.csv'      , header = 0)
Fp5  = pd.read_csv(path + 'MACCSFingerprinter.csv'          , header = 0)
Fp6  = pd.read_csv(path + 'PubchemFingerprinter.csv'        , header = 0)
Fp7  = pd.read_csv(path + 'SubstructureFingerprinter.csv'   , header = 0)
Fp8  = pd.read_csv(path + 'SubstructureFingerprintCount.csv', header = 0)
Fp9  = pd.read_csv(path + 'KlekotaRothFingerprinter.csv'    , header = 0)
Fp10 = pd.read_csv(path + 'KlekotaRothFingerprintCount.csv' , header = 0)
Fp11 = pd.read_csv(path + 'AtomPairs2DFingerprinter.csv'    , header = 0)
Fp12 = pd.read_csv(path + 'AtomPairs2DFingerprintCount.csv' , header = 0)
124/37:
list_Fp = ['Descriptor.xml', 'Fingerprinter.xml', 'ExtendedFingerprinter.xml', 'EStateFingerprinter.xml',
           'GraphOnlyFingerprinter.xml', 'MACCSFingerprinter.xml', 'PubchemFingerprinter.xml',
           'SubstructureFingerprinter.xml', 'KlekotaRothFingerprinter.xml', 'AtomPairs2DFingerprinter.xml', 'AtomPairs2DFingerprintCount.xml', 'KlekotaRothFingerprintCount.xml', 'SubstructureFingerprintCount.xml' ]
124/38:
! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/AtomPairs2DFingerprintCount.xml' \
    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \
    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/AtomPairs2DFingerprintCount.csv'
124/39:
! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/KlekotaRothFingerprintCount.xml' \
    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \
    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/KlekotaRothFingerprintCount.csv'
124/40:
! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/SubstructureFingerprintCount.xml' \
    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \
    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/SubstructureFingerprintCount.csv'
135/22:
path = r'Fingerprint/'

Fp1  = pd.read_csv(path + 'Fingerprinter.csv'               , header = 0)
Fp2  = pd.read_csv(path + 'ExtendedFingerprinter.csv'       , header = 0)
Fp3  = pd.read_csv(path + 'EStateFingerprinter.csv'         , header = 0)
Fp4  = pd.read_csv(path + 'GraphOnlyFingerprinter.csv'      , header = 0)
Fp5  = pd.read_csv(path + 'MACCSFingerprinter.csv'          , header = 0)
Fp6  = pd.read_csv(path + 'PubchemFingerprinter.csv'        , header = 0)
Fp7  = pd.read_csv(path + 'SubstructureFingerprinter.csv'   , header = 0)
Fp8  = pd.read_csv(path + 'SubstructureFingerprintCount.csv', header = 0)
Fp9  = pd.read_csv(path + 'KlekotaRothFingerprinter.csv'    , header = 0)
Fp10 = pd.read_csv(path + 'KlekotaRothFingerprintCount.csv' , header = 0)
Fp11 = pd.read_csv(path + 'AtomPairs2DFingerprinter.csv'    , header = 0)
Fp12 = pd.read_csv(path + 'AtomPairs2DFingerprintCount.csv' , header = 0)
135/23: Fp12
124/41:
! java -jar  '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/Fingerprinter.xml' \
    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \
    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/Fingerprinter.csv'
124/42:
! java -jar  '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/Fingerprinter.xml' \
    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \
    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/Fingerprinter.csv'
124/43:
! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/ExtendedFingerprinter.xml' \
    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \
    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/ExtendedFingerprinter.csv'
136/128:
selection = ['smiles']
smiles = data[selection]
smiles.to_csv('new_smiles.smi', sep='\t', index=False, header=False)
136/129:
selection = ['smiles']
smiles = data[selection]
smiles.to_csv(r'/smiles/new_smiles.smi', sep='\t', index=False, header=False)
136/130:
selection = ['smiles']
smiles = data[selection]
smiles.to_csv(r'smiles/new_smiles.smi', sep='\t', index=False, header=False)
124/44:
! java -jar  '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/Fingerprinter.xml' \
    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \
    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/Fingerprinter.csv'
124/45:
! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/ExtendedFingerprinter.xml' \
    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \
    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/ExtendedFingerprinter.csv'
124/46:
! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/EStateFingerprinter.xml' \
    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \
    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/EStateFingerprinter.csv'
124/47:
! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/GraphOnlyFingerprinter.xml' \
    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \
    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/GraphOnlyFingerprinter.csv'
124/48:
! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/MACCSFingerprinter.xml' \
    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \
    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/MACCSFingerprinter.csv'
124/49:
! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PubchemFingerprinter.xml' \
    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \
    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/PubchemFingerprinter.csv'
124/50:
! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/SubstructureFingerprinter.xml' \
    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \
    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/SubstructureFingerprinter.csv'
124/51:
! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/KlekotaRothFingerprinter.xml' \
    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \
    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/KlekotaRothFingerprinter.csv'
124/52:
! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/AtomPairs2DFingerprinter.xml' \
    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \
    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/AtomPairs2DFingerprinter.csv'
124/53:
! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/AtomPairs2DFingerprintCount.xml' \
    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \
    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/AtomPairs2DFingerprintCount.csv'
124/54:
! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/KlekotaRothFingerprintCount.xml' \
    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \
    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/KlekotaRothFingerprintCount.csv'
124/55:
! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/SubstructureFingerprintCount.xml' \
    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles.smi' \
    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/SubstructureFingerprintCount.csv'
135/24:
path = r'Fingerprint/'

Fp1  = pd.read_csv(path + 'Fingerprinter.csv'               , header = 0)
Fp2  = pd.read_csv(path + 'ExtendedFingerprinter.csv'       , header = 0)
Fp3  = pd.read_csv(path + 'EStateFingerprinter.csv'         , header = 0)
Fp4  = pd.read_csv(path + 'GraphOnlyFingerprinter.csv'      , header = 0)
Fp5  = pd.read_csv(path + 'MACCSFingerprinter.csv'          , header = 0)
Fp6  = pd.read_csv(path + 'PubchemFingerprinter.csv'        , header = 0)
Fp7  = pd.read_csv(path + 'SubstructureFingerprinter.csv'   , header = 0)
Fp8  = pd.read_csv(path + 'SubstructureFingerprintCount.csv', header = 0)
Fp9  = pd.read_csv(path + 'KlekotaRothFingerprinter.csv'    , header = 0)
Fp10 = pd.read_csv(path + 'KlekotaRothFingerprintCount.csv' , header = 0)
Fp11 = pd.read_csv(path + 'AtomPairs2DFingerprinter.csv'    , header = 0)
Fp12 = pd.read_csv(path + 'AtomPairs2DFingerprintCount.csv' , header = 0)
135/25:
path = r'Fingerprint/'

Fp1  = pd.read_csv(path + 'Fingerprinter.csv'               , header = 0)
Fp2  = pd.read_csv(path + 'ExtendedFingerprinter.csv'       , header = 0)
Fp3  = pd.read_csv(path + 'EStateFingerprinter.csv'         , header = 0)
Fp4  = pd.read_csv(path + 'GraphOnlyFingerprinter.csv'      , header = 0)
Fp5  = pd.read_csv(path + 'MACCSFingerprinter.csv'          , header = 0)
Fp6  = pd.read_csv(path + 'PubchemFingerprinter.csv'        , header = 0)
Fp7  = pd.read_csv(path + 'SubstructureFingerprinter.csv'   , header = 0)
Fp8  = pd.read_csv(path + 'SubstructureFingerprintCount.csv', header = 0)
Fp9  = pd.read_csv(path + 'KlekotaRothFingerprinter.csv'    , header = 0)
Fp10 = pd.read_csv(path + 'KlekotaRothFingerprintCount.csv' , header = 0)
Fp11 = pd.read_csv(path + 'AtomPairs2DFingerprinter.csv'    , header = 0)
Fp12 = pd.read_csv(path + 'AtomPairs2DFingerprintCount.csv' , header = 0)
135/26: Fp12
135/27: pwd
136/131:
data = pd.concat([df6, df6_lipinski], axis=1)
data.to_csv('df6_lipinski.csv')
data
136/132:
data = pd.concat([df6, df6_lipinski], axis=1)
data.to_csv('df6_lipinski.csv')
data
135/28: data = pd.read_csv('df6_lipinski.csv')
135/29:
data = pd.read_csv('df6_lipinski.csv')
data
135/30:
df = pd.read_csv('df6_lipinski.csv')
df
135/31: name = df['name']
135/32: name = df[name]
135/33: name = df[compound]
135/34: name = df.compound
135/35: name
135/36: name = df['compound']
135/37: name
135/38: name = pd.df['compound']
135/39: name = df[compound]
135/40: name.shape
135/41: name = df['compound']
135/42: name.shape
135/43: Fp1 = Fp1.replace(columns = {'Name':'compound'})
135/44: Fp1 = Fp1.replace({'Name':'compound'})
135/45: Fp1
135/46: Fp1 = Fp1.replace({'Name':'name'})
135/47: Fp1
135/48: Fp1 = Fp1.replace(['Name':'name'])
135/49: Fp1 = Fp1.replace(['Name','name'])
135/50: Fp1
135/51: Fp1.Name = name
135/52: Fp1
135/53:
Fp1.Name = name
Fp2.Name = name
Fp3.Name = name
Fp4.Name = name
Fp5.Name = name
Fp6.Name = name
Fp7.Name = name
Fp8.Name = name
Fp9.Name = name
Fp10.Name = name
Fp11.Name = name
Fp12.Name = name
135/54:
from sklearn import preprocessing

def normalized (Fp):
    
    Name = Fp.Name
    Fp_ix = Fp.ix[:,1:]
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp_ix)
    Fp_normalized = pd.DataFrame(np_scaled)
    Fp_normalized
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp_ix.columns)
    Fp_normalized['Name'] = Name
    
    return Fp_normalized
135/55:
Fp1_n  = normalized (Fp1 )
Fp2_n  = normalized (Fp2 )
Fp3_n  = normalized (Fp3 )
Fp4_n  = normalized (Fp4 )
Fp5_n  = normalized (Fp5 )
Fp6_n  = normalized (Fp6 )
Fp7_n  = normalized (Fp7 )
Fp8_n  = normalized (Fp8 )
Fp9_n  = normalized (Fp9 )
Fp10_n = normalized (Fp10)
Fp11_n = normalized (Fp11)
Fp12_n = normalized (Fp12)
135/56: Fp1
135/57:
from sklearn import preprocessing

def normalized (Fp):
    
    Name = Fp.Name
    Fp_ix = Fp.x[:,1:]
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp_ix)
    Fp_normalized = pd.DataFrame(np_scaled)
    Fp_normalized
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp_ix.columns)
    Fp_normalized['Name'] = Name
    
    return Fp_normalized
135/58:
Fp1_n  = normalized (Fp1 )
Fp2_n  = normalized (Fp2 )
Fp3_n  = normalized (Fp3 )
Fp4_n  = normalized (Fp4 )
Fp5_n  = normalized (Fp5 )
Fp6_n  = normalized (Fp6 )
Fp7_n  = normalized (Fp7 )
Fp8_n  = normalized (Fp8 )
Fp9_n  = normalized (Fp9 )
Fp10_n = normalized (Fp10)
Fp11_n = normalized (Fp11)
Fp12_n = normalized (Fp12)
135/59:
from sklearn import preprocessing

def normalized (Fp):
    
    Name = Fp.Name
    Fp_ix = Fp[:,1:]
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp_ix)
    Fp_normalized = pd.DataFrame(np_scaled)
    Fp_normalized
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp_ix.columns)
    Fp_normalized['Name'] = Name
    
    return Fp_normalized
135/60:
Fp1_n  = normalized (Fp1 )
Fp2_n  = normalized (Fp2 )
Fp3_n  = normalized (Fp3 )
Fp4_n  = normalized (Fp4 )
Fp5_n  = normalized (Fp5 )
Fp6_n  = normalized (Fp6 )
Fp7_n  = normalized (Fp7 )
Fp8_n  = normalized (Fp8 )
Fp9_n  = normalized (Fp9 )
Fp10_n = normalized (Fp10)
Fp11_n = normalized (Fp11)
Fp12_n = normalized (Fp12)
135/61: Fp1.drop(Name)
135/62: Fp1.drop('Name')
135/63: Fp1.drop('Name', axis=1)
135/64:
Fp1.Name = name
Fp2.Name = name
Fp3.Name = name
Fp4.Name = name
Fp5.Name = name
Fp6.Name = name
Fp7.Name = name
Fp8.Name = name
Fp9.Name = name
Fp10.Name = name
Fp11.Name = name
Fp12.Name = name
135/65: Fp1
135/66:
Fp1.drop('Name', axis=1)
Fp2.drop('Name', axis=1)
Fp3.drop('Name', axis=1)
Fp4.drop('Name', axis=1)
Fp5.drop('Name', axis=1)
Fp6.drop('Name', axis=1)
Fp7.drop('Name', axis=1)
Fp8.drop('Name', axis=1)
Fp9.drop('Name', axis=1)
Fp10.drop('Name', axis=1)
Fp11.drop('Name', axis=1)
Fp12.drop('Name', axis=1)
135/67:
Fp1.drop('Name', axis=1)
Fp2.drop('Name', axis=1)
Fp3.drop('Name', axis=1)
Fp4.drop('Name', axis=1)
Fp5.drop('Name', axis=1)
Fp6.drop('Name', axis=1)
Fp7.drop('Name', axis=1)
Fp8.drop('Name', axis=1)
Fp9.drop('Name', axis=1)
Fp10.drop('Name', axis=1)
Fp11.drop('Name', axis=1)
Fp12.drop('Name', axis=1)
135/68:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled)
    Fp_normalized
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized['Name'] = Name
    
    return Fp_normalized
135/69:
Fp1_n  = normalized (Fp1 )
Fp2_n  = normalized (Fp2 )
Fp3_n  = normalized (Fp3 )
Fp4_n  = normalized (Fp4 )
Fp5_n  = normalized (Fp5 )
Fp6_n  = normalized (Fp6 )
Fp7_n  = normalized (Fp7 )
Fp8_n  = normalized (Fp8 )
Fp9_n  = normalized (Fp9 )
Fp10_n = normalized (Fp10)
Fp11_n = normalized (Fp11)
Fp12_n = normalized (Fp12)
135/70: Fp1
135/71:
Fp1.drop('Name', axis=1)
Fp2.drop('Name', axis=1)
Fp3.drop('Name', axis=1)
Fp4.drop('Name', axis=1)
Fp5.drop('Name', axis=1)
Fp6.drop('Name', axis=1)
Fp7.drop('Name', axis=1)
Fp8.drop('Name', axis=1)
Fp9.drop('Name', axis=1)
Fp10.drop('Name', axis=1)
Fp11.drop('Name', axis=1)
Fp12.drop('Name', axis=1)
135/72: Fp1
135/73:
Fpp1 = Fp1.drop('Name', axis=1)
Fp2.drop('Name', axis=1)
Fp3.drop('Name', axis=1)
Fp4.drop('Name', axis=1)
Fp5.drop('Name', axis=1)
Fp6.drop('Name', axis=1)
Fp7.drop('Name', axis=1)
Fp8.drop('Name', axis=1)
Fp9.drop('Name', axis=1)
Fp10.drop('Name', axis=1)
Fp11.drop('Name', axis=1)
Fp12.drop('Name', axis=1)
135/74: Fpp1
135/75: Fp1_n  = normalized (Fpp1 )
135/76: Fp1_n  = normalized (Fpp1)
135/77:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled)
    Fp_normalized
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    
    return Fp_normalized
135/78: Fp1_n  = normalized (Fpp1)
135/79: Fp1_n
135/80:
Fpp1 = Fp1.drop('Name', axis=1)
Fpp2 = Fp2.drop('Name', axis=1)
Fpp3 = Fp3.drop('Name', axis=1)
Fpp4 = Fp4.drop('Name', axis=1)
Fpp5 = Fp5.drop('Name', axis=1)
Fpp6 = Fp6.drop('Name', axis=1)
Fpp7 = Fp7.drop('Name', axis=1)
Fpp8 = Fp8.drop('Name', axis=1)
Fpp9 = Fp9.drop('Name', axis=1)
Fpp10 = Fp10.drop('Name', axis=1)
Fpp11 = Fp11.drop('Name', axis=1)
Fpp12 = Fp12.drop('Name', axis=1)
135/81:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled)
    Fp_normalized
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    
    return Fp_normalized
135/82:
Fp1_n  = normalized (Fpp1 )
Fp2_n  = normalized (Fpp2 )
Fp3_n  = normalized (Fpp3 )
Fp4_n  = normalized (Fpp4 )
Fp5_n  = normalized (Fpp5 )
Fp6_n  = normalized (Fpp6 )
Fp7_n  = normalized (Fpp7 )
Fp8_n  = normalized (Fpp8 )
Fp9_n  = normalized (Fpp9 )
Fp10_n = normalized (Fpp10)
Fp11_n = normalized (Fpp11)
Fp12_n = normalized (Fpp12)
135/83: Fp1_n
140/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
140/2:
path = r'Fingerprint/'

Fp1  = pd.read_csv(path + 'Fingerprinter.csv'               , header = 0)
Fp2  = pd.read_csv(path + 'ExtendedFingerprinter.csv'       , header = 0)
Fp3  = pd.read_csv(path + 'EStateFingerprinter.csv'         , header = 0)
Fp4  = pd.read_csv(path + 'GraphOnlyFingerprinter.csv'      , header = 0)
Fp5  = pd.read_csv(path + 'MACCSFingerprinter.csv'          , header = 0)
Fp6  = pd.read_csv(path + 'PubchemFingerprinter.csv'        , header = 0)
Fp7  = pd.read_csv(path + 'SubstructureFingerprinter.csv'   , header = 0)
Fp8  = pd.read_csv(path + 'SubstructureFingerprintCount.csv', header = 0)
Fp9  = pd.read_csv(path + 'KlekotaRothFingerprinter.csv'    , header = 0)
Fp10 = pd.read_csv(path + 'KlekotaRothFingerprintCount.csv' , header = 0)
Fp11 = pd.read_csv(path + 'AtomPairs2DFingerprinter.csv'    , header = 0)
Fp12 = pd.read_csv(path + 'AtomPairs2DFingerprintCount.csv' , header = 0)
140/3: Fp12
140/4:
df = pd.read_csv('df6_lipinski.csv')
df
140/5: name = df['compound']
140/6: name.shape
140/7:
Fp1.Name = name
Fp2.Name = name
Fp3.Name = name
Fp4.Name = name
Fp5.Name = name
Fp6.Name = name
Fp7.Name = name
Fp8.Name = name
Fp9.Name = name
Fp10.Name = name
Fp11.Name = name
Fp12.Name = name
140/8:
Fpp1 = Fp1.drop('Name', axis=1)
Fpp2 = Fp2.drop('Name', axis=1)
Fpp3 = Fp3.drop('Name', axis=1)
Fpp4 = Fp4.drop('Name', axis=1)
Fpp5 = Fp5.drop('Name', axis=1)
Fpp6 = Fp6.drop('Name', axis=1)
Fpp7 = Fp7.drop('Name', axis=1)
Fpp8 = Fp8.drop('Name', axis=1)
Fpp9 = Fp9.drop('Name', axis=1)
Fpp10 = Fp10.drop('Name', axis=1)
Fpp11 = Fp11.drop('Name', axis=1)
Fpp12 = Fp12.drop('Name', axis=1)
140/9:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled)
    Fp_normalized
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    
    return Fp_normalized
140/10:
Fp1_n  = normalized (Fpp1 )
Fp2_n  = normalized (Fpp2 )
Fp3_n  = normalized (Fpp3 )
Fp4_n  = normalized (Fpp4 )
Fp5_n  = normalized (Fpp5 )
Fp6_n  = normalized (Fpp6 )
Fp7_n  = normalized (Fpp7 )
Fp8_n  = normalized (Fpp8 )
Fp9_n  = normalized (Fpp9 )
Fp10_n = normalized (Fpp10)
Fp11_n = normalized (Fpp11)
Fp12_n = normalized (Fpp12)
140/11: Fp1_n
140/12:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled)
    Fp_normalized
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    
    return Fp_normalized
140/13:
raw1  = df.merge(Fp1_n , on='chemblId', how='outer')
raw2  = df.merge(Fp2_n , on='chemblId', how='outer')
raw3  = df.merge(Fp3_n , on='chemblId', how='outer')
raw4  = df.merge(Fp4_n , on='chemblId', how='outer')
raw5  = df.merge(Fp5_n , on='chemblId', how='outer')
raw6  = df.merge(Fp6_n , on='chemblId', how='outer')
raw7  = df.merge(Fp7_n , on='chemblId', how='outer')
raw8  = df.merge(Fp8_n , on='chemblId', how='outer')
raw9  = df.merge(Fp9_n , on='chemblId', how='outer')
raw10 = df.merge(Fp10_n, on='chemblId', how='outer')
raw11 = df.merge(Fp11_n, on='chemblId', how='outer')
raw12 = df.merge(Fp12_n, on='chemblId', how='outer')
140/14: raw1  = df.merge(Fp1_n , on='Name', how='outer')
140/15: raw1  = df.merge(Fp1_n , on='Name', how='outer')
140/16:
Fp1_n .to_csv('Train_Fp_normalized/Fingerprinter.csv'               , sep=',' ,index=False)
Fp2_n .to_csv('Train_Fp_normalized/ExtendedFingerprinter.csv'       , sep=',' ,index=False)
Fp3_n .to_csv('Train_Fp_normalized/EStateFingerprinter.csv'         , sep=',' ,index=False)
Fp4_n .to_csv('Train_Fp_normalized/GraphOnlyFingerprinter.csv'      , sep=',' ,index=False)
Fp5_n .to_csv('Train_Fp_normalized/MACCSFingerprinter.csv'          , sep=',' ,index=False)
Fp6_n .to_csv('Train_Fp_normalized/PubchemFingerprinter.csv'        , sep=',' ,index=False)
Fp7_n .to_csv('Train_Fp_normalized/SubstructureFingerprinter.csv'   , sep=',' ,index=False)
Fp8_n .to_csv('Train_Fp_normalized/SubstructureFingerprintCount.csv', sep=',' ,index=False)
Fp9_n .to_csv('Train_Fp_normalized/KlekotaRothFingerprinter.csv'    , sep=',' ,index=False)
Fp10_n.to_csv('Train_Fp_normalized/KlekotaRothFingerprintCount.csv' , sep=',' ,index=False)
Fp11_n.to_csv('Train_Fp_normalized/AtomPairs2DFingerprinter.csv'    , sep=',' ,index=False)
Fp12_n.to_csv('Train_Fp_normalized/AtomPairs2DFingerprintCount.csv' , sep=',' ,index=False)
140/17:
Fp1_n .to_csv('Fp_normalized/Fingerprinter.csv'               , sep=',' ,index=False)
Fp2_n .to_csv('Fp_normalized/ExtendedFingerprinter.csv'       , sep=',' ,index=False)
Fp3_n .to_csv('Fp_normalized/EStateFingerprinter.csv'         , sep=',' ,index=False)
Fp4_n .to_csv('Fp_normalized/GraphOnlyFingerprinter.csv'      , sep=',' ,index=False)
Fp5_n .to_csv('Fp_normalized/MACCSFingerprinter.csv'          , sep=',' ,index=False)
Fp6_n .to_csv('Fp_normalized/PubchemFingerprinter.csv'        , sep=',' ,index=False)
Fp7_n .to_csv('Fp_normalized/SubstructureFingerprinter.csv'   , sep=',' ,index=False)
Fp8_n .to_csv('Fp_normalized/SubstructureFingerprintCount.csv', sep=',' ,index=False)
Fp9_n .to_csv('Fp_normalized/KlekotaRothFingerprinter.csv'    , sep=',' ,index=False)
Fp10_n.to_csv('Fp_normalized/KlekotaRothFingerprintCount.csv' , sep=',' ,index=False)
Fp11_n.to_csv('Fp_normalized/AtomPairs2DFingerprinter.csv'    , sep=',' ,index=False)
Fp12_n.to_csv('Fp_normalized/AtomPairs2DFingerprintCount.csv' , sep=',' ,index=False)
140/18: raw1  = df.merge(Fp1_n , on='Name', how='outer')
140/19:
Fp1_n.loc[:, 'Name'] = Fp1.Name
Fp1_n
140/20:
Fp1_n.loc['Name', :] = Fp1.Name
Fp1_n
140/21:
Fp1_n.loc['Name', :] = Fp1.Name
Fp1_n
140/22:
Fp1_n.loc['Name', -1:] = Fp1.Name
Fp1_n
140/23:
Fp1_n.loc['Name', :-1] = Fp1.Name
Fp1_n
140/24:
Fp1_n.loc['Name', :-1] = Fp1.Name
Fp1_n
140/25:
Fp1_n.loc['Name', 1:] = Fp1.Name
Fp1_n
140/26:
Fp1_n.loc['Name', 1:] = Fp1.Name
Fp1_n
140/27:
Fp1_n.loc[1:, 'Name'] = Fp1.Name
Fp1_n
140/28:
Fp1_n.loc[1:, 'Name'] = Fp1.Name
Fp1_n
140/29:
Fp1_n.loc[:, 'Name'] = Fp1.Name
Fp1_n
140/30:
Fp1_n.loc[:, 'Name'] = Fp1.Name
Fp1_n.reset_index('Name', drop=True)
140/31:
Fp1_n.loc[:, 'Name'] = Fp1.Name
Fp1_n.reset_index(Name, drop=True)
140/32:
Fp1_n.loc[:, 'Name'] = Fp1.Name
Fp1_n.reset_index('Name', drop=True)
140/33:
Fp1_n.loc[:, 'Name'] = Fp1.Name
Fp1_n.set_index('Name', drop=True)
140/34:
Fp1_n.loc[:, 'Name'] = Fp1.Name
Fp1_n
140/35: Fp1_n
140/36: Fp1.shape
140/37: Fp1_n.shape
140/38: Fp1_n
140/39:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled)
    Fp_normalized

    
    return Fp_normalized
140/40:
Fp1_n  = normalized (Fpp1 )
Fp2_n  = normalized (Fpp2 )
Fp3_n  = normalized (Fpp3 )
Fp4_n  = normalized (Fpp4 )
Fp5_n  = normalized (Fpp5 )
Fp6_n  = normalized (Fpp6 )
Fp7_n  = normalized (Fpp7 )
Fp8_n  = normalized (Fpp8 )
Fp9_n  = normalized (Fpp9 )
Fp10_n = normalized (Fpp10)
Fp11_n = normalized (Fpp11)
Fp12_n = normalized (Fpp12)
140/41: Fp1_n
140/42:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
140/43:
Fp1_n  = normalized (Fpp1 )
Fp2_n  = normalized (Fpp2 )
Fp3_n  = normalized (Fpp3 )
Fp4_n  = normalized (Fpp4 )
Fp5_n  = normalized (Fpp5 )
Fp6_n  = normalized (Fpp6 )
Fp7_n  = normalized (Fpp7 )
Fp8_n  = normalized (Fpp8 )
Fp9_n  = normalized (Fpp9 )
Fp10_n = normalized (Fpp10)
Fp11_n = normalized (Fpp11)
Fp12_n = normalized (Fpp12)
140/44: Fp1_n
140/45: Fp1_n.loc['Name'] = Fp1.Name
140/46:
Fp1_n.loc['Name'] = Fp1.Name
Fp1_n
140/47:
Fp1_n.loc['Name': 1] = Fp1.Name
Fp1_n
140/48:
Fp1_n.["Name"] = Fp1.Name
Fp1_n
140/49:
Fp1_n["Name"] = Fp1.Name
Fp1_n
141/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
141/2:
path = r'Fingerprint/'

Fp1  = pd.read_csv(path + 'Fingerprinter.csv'               , header = 0)
Fp2  = pd.read_csv(path + 'ExtendedFingerprinter.csv'       , header = 0)
Fp3  = pd.read_csv(path + 'EStateFingerprinter.csv'         , header = 0)
Fp4  = pd.read_csv(path + 'GraphOnlyFingerprinter.csv'      , header = 0)
Fp5  = pd.read_csv(path + 'MACCSFingerprinter.csv'          , header = 0)
Fp6  = pd.read_csv(path + 'PubchemFingerprinter.csv'        , header = 0)
Fp7  = pd.read_csv(path + 'SubstructureFingerprinter.csv'   , header = 0)
Fp8  = pd.read_csv(path + 'SubstructureFingerprintCount.csv', header = 0)
Fp9  = pd.read_csv(path + 'KlekotaRothFingerprinter.csv'    , header = 0)
Fp10 = pd.read_csv(path + 'KlekotaRothFingerprintCount.csv' , header = 0)
Fp11 = pd.read_csv(path + 'AtomPairs2DFingerprinter.csv'    , header = 0)
Fp12 = pd.read_csv(path + 'AtomPairs2DFingerprintCount.csv' , header = 0)
141/3: Fp1.shape
141/4:
df = pd.read_csv('df6_lipinski.csv')
df
141/5: name = df['compound']
141/6: name.shape
141/7:
Fp1.Name = name
Fp2.Name = name
Fp3.Name = name
Fp4.Name = name
Fp5.Name = name
Fp6.Name = name
Fp7.Name = name
Fp8.Name = name
Fp9.Name = name
Fp10.Name = name
Fp11.Name = name
Fp12.Name = name
141/8:
Fpp1 = Fp1.drop('Name', axis=1)
Fpp2 = Fp2.drop('Name', axis=1)
Fpp3 = Fp3.drop('Name', axis=1)
Fpp4 = Fp4.drop('Name', axis=1)
Fpp5 = Fp5.drop('Name', axis=1)
Fpp6 = Fp6.drop('Name', axis=1)
Fpp7 = Fp7.drop('Name', axis=1)
Fpp8 = Fp8.drop('Name', axis=1)
Fpp9 = Fp9.drop('Name', axis=1)
Fpp10 = Fp10.drop('Name', axis=1)
Fpp11 = Fp11.drop('Name', axis=1)
Fpp12 = Fp12.drop('Name', axis=1)
141/9:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
141/10:
Fp1_n  = normalized (Fpp1 )
Fp2_n  = normalized (Fpp2 )
Fp3_n  = normalized (Fpp3 )
Fp4_n  = normalized (Fpp4 )
Fp5_n  = normalized (Fpp5 )
Fp6_n  = normalized (Fpp6 )
Fp7_n  = normalized (Fpp7 )
Fp8_n  = normalized (Fpp8 )
Fp9_n  = normalized (Fpp9 )
Fp10_n = normalized (Fpp10)
Fp11_n = normalized (Fpp11)
Fp12_n = normalized (Fpp12)
141/11: Fp1_n
141/12:
Fp1_n["Name"] = Fp1.Name
Fp1_n
141/13:
Fp1_n["Name"] = Fp1.Name
Fp1_n.set_index('Name', drop=True)
141/14:
Fp1_n["Name"] = Fp1.Name
Fp2_n["Name"] = Fp2.Name
Fp3_n["Name"] = Fp3.Name
Fp4_n["Name"] = Fp4.Name
Fp5_n["Name"] = Fp5.Name
Fp6_n["Name"] = Fp6.Name
Fp7_n["Name"] = Fp7.Name
Fp8_n["Name"] = Fp8.Name
Fp9_n["Name"] = Fp9.Name
Fp10_n["Name"] = Fp10.Name
Fp11_n["Name"] = Fp11.Name
141/15:
Fp1_n["Name"] = Fp1.Name
Fp2_n["Name"] = Fp2.Name
Fp3_n["Name"] = Fp3.Name
Fp4_n["Name"] = Fp4.Name
Fp5_n["Name"] = Fp5.Name
Fp6_n["Name"] = Fp6.Name
Fp7_n["Name"] = Fp7.Name
Fp8_n["Name"] = Fp8.Name
Fp9_n["Name"] = Fp9.Name
Fp10_n["Name"] = Fp10.Name
Fp11_n["Name"] = Fp11.Name
Fp12_n["Name"] = Fp12.Name
141/16:
Fp1_n.set_index('Name', drop=True)
Fp2_n.set_index('Name', drop=True)
Fp3_n.set_index('Name', drop=True)
Fp4_n.set_index('Name', drop=True)
Fp5_n.set_index('Name', drop=True)
Fp6_n.set_index('Name', drop=True)
Fp7_n.set_index('Name', drop=True)
Fp8_n.set_index('Name', drop=True)
Fp9_n.set_index('Name', drop=True)
Fp10_n.set_index('Name', drop=True)
Fp11_n.set_index('Name', drop=True)
Fp12_n.set_index('Name', drop=True)
141/17:
Fp1_n .to_csv('Fp_normalized/Fingerprinter.csv'               , sep=',' ,index=False)
Fp2_n .to_csv('Fp_normalized/ExtendedFingerprinter.csv'       , sep=',' ,index=False)
Fp3_n .to_csv('Fp_normalized/EStateFingerprinter.csv'         , sep=',' ,index=False)
Fp4_n .to_csv('Fp_normalized/GraphOnlyFingerprinter.csv'      , sep=',' ,index=False)
Fp5_n .to_csv('Fp_normalized/MACCSFingerprinter.csv'          , sep=',' ,index=False)
Fp6_n .to_csv('Fp_normalized/PubchemFingerprinter.csv'        , sep=',' ,index=False)
Fp7_n .to_csv('Fp_normalized/SubstructureFingerprinter.csv'   , sep=',' ,index=False)
Fp8_n .to_csv('Fp_normalized/SubstructureFingerprintCount.csv', sep=',' ,index=False)
Fp9_n .to_csv('Fp_normalized/KlekotaRothFingerprinter.csv'    , sep=',' ,index=False)
Fp10_n.to_csv('Fp_normalized/KlekotaRothFingerprintCount.csv' , sep=',' ,index=False)
Fp11_n.to_csv('Fp_normalized/AtomPairs2DFingerprinter.csv'    , sep=',' ,index=False)
Fp12_n.to_csv('Fp_normalized/AtomPairs2DFingerprintCount.csv' , sep=',' ,index=False)
141/18:
Fp1_n.set_index('Name', drop=True)
Fp2_n.set_index('Name', drop=True)
Fp3_n.set_index('Name', drop=True)
Fp4_n.set_index('Name', drop=True)
Fp5_n.set_index('Name', drop=True)
Fp6_n.set_index('Name', drop=True)
Fp7_n.set_index('Name', drop=True)
Fp8_n.set_index('Name', drop=True)
Fp9_n.set_index('Name', drop=True)
Fp10_n.set_index('Name', drop=True)
Fp11_n.set_index('Name', drop=True)
Fp12_n.set_index('Name', drop=True)
141/19: Fp12
141/20: Fp12_n
141/21: Fp1_n.set_index('Name', drop=True)
141/22: Fp2_n.set_index('Name', drop=True)
141/23:
Fp1_n.set_index('Name', drop=True),
Fp2_n.set_index('Name', drop=True)
Fp3_n.set_index('Name', drop=True)
Fp4_n.set_index('Name', drop=True)
Fp5_n.set_index('Name', drop=True)
Fp6_n.set_index('Name', drop=True)
Fp7_n.set_index('Name', drop=True)
Fp8_n.set_index('Name', drop=True)
Fp9_n.set_index('Name', drop=True)
Fp10_n.set_index('Name', drop=True)
Fp11_n.set_index('Name', drop=True)
Fp12_n.set_index('Name', drop=True)
141/24:
Fp1_n.set_index('Name', drop=True)
Fp2_n.set_index('Name', drop=True)
Fp3_n.set_index('Name', drop=True)
Fp4_n.set_index('Name', drop=True)
Fp5_n.set_index('Name', drop=True)
Fp6_n.set_index('Name', drop=True)
Fp7_n.set_index('Name', drop=True)
Fp8_n.set_index('Name', drop=True)
Fp9_n.set_index('Name', drop=True)
Fp10_n.set_index('Name', drop=True)
Fp11_n.set_index('Name', drop=True)
Fp12_n.set_index('Name', drop=True)
141/25: Fp12_n
141/26: Fp2_n
141/27: Fp1_n
141/28: Fp1_n.set_index('Name', drop=True)
141/29: Fp2_n.set_index('Name', drop=True)
141/30:
Fp2_n.set_index('Name', drop=True)
Fp3_n.set_index('Name', drop=True)
141/31:
Fp2_n.set_index('Name', drop=True)
Fp3_n.set_index('Name', drop=True)

Fp3_n
141/32: Fp2_n.set_index('Name', drop=True)
141/33: Fp3_n.set_index('Name', drop=True)
141/34: Fp1_n
141/35:
Fp1_n = Fp1_n.set_index('Name', drop=True)
Fp2_n = Fp2_n.set_index('Name', drop=True)
Fp3_n = Fp3_n.set_index('Name', drop=True)
Fp4_n = Fp4_n.set_index('Name', drop=True)
Fp5_n = Fp5_n.set_index('Name', drop=True)
Fp6_n = Fp6_n.set_index('Name', drop=True)
Fp7_n = Fp7_n.set_index('Name', drop=True)
Fp8_n = Fp8_n.set_index('Name', drop=True)
Fp9_n = Fp9_n.set_index('Name', drop=True)
Fp10_n = Fp10_n.set_index('Name', drop=True)
Fp11_n = Fp11_n.set_index('Name', drop=True)
Fp12_n = Fp12_n.set_index('Name', drop=True)
141/36:
Fp1_n .to_csv('Fp_normalized/Fingerprinter.csv'               , sep=',' ,index=False)
Fp2_n .to_csv('Fp_normalized/ExtendedFingerprinter.csv'       , sep=',' ,index=False)
Fp3_n .to_csv('Fp_normalized/EStateFingerprinter.csv'         , sep=',' ,index=False)
Fp4_n .to_csv('Fp_normalized/GraphOnlyFingerprinter.csv'      , sep=',' ,index=False)
Fp5_n .to_csv('Fp_normalized/MACCSFingerprinter.csv'          , sep=',' ,index=False)
Fp6_n .to_csv('Fp_normalized/PubchemFingerprinter.csv'        , sep=',' ,index=False)
Fp7_n .to_csv('Fp_normalized/SubstructureFingerprinter.csv'   , sep=',' ,index=False)
Fp8_n .to_csv('Fp_normalized/SubstructureFingerprintCount.csv', sep=',' ,index=False)
Fp9_n .to_csv('Fp_normalized/KlekotaRothFingerprinter.csv'    , sep=',' ,index=False)
Fp10_n.to_csv('Fp_normalized/KlekotaRothFingerprintCount.csv' , sep=',' ,index=False)
Fp11_n.to_csv('Fp_normalized/AtomPairs2DFingerprinter.csv'    , sep=',' ,index=False)
Fp12_n.to_csv('Fp_normalized/AtomPairs2DFingerprintCount.csv' , sep=',' ,index=False)
141/37:
Fp1_n = Fp1_n.set_index('Name', drop=True)
Fp2_n = Fp2_n.set_index('Name', drop=True)
Fp3_n = Fp3_n.set_index('Name', drop=True)
Fp4_n = Fp4_n.set_index('Name', drop=True)
Fp5_n = Fp5_n.set_index('Name', drop=True)
Fp6_n = Fp6_n.set_index('Name', drop=True)
Fp7_n = Fp7_n.set_index('Name', drop=True)
Fp8_n = Fp8_n.set_index('Name', drop=True)
Fp9_n = Fp9_n.set_index('Name', drop=True)
Fp10_n = Fp10_n.set_index('Name', drop=True)
Fp11_n = Fp11_n.set_index('Name', drop=True)
Fp12_n = Fp12_n.set_index('Name', drop=True)
141/38:
Fp1_n["Name"] = Fp1.Name
Fp2_n["Name"] = Fp2.Name
Fp3_n["Name"] = Fp3.Name
Fp4_n["Name"] = Fp4.Name
Fp5_n["Name"] = Fp5.Name
Fp6_n["Name"] = Fp6.Name
Fp7_n["Name"] = Fp7.Name
Fp8_n["Name"] = Fp8.Name
Fp9_n["Name"] = Fp9.Name
Fp10_n["Name"] = Fp10.Name
Fp11_n["Name"] = Fp11.Name
Fp12_n["Name"] = Fp12.Name
141/39:
Fp1_n = Fp1_n.set_index('Name', drop=True)
Fp2_n = Fp2_n.set_index('Name', drop=True)
Fp3_n = Fp3_n.set_index('Name', drop=True)
Fp4_n = Fp4_n.set_index('Name', drop=True)
Fp5_n = Fp5_n.set_index('Name', drop=True)
Fp6_n = Fp6_n.set_index('Name', drop=True)
Fp7_n = Fp7_n.set_index('Name', drop=True)
Fp8_n = Fp8_n.set_index('Name', drop=True)
Fp9_n = Fp9_n.set_index('Name', drop=True)
Fp10_n = Fp10_n.set_index('Name', drop=True)
Fp11_n = Fp11_n.set_index('Name', drop=True)
Fp12_n = Fp12_n.set_index('Name', drop=True)
141/40:
Fp1_n .to_csv('Fp_normalized/Fingerprinter.csv'               , sep=',' ,index=False)
Fp2_n .to_csv('Fp_normalized/ExtendedFingerprinter.csv'       , sep=',' ,index=False)
Fp3_n .to_csv('Fp_normalized/EStateFingerprinter.csv'         , sep=',' ,index=False)
Fp4_n .to_csv('Fp_normalized/GraphOnlyFingerprinter.csv'      , sep=',' ,index=False)
Fp5_n .to_csv('Fp_normalized/MACCSFingerprinter.csv'          , sep=',' ,index=False)
Fp6_n .to_csv('Fp_normalized/PubchemFingerprinter.csv'        , sep=',' ,index=False)
Fp7_n .to_csv('Fp_normalized/SubstructureFingerprinter.csv'   , sep=',' ,index=False)
Fp8_n .to_csv('Fp_normalized/SubstructureFingerprintCount.csv', sep=',' ,index=False)
Fp9_n .to_csv('Fp_normalized/KlekotaRothFingerprinter.csv'    , sep=',' ,index=False)
Fp10_n.to_csv('Fp_normalized/KlekotaRothFingerprintCount.csv' , sep=',' ,index=False)
Fp11_n.to_csv('Fp_normalized/AtomPairs2DFingerprinter.csv'    , sep=',' ,index=False)
Fp12_n.to_csv('Fp_normalized/AtomPairs2DFingerprintCount.csv' , sep=',' ,index=False)
141/41: Fp12_n
141/42:
path = r'Fingerprint/'

Fp1  = pd.read_csv(path + 'Fingerprinter.csv'               , header = 0)
Fp2  = pd.read_csv(path + 'ExtendedFingerprinter.csv'       , header = 0)
Fp3  = pd.read_csv(path + 'EStateFingerprinter.csv'         , header = 0)
Fp4  = pd.read_csv(path + 'GraphOnlyFingerprinter.csv'      , header = 0)
Fp5  = pd.read_csv(path + 'MACCSFingerprinter.csv'          , header = 0)
Fp6  = pd.read_csv(path + 'PubchemFingerprinter.csv'        , header = 0)
Fp7  = pd.read_csv(path + 'SubstructureFingerprinter.csv'   , header = 0)
Fp8  = pd.read_csv(path + 'SubstructureFingerprintCount.csv', header = 0)
Fp9  = pd.read_csv(path + 'KlekotaRothFingerprinter.csv'    , header = 0)
Fp10 = pd.read_csv(path + 'KlekotaRothFingerprintCount.csv' , header = 0)
Fp11 = pd.read_csv(path + 'AtomPairs2DFingerprinter.csv'    , header = 0)
Fp12 = pd.read_csv(path + 'AtomPairs2DFingerprintCount.csv' , header = 0)
141/43: Fp1.shape
141/44:
df = pd.read_csv('df6_lipinski.csv')
df
141/45: name = df['compound']
141/46: name.shape
141/47:
Fp1.Name = name
Fp2.Name = name
Fp3.Name = name
Fp4.Name = name
Fp5.Name = name
Fp6.Name = name
Fp7.Name = name
Fp8.Name = name
Fp9.Name = name
Fp10.Name = name
Fp11.Name = name
Fp12.Name = name
141/48: Fp12
141/49:
Fpp1 = Fp1.drop('Name', axis=1)
Fpp2 = Fp2.drop('Name', axis=1)
Fpp3 = Fp3.drop('Name', axis=1)
Fpp4 = Fp4.drop('Name', axis=1)
Fpp5 = Fp5.drop('Name', axis=1)
Fpp6 = Fp6.drop('Name', axis=1)
Fpp7 = Fp7.drop('Name', axis=1)
Fpp8 = Fp8.drop('Name', axis=1)
Fpp9 = Fp9.drop('Name', axis=1)
Fpp10 = Fp10.drop('Name', axis=1)
Fpp11 = Fp11.drop('Name', axis=1)
Fpp12 = Fp12.drop('Name', axis=1)
141/50:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
141/51:
Fp1_n  = normalized (Fpp1 )
Fp2_n  = normalized (Fpp2 )
Fp3_n  = normalized (Fpp3 )
Fp4_n  = normalized (Fpp4 )
Fp5_n  = normalized (Fpp5 )
Fp6_n  = normalized (Fpp6 )
Fp7_n  = normalized (Fpp7 )
Fp8_n  = normalized (Fpp8 )
Fp9_n  = normalized (Fpp9 )
Fp10_n = normalized (Fpp10)
Fp11_n = normalized (Fpp11)
Fp12_n = normalized (Fpp12)
141/52: Fp1_n
141/53:
Fp1_n["Name"] = Fp1.Name
Fp2_n["Name"] = Fp2.Name
Fp3_n["Name"] = Fp3.Name
Fp4_n["Name"] = Fp4.Name
Fp5_n["Name"] = Fp5.Name
Fp6_n["Name"] = Fp6.Name
Fp7_n["Name"] = Fp7.Name
Fp8_n["Name"] = Fp8.Name
Fp9_n["Name"] = Fp9.Name
Fp10_n["Name"] = Fp10.Name
Fp11_n["Name"] = Fp11.Name
Fp12_n["Name"] = Fp12.Name
141/54:
Fp1_n = Fp1_n.set_index('Name', drop=True)
Fp2_n = Fp2_n.set_index('Name', drop=True)
Fp3_n = Fp3_n.set_index('Name', drop=True)
Fp4_n = Fp4_n.set_index('Name', drop=True)
Fp5_n = Fp5_n.set_index('Name', drop=True)
Fp6_n = Fp6_n.set_index('Name', drop=True)
Fp7_n = Fp7_n.set_index('Name', drop=True)
Fp8_n = Fp8_n.set_index('Name', drop=True)
Fp9_n = Fp9_n.set_index('Name', drop=True)
Fp10_n = Fp10_n.set_index('Name', drop=True)
Fp11_n = Fp11_n.set_index('Name', drop=True)
Fp12_n = Fp12_n.set_index('Name', drop=True)
141/55:
Fp1_n .to_csv('Fp_normalized/Fingerprinter.csv'               , sep=',' ,index=False)
Fp2_n .to_csv('Fp_normalized/ExtendedFingerprinter.csv'       , sep=',' ,index=False)
Fp3_n .to_csv('Fp_normalized/EStateFingerprinter.csv'         , sep=',' ,index=False)
Fp4_n .to_csv('Fp_normalized/GraphOnlyFingerprinter.csv'      , sep=',' ,index=False)
Fp5_n .to_csv('Fp_normalized/MACCSFingerprinter.csv'          , sep=',' ,index=False)
Fp6_n .to_csv('Fp_normalized/PubchemFingerprinter.csv'        , sep=',' ,index=False)
Fp7_n .to_csv('Fp_normalized/SubstructureFingerprinter.csv'   , sep=',' ,index=False)
Fp8_n .to_csv('Fp_normalized/SubstructureFingerprintCount.csv', sep=',' ,index=False)
Fp9_n .to_csv('Fp_normalized/KlekotaRothFingerprinter.csv'    , sep=',' ,index=False)
Fp10_n.to_csv('Fp_normalized/KlekotaRothFingerprintCount.csv' , sep=',' ,index=False)
Fp11_n.to_csv('Fp_normalized/AtomPairs2DFingerprinter.csv'    , sep=',' ,index=False)
Fp12_n.to_csv('Fp_normalized/AtomPairs2DFingerprintCount.csv' , sep=',' ,index=False)
141/56: Fp12_n
141/57:
Fp1_n["Name"] = Fp1.Name
Fp2_n["Name"] = Fp2.Name
Fp3_n["Name"] = Fp3.Name
Fp4_n["Name"] = Fp4.Name
Fp5_n["Name"] = Fp5.Name
Fp6_n["Name"] = Fp6.Name
Fp7_n["Name"] = Fp7.Name
Fp8_n["Name"] = Fp8.Name
Fp9_n["Name"] = Fp9.Name
Fp10_n["Name"] = Fp10.Name
Fp11_n["Name"] = Fp11.Name
Fp12_n["Name"] = Fp12.Name
141/58: Fp12_n
141/59:
Fp1_n = Fp1_n.set_index('Name', drop=True)
Fp2_n = Fp2_n.set_index('Name', drop=True)
Fp3_n = Fp3_n.set_index('Name', drop=True)
Fp4_n = Fp4_n.set_index('Name', drop=True)
Fp5_n = Fp5_n.set_index('Name', drop=True)
Fp6_n = Fp6_n.set_index('Name', drop=True)
Fp7_n = Fp7_n.set_index('Name', drop=True)
Fp8_n = Fp8_n.set_index('Name', drop=True)
Fp9_n = Fp9_n.set_index('Name', drop=True)
Fp10_n = Fp10_n.set_index('Name', drop=True)
Fp11_n = Fp11_n.set_index('Name', drop=True)
Fp12_n = Fp12_n.set_index('Name', drop=True)
141/60: Fp12_n
141/61: Fp1_n
141/62:
Fp1_n["Name"] = Fp1.Name
Fp2_n["Name"] = Fp2.Name
Fp3_n["Name"] = Fp3.Name
Fp4_n["Name"] = Fp4.Name
Fp5_n["Name"] = Fp5.Name
Fp6_n["Name"] = Fp6.Name
Fp7_n["Name"] = Fp7.Name
Fp8_n["Name"] = Fp8.Name
Fp9_n["Name"] = Fp9.Name
Fp10_n["Name"] = Fp10.Name
Fp11_n["Name"] = Fp11.Name
Fp12_n["Name"] = Fp12.Name
141/63: Fp1_n
141/64:
path = r'Fingerprint/'

Fp1  = pd.read_csv(path + 'Fingerprinter.csv'               , header = 0)
Fp2  = pd.read_csv(path + 'ExtendedFingerprinter.csv'       , header = 0)
Fp3  = pd.read_csv(path + 'EStateFingerprinter.csv'         , header = 0)
Fp4  = pd.read_csv(path + 'GraphOnlyFingerprinter.csv'      , header = 0)
Fp5  = pd.read_csv(path + 'MACCSFingerprinter.csv'          , header = 0)
Fp6  = pd.read_csv(path + 'PubchemFingerprinter.csv'        , header = 0)
Fp7  = pd.read_csv(path + 'SubstructureFingerprinter.csv'   , header = 0)
Fp8  = pd.read_csv(path + 'SubstructureFingerprintCount.csv', header = 0)
Fp9  = pd.read_csv(path + 'KlekotaRothFingerprinter.csv'    , header = 0)
Fp10 = pd.read_csv(path + 'KlekotaRothFingerprintCount.csv' , header = 0)
Fp11 = pd.read_csv(path + 'AtomPairs2DFingerprinter.csv'    , header = 0)
Fp12 = pd.read_csv(path + 'AtomPairs2DFingerprintCount.csv' , header = 0)
141/65: Fp1.shape
141/66:
df = pd.read_csv('df6_lipinski.csv')
df
141/67: name = df['compound']
141/68: name.shape
141/69:
Fp1.Name = name
Fp2.Name = name
Fp3.Name = name
Fp4.Name = name
Fp5.Name = name
Fp6.Name = name
Fp7.Name = name
Fp8.Name = name
Fp9.Name = name
Fp10.Name = name
Fp11.Name = name
Fp12.Name = name
141/70:
Fpp1 = Fp1.drop('Name', axis=1)
Fpp2 = Fp2.drop('Name', axis=1)
Fpp3 = Fp3.drop('Name', axis=1)
Fpp4 = Fp4.drop('Name', axis=1)
Fpp5 = Fp5.drop('Name', axis=1)
Fpp6 = Fp6.drop('Name', axis=1)
Fpp7 = Fp7.drop('Name', axis=1)
Fpp8 = Fp8.drop('Name', axis=1)
Fpp9 = Fp9.drop('Name', axis=1)
Fpp10 = Fp10.drop('Name', axis=1)
Fpp11 = Fp11.drop('Name', axis=1)
Fpp12 = Fp12.drop('Name', axis=1)
141/71:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
141/72:
Fp1_n  = normalized (Fpp1 )
Fp2_n  = normalized (Fpp2 )
Fp3_n  = normalized (Fpp3 )
Fp4_n  = normalized (Fpp4 )
Fp5_n  = normalized (Fpp5 )
Fp6_n  = normalized (Fpp6 )
Fp7_n  = normalized (Fpp7 )
Fp8_n  = normalized (Fpp8 )
Fp9_n  = normalized (Fpp9 )
Fp10_n = normalized (Fpp10)
Fp11_n = normalized (Fpp11)
Fp12_n = normalized (Fpp12)
141/73: Fp1_n
141/74:
Fp1_n["Name"] = Fp1.Name
Fp2_n["Name"] = Fp2.Name
Fp3_n["Name"] = Fp3.Name
Fp4_n["Name"] = Fp4.Name
Fp5_n["Name"] = Fp5.Name
Fp6_n["Name"] = Fp6.Name
Fp7_n["Name"] = Fp7.Name
Fp8_n["Name"] = Fp8.Name
Fp9_n["Name"] = Fp9.Name
Fp10_n["Name"] = Fp10.Name
Fp11_n["Name"] = Fp11.Name
Fp12_n["Name"] = Fp12.Name
141/75: Fp1_n
141/76: Fp1_n = Fp1_n.set_index('Name')
141/77: Fp1_n
141/78:
Fp1_n = Fp1_n.set_index('Name')
Fp2_n = Fp2_n.set_index('Name')
Fp3_n = Fp3_n.set_index('Name')
Fp4_n = Fp4_n.set_index('Name')
Fp5_n = Fp5_n.set_index('Name')
Fp6_n = Fp6_n.set_index('Name')
Fp7_n = Fp7_n.set_index('Name')
Fp8_n = Fp8_n.set_index('Name')
Fp9_n = Fp9_n.set_index('Name')
Fp10_n = Fp10_n.set_index('Name')
Fp11_n = Fp11_n.set_index('Name')
Fp12_n = Fp12_n.set_index('Name')
141/79:
Fp1_n["Name"] = Fp1.Name
Fp2_n["Name"] = Fp2.Name
Fp3_n["Name"] = Fp3.Name
Fp4_n["Name"] = Fp4.Name
Fp5_n["Name"] = Fp5.Name
Fp6_n["Name"] = Fp6.Name
Fp7_n["Name"] = Fp7.Name
Fp8_n["Name"] = Fp8.Name
Fp9_n["Name"] = Fp9.Name
Fp10_n["Name"] = Fp10.Name
Fp11_n["Name"] = Fp11.Name
Fp12_n["Name"] = Fp12.Name
141/80: Fp1_n
141/81:
Fp1_n = Fp1_n.set_index('Name')
Fp2_n = Fp2_n.set_index('Name')
Fp3_n = Fp3_n.set_index('Name')
Fp4_n = Fp4_n.set_index('Name')
Fp5_n = Fp5_n.set_index('Name')
Fp6_n = Fp6_n.set_index('Name')
Fp7_n = Fp7_n.set_index('Name')
Fp8_n = Fp8_n.set_index('Name')
Fp9_n = Fp9_n.set_index('Name')
Fp10_n = Fp10_n.set_index('Name')
Fp11_n = Fp11_n.set_index('Name')
Fp12_n = Fp12_n.set_index('Name')
141/82: Fp1_n
141/83:
Fp1_n["Name"] = Fp1.Name
Fp2_n["Name"] = Fp2.Name
Fp3_n["Name"] = Fp3.Name
Fp4_n["Name"] = Fp4.Name
Fp5_n["Name"] = Fp5.Name
Fp6_n["Name"] = Fp6.Name
Fp7_n["Name"] = Fp7.Name
Fp8_n["Name"] = Fp8.Name
Fp9_n["Name"] = Fp9.Name
Fp10_n["Name"] = Fp10.Name
Fp11_n["Name"] = Fp11.Name
Fp12_n["Name"] = Fp12.Name
141/84:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
141/85:
path = r'Fingerprint/'

Fp1  = pd.read_csv(path + 'Fingerprinter.csv'               , header = 0)
Fp2  = pd.read_csv(path + 'ExtendedFingerprinter.csv'       , header = 0)
Fp3  = pd.read_csv(path + 'EStateFingerprinter.csv'         , header = 0)
Fp4  = pd.read_csv(path + 'GraphOnlyFingerprinter.csv'      , header = 0)
Fp5  = pd.read_csv(path + 'MACCSFingerprinter.csv'          , header = 0)
Fp6  = pd.read_csv(path + 'PubchemFingerprinter.csv'        , header = 0)
Fp7  = pd.read_csv(path + 'SubstructureFingerprinter.csv'   , header = 0)
Fp8  = pd.read_csv(path + 'SubstructureFingerprintCount.csv', header = 0)
Fp9  = pd.read_csv(path + 'KlekotaRothFingerprinter.csv'    , header = 0)
Fp10 = pd.read_csv(path + 'KlekotaRothFingerprintCount.csv' , header = 0)
Fp11 = pd.read_csv(path + 'AtomPairs2DFingerprinter.csv'    , header = 0)
Fp12 = pd.read_csv(path + 'AtomPairs2DFingerprintCount.csv' , header = 0)
141/86: Fp1.shape
141/87:
df = pd.read_csv('df6_lipinski.csv')
df
141/88: name = df['compound']
141/89:
path = r'Fingerprint/'

Fp1  = pd.read_csv(path + 'Fingerprinter.csv'               , header = 0)
Fp2  = pd.read_csv(path + 'ExtendedFingerprinter.csv'       , header = 0)
Fp3  = pd.read_csv(path + 'EStateFingerprinter.csv'         , header = 0)
Fp4  = pd.read_csv(path + 'GraphOnlyFingerprinter.csv'      , header = 0)
Fp5  = pd.read_csv(path + 'MACCSFingerprinter.csv'          , header = 0)
Fp6  = pd.read_csv(path + 'PubchemFingerprinter.csv'        , header = 0)
Fp7  = pd.read_csv(path + 'SubstructureFingerprinter.csv'   , header = 0)
Fp8  = pd.read_csv(path + 'SubstructureFingerprintCount.csv', header = 0)
Fp9  = pd.read_csv(path + 'KlekotaRothFingerprinter.csv'    , header = 0)
Fp10 = pd.read_csv(path + 'KlekotaRothFingerprintCount.csv' , header = 0)
Fp11 = pd.read_csv(path + 'AtomPairs2DFingerprinter.csv'    , header = 0)
Fp12 = pd.read_csv(path + 'AtomPairs2DFingerprintCount.csv' , header = 0)
141/90: Fp1.shape
141/91:
df = pd.read_csv('df6_lipinski.csv')
df
141/92: name = df['compound']
141/93: name.shape
141/94:
Fp1.Name = name
Fp2.Name = name
Fp3.Name = name
Fp4.Name = name
Fp5.Name = name
Fp6.Name = name
Fp7.Name = name
Fp8.Name = name
Fp9.Name = name
Fp10.Name = name
Fp11.Name = name
Fp12.Name = name
141/95:
Fpp1 = Fp1.drop('Name', axis=1)
Fpp2 = Fp2.drop('Name', axis=1)
Fpp3 = Fp3.drop('Name', axis=1)
Fpp4 = Fp4.drop('Name', axis=1)
Fpp5 = Fp5.drop('Name', axis=1)
Fpp6 = Fp6.drop('Name', axis=1)
Fpp7 = Fp7.drop('Name', axis=1)
Fpp8 = Fp8.drop('Name', axis=1)
Fpp9 = Fp9.drop('Name', axis=1)
Fpp10 = Fp10.drop('Name', axis=1)
Fpp11 = Fp11.drop('Name', axis=1)
Fpp12 = Fp12.drop('Name', axis=1)
141/96:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
141/97:
Fp1_n  = normalized (Fpp1 )
Fp2_n  = normalized (Fpp2 )
Fp3_n  = normalized (Fpp3 )
Fp4_n  = normalized (Fpp4 )
Fp5_n  = normalized (Fpp5 )
Fp6_n  = normalized (Fpp6 )
Fp7_n  = normalized (Fpp7 )
Fp8_n  = normalized (Fpp8 )
Fp9_n  = normalized (Fpp9 )
Fp10_n = normalized (Fpp10)
Fp11_n = normalized (Fpp11)
Fp12_n = normalized (Fpp12)
141/98: Fp1_n
141/99:
Fp1_n["Name"] = Fp1.Name
Fp2_n["Name"] = Fp2.Name
Fp3_n["Name"] = Fp3.Name
Fp4_n["Name"] = Fp4.Name
Fp5_n["Name"] = Fp5.Name
Fp6_n["Name"] = Fp6.Name
Fp7_n["Name"] = Fp7.Name
Fp8_n["Name"] = Fp8.Name
Fp9_n["Name"] = Fp9.Name
Fp10_n["Name"] = Fp10.Name
Fp11_n["Name"] = Fp11.Name
Fp12_n["Name"] = Fp12.Name
141/100: Fp1_n
141/101:
Fp1_n = Fp1_n.set_index('Name')
Fp2_n = Fp2_n.set_index('Name')
Fp3_n = Fp3_n.set_index('Name')
Fp4_n = Fp4_n.set_index('Name')
Fp5_n = Fp5_n.set_index('Name')
Fp6_n = Fp6_n.set_index('Name')
Fp7_n = Fp7_n.set_index('Name')
Fp8_n = Fp8_n.set_index('Name')
Fp9_n = Fp9_n.set_index('Name')
Fp10_n = Fp10_n.set_index('Name')
Fp11_n = Fp11_n.set_index('Name')
Fp12_n = Fp12_n.set_index('Name')
141/102: Fp1_n
141/103:
Fp1_n .to_csv('Fp_normalized/Fingerprinter.csv'               , sep=',' ,index=False)
Fp2_n .to_csv('Fp_normalized/ExtendedFingerprinter.csv'       , sep=',' ,index=False)
Fp3_n .to_csv('Fp_normalized/EStateFingerprinter.csv'         , sep=',' ,index=False)
Fp4_n .to_csv('Fp_normalized/GraphOnlyFingerprinter.csv'      , sep=',' ,index=False)
Fp5_n .to_csv('Fp_normalized/MACCSFingerprinter.csv'          , sep=',' ,index=False)
Fp6_n .to_csv('Fp_normalized/PubchemFingerprinter.csv'        , sep=',' ,index=False)
Fp7_n .to_csv('Fp_normalized/SubstructureFingerprinter.csv'   , sep=',' ,index=False)
Fp8_n .to_csv('Fp_normalized/SubstructureFingerprintCount.csv', sep=',' ,index=False)
Fp9_n .to_csv('Fp_normalized/KlekotaRothFingerprinter.csv'    , sep=',' ,index=False)
Fp10_n.to_csv('Fp_normalized/KlekotaRothFingerprintCount.csv' , sep=',' ,index=False)
Fp11_n.to_csv('Fp_normalized/AtomPairs2DFingerprinter.csv'    , sep=',' ,index=False)
Fp12_n.to_csv('Fp_normalized/AtomPairs2DFingerprintCount.csv' , sep=',' ,index=False)
141/104: Fp1_n
141/105: Fp2_n
141/106: raw1  = df.merge(Fp1_n , on='Name', how='outer')
141/107: df.Name = name
141/108: df.Name = Name
141/109: df.Name = name
141/110:
df.Name = name
df
141/111:
df2 = df.rename({'compound': 'Name'}, axis=1)
df2
141/112:
df2 = df.rename({'compound': 'Name'}, axis=1)
df2.drop('Unnamed: 0')
141/113:
df2 = df.rename({'compound': 'Name'}, axis=1)
df2.drop('Unnamed: 0', axis=1)
141/114:
df2 = df.rename({'compound': 'Name'}, axis=1)
df2 = df2.drop('Unnamed: 0', axis=1)
df3 = df2.set_index('Name')
141/115:
df2 = df.rename({'compound': 'Name'}, axis=1)
df2 = df2.drop('Unnamed: 0', axis=1)
df3 = df2.set_index('Name')
df3
141/116: raw1  = df.merge(Fp1_n , on='Name', how='outer')
141/117: raw1  = df.merge(Fp1_n, df3 , on='Name', how='outer')
141/118: raw1
141/119: raw1  = df.merge(Fp1_n, df3 , on='Name', how='outer')
141/120: raw1
141/121: raw1  = df3.merge(Fp1_n, on='Name', how='outer')
141/122: raw1
141/123: df4 = df3['LogKp']
141/124: LogKp = df3['LogKp']
141/125:
LogKp = df3['LogKp']
LogKp
141/126: raw1  = LogKp.merge(Fp1_n, on='Name', how='outer')
141/127:
LogKp = Datafram.df3['LogKp']
LogKp
141/128:
LogKp = df3['LogKp'].Dataframe
LogKp
141/129:
LogKp = df3['LogKp']
LogKp = LogKp.to_frame
141/130:
LogKp = df3['LogKp']
LogKp = LogKp.to_frame
LogKp
141/131:
LogKp = df3['LogKp']
LogKp = LogKp.to_frame()
LogKp
141/132: raw1  = LogKp.merge(Fp1_n, on='Name', how='outer')
141/133:
raw1  = LogKp.merge(Fp1_n, on='Name', how='outer')
raw2  = LogKp.merge(Fp2_n, on='Name', how='outer')
raw3  = LogKp.merge(Fp3_n, on='Name', how='outer')
raw4  = LogKp.merge(Fp4_n, on='Name', how='outer')
raw5  = LogKp.merge(Fp5_n, on='Name', how='outer')
raw6  = LogKp.merge(Fp6_n, on='Name', how='outer')
raw7  = LogKp.merge(Fp7_n, on='Name', how='outer')
raw8  = LogKp.merge(Fp8_n, on='Name', how='outer')
raw9  = LogKp.merge(Fp9_n, on='Name', how='outer')
raw10  = LogKp.merge(Fp10_n, on='Name', how='outer')
raw11  = LogKp.merge(Fp11_n, on='Name', how='outer')
raw12  = LogKp.merge(Fp12_n, on='Name', how='outer')
141/134: raw1
141/135:
raw1 .to_csv('QSAR/Kp_Fingerprinter.csv'               , sep=',' ,index=True)
raw2 .to_csv('QSAR/Kp_ExtendedFingerprinter.csv'       , sep=',' ,index=True)
raw3 .to_csv('QSAR/Kp_EStateFingerprinter.csv'         , sep=',' ,index=True)
raw4 .to_csv('QSAR/Kp_GraphOnlyFingerprinter.csv'      , sep=',' ,index=True)
raw5 .to_csv('QSAR/Kp_MACCSFingerprinter.csv'          , sep=',' ,index=True)
raw6 .to_csv('QSAR/Kp_PubchemFingerprinter.csv'        , sep=',' ,index=True)
raw7 .to_csv('QSAR/Kp_SubstructureFingerprinter.csv'   , sep=',' ,index=True)
raw8 .to_csv('QSAR/Kp_SubstructureFingerprintCount.csv', sep=',' ,index=True)
raw9 .to_csv('QSAR/Kp_KlekotaRothFingerprinter.csv'    , sep=',' ,index=True)
raw10.to_csv('QSAR/Kp_KlekotaRothFingerprintCount.csv' , sep=',' ,index=True)
raw11.to_csv('QSAR/Kp_AtomPairs2DFingerprinter.csv'    , sep=',' ,index=True)
raw12.to_csv('QSAR/Kp_AtomPairs2DFingerprintCount.csv' , sep=',' ,index=True)
141/136:
print (len(raw1 ),len(raw2 ),len(raw3 ),len(raw4 ),len(raw5 )
      ,len(raw6 ),len(raw7 ),len(raw8 ),len(raw9 ),len(raw10)
      ,len(raw11),len(raw12))
141/137:
print (len(raw1 .columns),len(raw2 .columns),len(raw3 .columns),len(raw4 .columns),len(raw5 )
      ,len(raw6 .columns),len(raw7 .columns),len(raw8 .columns),len(raw9 .columns),len(raw10)
      ,len(raw11.columns),len(raw12.columns))
141/138:
import numpy as np

def Remove_useless_descriptor(df, threshold):
    
    des1 = len(df.columns) 
    
    h = df.columns.tolist()
    df = df.as_matrix().astype(np.float)
    df = np.array(df)

    STDEV = np.std(df, axis=0)
    idx = [idx for idx, val in enumerate(STDEV) if val > threshold]
    df2 = df[:,idx]
    hx = np.array(h)[idx]
    
    df = pd.DataFrame(df2, columns=[hx])
    
    des2 = len(df.columns)
    
    print 'from Remove useless descriptor'
    print "The initial set of " + str(des1) + \
          " descriptors has been reduced to " + str(des2) + " descriptors."
    
    return df, des1, des2
141/139:
import numpy as np

def Remove_useless_descriptor(df, threshold):
    
    des1 = len(df.columns) 
    
    h = df.columns.tolist()
    df = df.as_matrix().astype(np.float)
    df = np.array(df)

    STDEV = np.std(df, axis=0)
    idx = [idx for idx, val in enumerate(STDEV) if val > threshold]
    df2 = df[:,idx]
    hx = np.array(h)[idx]
    
    df = pd.DataFrame(df2, columns=[hx])
    
    des2 = len(df.columns)
    
    print('from Remove useless descriptor')
    print("The initial set of " + str(des1) + \
          " descriptors has been reduced to " + str(des2) + " descriptors.")
    
    return df, des1, des2
141/140:
from scipy import stats

def correlation(df, threshold):

    des3 = len(df.columns) 
    corr = stats.pearsonr
    col_corr = set() # Set of all the names of deleted columns
    corr_matrix = df.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if corr_matrix.iloc[i, j] >= threshold:
                colname = corr_matrix.columns[i] # getting the name of column
                col_corr.add(colname)
                if colname in df.columns:
                    del df[colname] # deleting the column from the dataset
    des4 = len(df.columns) 

    print 'from Remove correlation'
    print ("The initial set of " + str(des3) + ' descriptors'+ 
           " has been reduced to " + str(des4) + " descriptors.")

    return df, des3, des4
141/141:
from scipy import stats

def correlation(df, threshold):

    des3 = len(df.columns) 
    corr = stats.pearsonr
    col_corr = set() # Set of all the names of deleted columns
    corr_matrix = df.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if corr_matrix.iloc[i, j] >= threshold:
                colname = corr_matrix.columns[i] # getting the name of column
                col_corr.add(colname)
                if colname in df.columns:
                    del df[colname] # deleting the column from the dataset
    des4 = len(df.columns) 

    print ('from Remove correlation')
    print ("The initial set of " + str(des3) + ' descriptors'+ 
           " has been reduced to " + str(des4) + " descriptors.")

    return df, des3, des4
141/142:
from sklearn.cross_validation import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.cross_validation import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from sklearn import cross_validation
from collections import defaultdict

def build_model(X, Y, seed, hx, f):
    
    
    #Data split using 70/30 ratio
    X_internal, X_external, Y_internal, Y_external = train_test_split(X,
                                                        Y, test_size=0.3,
                                                        random_state=seed)

    # Training set
    rf = RandomForestRegressor(n_estimators=400, 
                               max_features='sqrt', 
                               min_samples_leaf = 1,
                               random_state = 13,
                               n_jobs=-1)
    
    rf.fit(X_internal,Y_internal)
    prediction = rf.predict(X_internal)
    
    # Cross-validation
    cv = cross_validation.cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)
    
    # External set  
    prediction_external = rf.predict(X_external)
    
    #print result from each seed    
    R2_train.append(r2_score(Y_internal, prediction))
    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))
    Q2_CV.append(r2_score(Y_internal, cv))
    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))
    Q2_External.append(r2_score(Y_external, prediction_external))
    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))
    
    #Feature Importance
    Feature = hx[:]
    feature_importance = rf.feature_importances_
    importances = 100.0 * (feature_importance / feature_importance.max()) #index
    
    for i, fx in enumerate(Feature):
        importances_dict[fx].append(importances[i])
    
    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict
141/143:
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.cross_validation import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from sklearn import cross_validation
from collections import defaultdict

def build_model(X, Y, seed, hx, f):
    
    
    #Data split using 70/30 ratio
    X_internal, X_external, Y_internal, Y_external = train_test_split(X,
                                                        Y, test_size=0.3,
                                                        random_state=seed)

    # Training set
    rf = RandomForestRegressor(n_estimators=400, 
                               max_features='sqrt', 
                               min_samples_leaf = 1,
                               random_state = 13,
                               n_jobs=-1)
    
    rf.fit(X_internal,Y_internal)
    prediction = rf.predict(X_internal)
    
    # Cross-validation
    cv = cross_validation.cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)
    
    # External set  
    prediction_external = rf.predict(X_external)
    
    #print result from each seed    
    R2_train.append(r2_score(Y_internal, prediction))
    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))
    Q2_CV.append(r2_score(Y_internal, cv))
    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))
    Q2_External.append(r2_score(Y_external, prediction_external))
    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))
    
    #Feature Importance
    Feature = hx[:]
    feature_importance = rf.feature_importances_
    importances = 100.0 * (feature_importance / feature_importance.max()) #index
    
    for i, fx in enumerate(Feature):
        importances_dict[fx].append(importances[i])
    
    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict
141/144:
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from sklearn import cross_validation
from collections import defaultdict

def build_model(X, Y, seed, hx, f):
    
    
    #Data split using 70/30 ratio
    X_internal, X_external, Y_internal, Y_external = train_test_split(X,
                                                        Y, test_size=0.3,
                                                        random_state=seed)

    # Training set
    rf = RandomForestRegressor(n_estimators=400, 
                               max_features='sqrt', 
                               min_samples_leaf = 1,
                               random_state = 13,
                               n_jobs=-1)
    
    rf.fit(X_internal,Y_internal)
    prediction = rf.predict(X_internal)
    
    # Cross-validation
    cv = cross_validation.cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)
    
    # External set  
    prediction_external = rf.predict(X_external)
    
    #print result from each seed    
    R2_train.append(r2_score(Y_internal, prediction))
    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))
    Q2_CV.append(r2_score(Y_internal, cv))
    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))
    Q2_External.append(r2_score(Y_external, prediction_external))
    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))
    
    #Feature Importance
    Feature = hx[:]
    feature_importance = rf.feature_importances_
    importances = 100.0 * (feature_importance / feature_importance.max()) #index
    
    for i, fx in enumerate(Feature):
        importances_dict[fx].append(importances[i])
    
    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict
141/145:
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from sklearn import cross_validate, cross_val_predict
from collections import defaultdict

def build_model(X, Y, seed, hx, f):
    
    
    #Data split using 70/30 ratio
    X_internal, X_external, Y_internal, Y_external = train_test_split(X,
                                                        Y, test_size=0.3,
                                                        random_state=seed)

    # Training set
    rf = RandomForestRegressor(n_estimators=400, 
                               max_features='sqrt', 
                               min_samples_leaf = 1,
                               random_state = 13,
                               n_jobs=-1)
    
    rf.fit(X_internal,Y_internal)
    prediction = rf.predict(X_internal)
    
    # Cross-validation
    cv = cross_validation.cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)
    
    # External set  
    prediction_external = rf.predict(X_external)
    
    #print result from each seed    
    R2_train.append(r2_score(Y_internal, prediction))
    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))
    Q2_CV.append(r2_score(Y_internal, cv))
    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))
    Q2_External.append(r2_score(Y_external, prediction_external))
    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))
    
    #Feature Importance
    Feature = hx[:]
    feature_importance = rf.feature_importances_
    importances = 100.0 * (feature_importance / feature_importance.max()) #index
    
    for i, fx in enumerate(Feature):
        importances_dict[fx].append(importances[i])
    
    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict
141/146:
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from sklearn import cross_val_predict, cross_validate
from collections import defaultdict

def build_model(X, Y, seed, hx, f):
    
    
    #Data split using 70/30 ratio
    X_internal, X_external, Y_internal, Y_external = train_test_split(X,
                                                        Y, test_size=0.3,
                                                        random_state=seed)

    # Training set
    rf = RandomForestRegressor(n_estimators=400, 
                               max_features='sqrt', 
                               min_samples_leaf = 1,
                               random_state = 13,
                               n_jobs=-1)
    
    rf.fit(X_internal,Y_internal)
    prediction = rf.predict(X_internal)
    
    # Cross-validation
    cv = cross_validation.cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)
    
    # External set  
    prediction_external = rf.predict(X_external)
    
    #print result from each seed    
    R2_train.append(r2_score(Y_internal, prediction))
    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))
    Q2_CV.append(r2_score(Y_internal, cv))
    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))
    Q2_External.append(r2_score(Y_external, prediction_external))
    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))
    
    #Feature Importance
    Feature = hx[:]
    feature_importance = rf.feature_importances_
    importances = 100.0 * (feature_importance / feature_importance.max()) #index
    
    for i, fx in enumerate(Feature):
        importances_dict[fx].append(importances[i])
    
    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict
141/147:
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict

def build_model(X, Y, seed, hx, f):
    
    
    #Data split using 70/30 ratio
    X_internal, X_external, Y_internal, Y_external = train_test_split(X,
                                                        Y, test_size=0.3,
                                                        random_state=seed)

    # Training set
    rf = RandomForestRegressor(n_estimators=400, 
                               max_features='sqrt', 
                               min_samples_leaf = 1,
                               random_state = 13,
                               n_jobs=-1)
    
    rf.fit(X_internal,Y_internal)
    prediction = rf.predict(X_internal)
    
    # Cross-validation
    cv = cross_validation.cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)
    
    # External set  
    prediction_external = rf.predict(X_external)
    
    #print result from each seed    
    R2_train.append(r2_score(Y_internal, prediction))
    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))
    Q2_CV.append(r2_score(Y_internal, cv))
    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))
    Q2_External.append(r2_score(Y_external, prediction_external))
    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))
    
    #Feature Importance
    Feature = hx[:]
    feature_importance = rf.feature_importances_
    importances = 100.0 * (feature_importance / feature_importance.max()) #index
    
    for i, fx in enumerate(Feature):
        importances_dict[fx].append(importances[i])
    
    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict
141/148:
from copy import deepcopy

def Y_scrambling(X_internal, X_external, Y_internal, Y_external):
    # Do the Y-scrambling. Loop over the actual learning for 100 times.
    for randomseedcounter in range(1,101):
        y_train_scrambled = deepcopy(Y_internal)
        X_train_scrambled = deepcopy(X_internal)
        np.random.shuffle(y_train_scrambled)
        np.random.shuffle(X_train_scrambled)

        # training was done on "scrambled" data - prediction on test set
        RF_scrambled         = RandomForestRegressor()
        RF_scrambled         = RF_scrambled.fit(X_internal,y_train_scrambled)
        y_predict_scrambled  = RF_scrambled.predict(X_external)
    
        acclist_predictionOnTest_scrambledtrain.append((RF_scrambled.score(X_external,Y_external))**2)
    
        # training was done on "scrambled" data - prediction on train set
        y_predict_scrambled_predictTrain  = RF_scrambled.predict(X_internal)
    
        acclist_predictionOnTrain_scrambledtrain.append((RF_scrambled.score(X_internal,Y_internal))**2)
        
        r2 = pd.DataFrame(acclist_predictionOnTrain_scrambledtrain, columns=['R2'])
        q2 = pd.DataFrame(acclist_predictionOnTest_scrambledtrain, columns=['Q2'])

        #result = pd.concat([r2, q2], axis=1, join='inner').to_csv(f+"_Y_scrambling.csv", header=False, index=False)
        
        
    return acclist_predictionOnTest_scrambledtrain,acclist_predictionOnTrain_scrambledtrain
141/149:
def mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, importances_dict):
    R2_train_mean = np.mean(R2_train)
    RMSE_train_mean = np.mean(RMSE_train)
    Q2_CV_mean = np.mean(Q2_CV)
    RMSE_CV_mean = np.mean(RMSE_CV)
    Q2_External_mean = np.mean(Q2_External)
    RMSE_External_mean = np.mean(RMSE_External)
    importances_mean0 = {}
    for fx in importances_dict:
        importances_mean0[fx] = np.mean(importances_dict[fx])
    importances_mean = sorted([(k,v) for k,v in importances_mean0.iteritems()],
                                    key=lambda x: x[1], reverse=True)
    
    #predictionOnTest_mean = np.mean(acclist_predictionOnTest_scrambledtrain)
    #predictionOnTrain_mean = np.mean(acclist_predictionOnTrain_scrambledtrain)
    
    return R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \
            RMSE_External_mean, importances_mean
141/150:
def std(R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, importances_dict):
    R2_train_std = np.std(R2_train)
    RMSE_train_std = np.std(RMSE_train)
    Q2_CV_std = np.std(Q2_CV)
    RMSE_CV_std = np.std(RMSE_CV)
    Q2_External_std = np.std(Q2_External)
    RMSE_External_std = np.std(RMSE_External)
    importances_std0 = {}
    for fx in importances_dict:
        importances_std0[fx] = np.std(importances_dict[fx])
    importances_std = sorted([(k,v) for k,v in importances_std0.iteritems()],
                                    key=lambda x: x[1], reverse=True)
    #predictionOnTest_std = np.std(acclist_predictionOnTest_scrambledtrain)
    #predictionOnTrain_std = np.std(acclist_predictionOnTrain_scrambledtrain)
    
    return R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\
            RMSE_External_std, importances_std
141/151:
def print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, 
           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, 
           RMSE_External_std, X_internal, X_external, X, f, des1, des2 ):
    #outfile = open('all_output.csv', 'a')
    name = [os.path.basename(f)]
    print_out = name
    print_out = [name.replace('.smi','') for name in print_out]
    
    print >> outfile, '%s,%d,%d,%d,%d,%0.4f,%0.4f,%0.4f,%0.4f,%d,%0.4f,%0.4f,%0.4f,%0.4f,%d,%0.4f,%0.4f,%0.4f,%0.4f' \
                                     % (print_out, len(X_internal),
                                        des1, des2, des4,
                                        R2_train_mean, R2_train_std,
                                        RMSE_train_mean, RMSE_train_std,
                                        len(X_internal),
                                        Q2_CV_mean, Q2_CV_std,
                                        RMSE_CV_mean, RMSE_CV_std,
                                        len(X_external),
                                        Q2_External_mean, Q2_External_std,
                                        RMSE_External_mean, RMSE_External_std,)
 
    print ('\nTraining set\n------------')
    print ('N: ' + (str(len(X_internal))))
    print ('R2: %0.4f'%(R2_train_mean))
    print ('std_R2: %0.4f'%(R2_train_std))
    print ('RMSE: %0.4f'%(RMSE_train_mean))
    print ('std_RMSE: %0.4f'%(RMSE_train_std))

    print ('\nCross-validation set\n------------')
    print 'N: ' + (str(len(X_internal)))
    print 'Q2: %0.4f'%(Q2_CV_mean)
    print 'std_Q2: %0.4f'%(Q2_CV_std)
    print 'RMSE: %0.4f'%(RMSE_CV_mean)
    print 'std_RMSE: %0.4f'%(RMSE_CV_std)

    print '\nExternal set\n------------'
    print 'N: ' + (str(len(X_external)))
    print 'Q2_EXt: %0.4f'%(Q2_External_mean)
    print 'std_Q2_EXt: %0.4f'%(Q2_External_std)
    print 'RMSE: %0.4f'%(RMSE_External_mean)
    print 'std_RMSE: %0.4f'%(RMSE_External_std)
141/152:
def print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, 
           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, 
           RMSE_External_std, X_internal, X_external, X, f, des1, des2 ):
    #outfile = open('all_output.csv', 'a')
    name = [os.path.basename(f)]
    print_out = name
    print_out = [name.replace('.smi','') for name in print_out]
    
    print >> outfile, '%s,%d,%d,%d,%d,%0.4f,%0.4f,%0.4f,%0.4f,%d,%0.4f,%0.4f,%0.4f,%0.4f,%d,%0.4f,%0.4f,%0.4f,%0.4f' \
                                     % (print_out, len(X_internal),
                                        des1, des2, des4,
                                        R2_train_mean, R2_train_std,
                                        RMSE_train_mean, RMSE_train_std,
                                        len(X_internal),
                                        Q2_CV_mean, Q2_CV_std,
                                        RMSE_CV_mean, RMSE_CV_std,
                                        len(X_external),
                                        Q2_External_mean, Q2_External_std,
                                        RMSE_External_mean, RMSE_External_std,)
 
    print ('\nTraining set\n------------')
    print ('N: ' + (str(len(X_internal))))
    print ('R2: %0.4f'%(R2_train_mean))
    print ('std_R2: %0.4f'%(R2_train_std))
    print ('RMSE: %0.4f'%(RMSE_train_mean))
    print ('std_RMSE: %0.4f'%(RMSE_train_std))

    print ('\nCross-validation set\n------------')
    print ('N: ' + (str(len(X_internal))))
    print ('Q2: %0.4f'%(Q2_CV_mean))
    print ('std_Q2: %0.4f'%(Q2_CV_std))
    print ('RMSE: %0.4f'%(RMSE_CV_mean))
    print ('std_RMSE: %0.4f'%(RMSE_CV_std))

    print ('\nExternal set\n------------')
    print ('N: ' + (str(len(X_external))))
    print ('Q2_EXt: %0.4f'%(Q2_External_mean))
    print ('std_Q2_EXt: %0.4f'%(Q2_External_std))
    print ('RMSE: %0.4f'%(RMSE_External_mean))
    print ('std_RMSE: %0.4f'%(RMSE_External_std))
141/153:
import matplotlib.pyplot as plt
from matplotlib.ticker import MultipleLocator
import pylab as py 
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import summary_table

def plot_model (f, X_internal, X_external, Y_internal, Y_external,
                R2_train_mean, Q2_External_mean,
                importances_mean, importances_std, Feature, prediction, 
                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain):
    
    # Prepare plot
    m  = rf.fit(X_internal,Y_internal)
    cm = plt.cm.RdBu
    cv = cross_validation.cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)

    fig_size = plt.rcParams["figure.figsize"]
    fig_size[0]=5
    fig_size[1]=5

    # Train Set
    x_train = np.array(Y_internal)
    y_train = m.predict(X_internal).flatten()
    py.scatter(x_train, y_train, s=50, marker='.', alpha=0.3,
            c='g', cmap=cm ,edgecolors='g')
                 #label=r"$R^{2}_{Tr}$ = %.4f" % R2_train_mean)
    
    
    # CV Set
    np.array(cv)
    x_test = np.array(Y_internal)
    y_test = cv
    py.scatter(x_test, y_test, s=50, marker='.', alpha=0.3,
            c='b', cmap=cm ,edgecolors='b') 
                 #label=r"$Q^{2}_{Ext}$ = %.4f" % Q2_External_mean)
        #2SD line
    X = sm.add_constant(x_test)
    res = sm.OLS(y_test, X).fit()

    st, data, ss2 = summary_table(res, alpha=0.05)
    fittedvalues = data[:,2]
        #predict_mean_se  = data[:,3]
    predict_mean_ci_low, predict_mean_ci_upp = data[:,4:6].T
    predict_ci_low, predict_ci_upp = data[:,6:8].T
        #2SD line
    plt.plot(X, predict_ci_low, '--b', linewidth=0.5, alpha=0.5)
    plt.plot(X, predict_ci_upp, '--b', linewidth=0.5, alpha=0.5)
        
    # External Set
    x_test = np.array(Y_external)
    y_test = m.predict(X_external).flatten()
    py.scatter(x_test, y_test, s=50, marker='.', alpha=0.3,
            c='r', cmap=cm ,edgecolors='r') 
                 #label=r"$Q^{2}_{Ext}$ = %.4f" % Q2_External_mean)
    
    
    #py.plot(x_train, np.polyval(np.polyfit(x_train,y_train,1), x_train), '--r') #mean line
    plt.legend(loc=2,prop={'size':6})
    plt.xlabel("Experimental $pIC_{50}$ values", fontsize=10)
    plt.ylabel("Predicted $pIC_{50}$ values", fontsize=10)
        
    min_axis = np.min(np.concatenate([Y_internal, prediction], axis=0))
    max_axis = np.max(np.concatenate([Y_internal, prediction], axis=0))
    plt.xlim([(min_axis*0.9),(max_axis*1.05)])
    plt.ylim([(min_axis*0.9),(max_axis*1.05)])
    plt.tick_params(axis='both', which='major', labelsize=14)

    
    # Save plot to file
    plt.savefig(f+'_compair_Fp.pdf', dpi=300)
    plt.show()
    
    # Y-scrambling plot
    py.scatter(Q2_CV_mean, R2_train_mean, s=100, marker='.', alpha=0.3,
            c='b', cmap=cm ,edgecolors='b') 
            
    py.scatter(acclist_predictionOnTest_scrambledtrain,acclist_predictionOnTrain_scrambledtrain, \
               s=100, marker='.', alpha=0.3, c='r', cmap=cm ,edgecolors='r')

    plt.legend(loc=2,prop={'size':6})
    plt.xlabel("$Q^{2}$", fontsize=10)
    plt.ylabel("$R^{2}$", fontsize=10)
    plt.tick_params(axis='both', which='major', labelsize=14)
    
    plt.ylim([0, 1])
    plt.xlim([0, 1])
    plt.axhline(0.5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
    plt.axvline(0.5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
    
    plt.savefig(f+'_Y_scrambling.pdf', dpi=300)

    plt.show()

    #Feature Importance 
    fig_size = plt.rcParams["figure.figsize"]
    fig_size[0]=5
    fig_size[1]=10

    barlist = plt.barh(range(20), [x[1] for x in importances_mean[:20]], 
          color="g", xerr=[x[1] for x in importances_std[:20]], align="center", \
                       error_kw=dict(ecolor='gray', lw=2, capsize=5, capthick=2))

    print (str("top10"), [x[0] for x in importances_mean[:10]])
    print ('')
    print (str("top20"), [x[0] for x in importances_mean[:20]])
    print ('')
    print (str("top30"), [x[0] for x in importances_mean[:30]])
    print ('')
    print (str("top40"), [x[0] for x in importances_mean[:40]])
    print ('')
    print (str("top50"), [x[0] for x in importances_mean[:50]])
    
    plt.yticks(range(20), [x[0] for x in importances_mean[:20]])
    plt.ylim([-1, 20])
    
    plt.xlabel(r"$\bf{Gini}$" + " "+ r"$\bf{index}$", fontsize=12)
    ax = plt.gca()
    ax.invert_yaxis()
    plt.tight_layout(pad=2.0, w_pad=0.7, h_pad=2.0)
    plt.savefig(f+'_Feature_importances.pdf', dpi=300)
    plt.show()
141/154:
import glob # to read multiple files 
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem
import os

%config InlineBackend.figure_format = 'retina'

! rm Result/CHEMBL206_IC50_Revision.csv

outfile = open('Result/Result_LogKp.csv', 'a')

print >> outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\
                    'R2_train,R2_train_std,' + \
                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \
                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std'

path = r'QSAR/'

for f in glob.glob(path + '*.csv'):
    df = pd.read_csv(f)
    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))
    df = df.fillna(method='ffill')
    Y = df["LogKp"].as_matrix().astype(np.float)
    data = df.ix[:,2:]
    
    print '\n\n************************************************************************************'
    print ''
    print (f)
    print ''
    
    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%
    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7

    h = data.columns.tolist()
    hx = np.array(h)

    data = data.as_matrix().astype(np.float)
    X = np.array(data)
    
    # Prepare empty lists to plot QSAR model
    R2_train = []
    RMSE_train = []
    Q2_CV = []
    RMSE_CV = []
    Q2_External = []
    RMSE_External = []
    importances_dict = defaultdict(list)
    
    # Prepare empty lists to plot the performance of accuracy.
    acclist_realRF                          = []
    acclist_realRF_predictTrain             = []
    acclist_predictionOnTest_scrambledtrain  = []
    acclist_predictionOnTrain_scrambledtrain = []
        
    for i in range(10):
        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)
            
    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \
                                                                        X_internal, X_external, Y_internal, Y_external)
    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \
        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\
        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, 
           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, 
           RMSE_External_std, X_internal, X_external, X, f, des1, des2)
    plot_model(f, X_internal, X_external, Y_internal, Y_external,
                R2_train_mean, Q2_External_mean,
                importances_mean, importances_std, Feature, prediction,
                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)
    
outfile.close()
    #END
141/155:
import glob # to read multiple files 
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem
import os

%config InlineBackend.figure_format = 'retina'

! rm Result/CHEMBL206_IC50_Revision.csv

outfile = open('Result/Result_LogKp.csv', 'a')

print >> outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\
                    'R2_train,R2_train_std,' + \
                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \
                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std'

path = r'QSAR/'

for f in glob.glob(path + '*.csv'):
    df = pd.read_csv(f)
    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))
    df = df.fillna(method='ffill')
    Y = df["LogKp"].as_matrix().astype(np.float)
    data = df.ix[:,2:]
    
    print ('\n\n************************************************************************************')
    print ('')
    print (f)
    print ('')
    
    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%
    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7

    h = data.columns.tolist()
    hx = np.array(h)

    data = data.as_matrix().astype(np.float)
    X = np.array(data)
    
    # Prepare empty lists to plot QSAR model
    R2_train = []
    RMSE_train = []
    Q2_CV = []
    RMSE_CV = []
    Q2_External = []
    RMSE_External = []
    importances_dict = defaultdict(list)
    
    # Prepare empty lists to plot the performance of accuracy.
    acclist_realRF                          = []
    acclist_realRF_predictTrain             = []
    acclist_predictionOnTest_scrambledtrain  = []
    acclist_predictionOnTrain_scrambledtrain = []
        
    for i in range(10):
        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)
            
    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \
                                                                        X_internal, X_external, Y_internal, Y_external)
    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \
        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\
        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, 
           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, 
           RMSE_External_std, X_internal, X_external, X, f, des1, des2)
    plot_model(f, X_internal, X_external, Y_internal, Y_external,
                R2_train_mean, Q2_External_mean,
                importances_mean, importances_std, Feature, prediction,
                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)
    
outfile.close()
    #END
141/156:
import glob # to read multiple files 
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem
import os

%config InlineBackend.figure_format = 'retina'

! rm Result/Result_LogKp.csv

outfile = open('Result/Result_LogKp.csv', 'a')

print >> outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\
                    'R2_train,R2_train_std,' + \
                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \
                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std'

path = r'QSAR/'

for f in glob.glob(path + '*.csv'):
    df = pd.read_csv(f)
    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))
    df = df.fillna(method='ffill')
    Y = df["LogKp"].as_matrix().astype(np.float)
    data = df.ix[:,2:]
    
    print ('\n\n************************************************************************************')
    print ('')
    print (f)
    print ('')
    
    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%
    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7

    h = data.columns.tolist()
    hx = np.array(h)

    data = data.as_matrix().astype(np.float)
    X = np.array(data)
    
    # Prepare empty lists to plot QSAR model
    R2_train = []
    RMSE_train = []
    Q2_CV = []
    RMSE_CV = []
    Q2_External = []
    RMSE_External = []
    importances_dict = defaultdict(list)
    
    # Prepare empty lists to plot the performance of accuracy.
    acclist_realRF                          = []
    acclist_realRF_predictTrain             = []
    acclist_predictionOnTest_scrambledtrain  = []
    acclist_predictionOnTrain_scrambledtrain = []
        
    for i in range(10):
        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)
            
    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \
                                                                        X_internal, X_external, Y_internal, Y_external)
    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \
        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\
        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, 
           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, 
           RMSE_External_std, X_internal, X_external, X, f, des1, des2)
    plot_model(f, X_internal, X_external, Y_internal, Y_external,
                R2_train_mean, Q2_External_mean,
                importances_mean, importances_std, Feature, prediction,
                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)
    
outfile.close()
    #END
141/157:
import glob # to read multiple files 
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem
import os

%config InlineBackend.figure_format = 'retina'

! rm Result/Result_LogKp.csv

outfile = open('Result/Result_LogKp.csv', 'a')

print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\
                    'R2_train,R2_train_std,' + \
                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \
                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')

path = r'QSAR/'

for f in glob.glob(path + '*.csv'):
    df = pd.read_csv(f)
    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))
    df = df.fillna(method='ffill')
    Y = df["LogKp"].as_matrix().astype(np.float)
    data = df.ix[:,2:]
    
    print ('\n\n************************************************************************************')
    print ('')
    print (f)
    print ('')
    
    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%
    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7

    h = data.columns.tolist()
    hx = np.array(h)

    data = data.as_matrix().astype(np.float)
    X = np.array(data)
    
    # Prepare empty lists to plot QSAR model
    R2_train = []
    RMSE_train = []
    Q2_CV = []
    RMSE_CV = []
    Q2_External = []
    RMSE_External = []
    importances_dict = defaultdict(list)
    
    # Prepare empty lists to plot the performance of accuracy.
    acclist_realRF                          = []
    acclist_realRF_predictTrain             = []
    acclist_predictionOnTest_scrambledtrain  = []
    acclist_predictionOnTrain_scrambledtrain = []
        
    for i in range(10):
        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)
            
    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \
                                                                        X_internal, X_external, Y_internal, Y_external)
    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \
        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\
        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, 
           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, 
           RMSE_External_std, X_internal, X_external, X, f, des1, des2)
    plot_model(f, X_internal, X_external, Y_internal, Y_external,
                R2_train_mean, Q2_External_mean,
                importances_mean, importances_std, Feature, prediction,
                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)
    
outfile.close()
    #END
141/158:
import glob # to read multiple files 
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem
import os

%config InlineBackend.figure_format = 'retina'

! rm Result/Result_LogKp.csv

outfile = open('Result/Result_LogKp.csv', 'a')

print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\
                    'R2_train,R2_train_std,' + \
                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \
                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')

path = r'QSAR/'

for f in glob.glob(path + '*.csv'):
    df = pd.read_csv(f)
    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))
    df = df.fillna(method='ffill')
    Y = df["LogKp"].astype(np.float)
    data = df.ix[:,2:]
    
    print ('\n\n************************************************************************************')
    print ('')
    print (f)
    print ('')
    
    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%
    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7

    h = data.columns.tolist()
    hx = np.array(h)

    data = data.as_matrix().astype(np.float)
    X = np.array(data)
    
    # Prepare empty lists to plot QSAR model
    R2_train = []
    RMSE_train = []
    Q2_CV = []
    RMSE_CV = []
    Q2_External = []
    RMSE_External = []
    importances_dict = defaultdict(list)
    
    # Prepare empty lists to plot the performance of accuracy.
    acclist_realRF                          = []
    acclist_realRF_predictTrain             = []
    acclist_predictionOnTest_scrambledtrain  = []
    acclist_predictionOnTrain_scrambledtrain = []
        
    for i in range(10):
        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)
            
    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \
                                                                        X_internal, X_external, Y_internal, Y_external)
    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \
        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\
        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, 
           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, 
           RMSE_External_std, X_internal, X_external, X, f, des1, des2)
    plot_model(f, X_internal, X_external, Y_internal, Y_external,
                R2_train_mean, Q2_External_mean,
                importances_mean, importances_std, Feature, prediction,
                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)
    
outfile.close()
    #END
141/159:
import glob # to read multiple files 
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem
import os

%config InlineBackend.figure_format = 'retina'

! rm Result/Result_LogKp.csv

outfile = open('Result/Result_LogKp.csv', 'a')

print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\
                    'R2_train,R2_train_std,' + \
                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \
                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')

path = r'QSAR/'

for f in glob.glob(path + '*.csv'):
    df = pd.read_csv(f)
    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))
    df = df.fillna(method='ffill')
    Y = df["LogKp"].astype(np.float)
    data = df
    
    print ('\n\n************************************************************************************')
    print ('')
    print (f)
    print ('')
    
    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%
    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7

    h = data.columns.tolist()
    hx = np.array(h)

    data = data.as_matrix().astype(np.float)
    X = np.array(data)
    
    # Prepare empty lists to plot QSAR model
    R2_train = []
    RMSE_train = []
    Q2_CV = []
    RMSE_CV = []
    Q2_External = []
    RMSE_External = []
    importances_dict = defaultdict(list)
    
    # Prepare empty lists to plot the performance of accuracy.
    acclist_realRF                          = []
    acclist_realRF_predictTrain             = []
    acclist_predictionOnTest_scrambledtrain  = []
    acclist_predictionOnTrain_scrambledtrain = []
        
    for i in range(10):
        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)
            
    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \
                                                                        X_internal, X_external, Y_internal, Y_external)
    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \
        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\
        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, 
           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, 
           RMSE_External_std, X_internal, X_external, X, f, des1, des2)
    plot_model(f, X_internal, X_external, Y_internal, Y_external,
                R2_train_mean, Q2_External_mean,
                importances_mean, importances_std, Feature, prediction,
                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)
    
outfile.close()
    #END
141/160:
import numpy as np

def Remove_useless_descriptor(df, threshold):
    
    des1 = len(df.columns) 
    
    h = df.columns.tolist()
    df = df..astype(np.float)
    df = np.array(df)

    STDEV = np.std(df, axis=0)
    idx = [idx for idx, val in enumerate(STDEV) if val > threshold]
    df2 = df[:,idx]
    hx = np.array(h)[idx]
    
    df = pd.DataFrame(df2, columns=[hx])
    
    des2 = len(df.columns)
    
    print('from Remove useless descriptor')
    print("The initial set of " + str(des1) + \
          " descriptors has been reduced to " + str(des2) + " descriptors.")
    
    return df, des1, des2
141/161:
import numpy as np

def Remove_useless_descriptor(df, threshold):
    
    des1 = len(df.columns) 
    
    h = df.columns.tolist()
    df = df.astype(np.float)
    df = np.array(df)

    STDEV = np.std(df, axis=0)
    idx = [idx for idx, val in enumerate(STDEV) if val > threshold]
    df2 = df[:,idx]
    hx = np.array(h)[idx]
    
    df = pd.DataFrame(df2, columns=[hx])
    
    des2 = len(df.columns)
    
    print('from Remove useless descriptor')
    print("The initial set of " + str(des1) + \
          " descriptors has been reduced to " + str(des2) + " descriptors.")
    
    return df, des1, des2
141/162:
import glob # to read multiple files 
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem
import os

%config InlineBackend.figure_format = 'retina'

! rm Result/Result_LogKp.csv

outfile = open('Result/Result_LogKp.csv', 'a')

print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\
                    'R2_train,R2_train_std,' + \
                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \
                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')

path = r'QSAR/'

for f in glob.glob(path + '*.csv'):
    df = pd.read_csv(f)
    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))
    df = df.fillna(method='ffill')
    Y = df["LogKp"].astype(np.float)
    data = df
    
    print ('\n\n************************************************************************************')
    print ('')
    print (f)
    print ('')
    
    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%
    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7

    h = data.columns.tolist()
    hx = np.array(h)

    data = data.as_matrix().astype(np.float)
    X = np.array(data)
    
    # Prepare empty lists to plot QSAR model
    R2_train = []
    RMSE_train = []
    Q2_CV = []
    RMSE_CV = []
    Q2_External = []
    RMSE_External = []
    importances_dict = defaultdict(list)
    
    # Prepare empty lists to plot the performance of accuracy.
    acclist_realRF                          = []
    acclist_realRF_predictTrain             = []
    acclist_predictionOnTest_scrambledtrain  = []
    acclist_predictionOnTrain_scrambledtrain = []
        
    for i in range(10):
        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)
            
    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \
                                                                        X_internal, X_external, Y_internal, Y_external)
    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \
        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\
        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, 
           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, 
           RMSE_External_std, X_internal, X_external, X, f, des1, des2)
    plot_model(f, X_internal, X_external, Y_internal, Y_external,
                R2_train_mean, Q2_External_mean,
                importances_mean, importances_std, Feature, prediction,
                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)
    
outfile.close()
    #END
141/163:
import numpy as np

def Remove_useless_descriptor(df, threshold):
    
    des1 = len(df.columns) 
    
    h = df.columns.tolist()
    df = df.as_matrix().astype(np.float)
    df = np.array(df)

    STDEV = np.std(df, axis=0)
    idx = [idx for idx, val in enumerate(STDEV) if val > threshold]
    df2 = df[:,idx]
    hx = np.array(h)[idx]
    
    df = pd.DataFrame(df2, columns=[hx])
    
    des2 = len(df.columns)
    
    print('from Remove useless descriptor')
    print("The initial set of " + str(des1) + \
          " descriptors has been reduced to " + str(des2) + " descriptors.")
    
    return df, des1, des2
141/164:
import glob # to read multiple files 
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem
import os

%config InlineBackend.figure_format = 'retina'

! rm Result/Result_LogKp.csv

outfile = open('Result/Result_LogKp.csv', 'a')

print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\
                    'R2_train,R2_train_std,' + \
                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \
                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')

path = r'QSAR/'

for f in glob.glob(path + '*.csv'):
    df = pd.read_csv(f)
    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))
    df = df.fillna(method='ffill')
    Y = df["LogKp"].astype(np.float)
    data = df
    
    print ('\n\n************************************************************************************')
    print ('')
    print (f)
    print ('')
    
    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%
    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7

    h = data.columns.tolist()
    hx = np.array(h)

    data = data.as_matrix().astype(np.float)
    X = np.array(data)
    
    # Prepare empty lists to plot QSAR model
    R2_train = []
    RMSE_train = []
    Q2_CV = []
    RMSE_CV = []
    Q2_External = []
    RMSE_External = []
    importances_dict = defaultdict(list)
    
    # Prepare empty lists to plot the performance of accuracy.
    acclist_realRF                          = []
    acclist_realRF_predictTrain             = []
    acclist_predictionOnTest_scrambledtrain  = []
    acclist_predictionOnTrain_scrambledtrain = []
        
    for i in range(10):
        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)
            
    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \
                                                                        X_internal, X_external, Y_internal, Y_external)
    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \
        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\
        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, 
           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, 
           RMSE_External_std, X_internal, X_external, X, f, des1, des2)
    plot_model(f, X_internal, X_external, Y_internal, Y_external,
                R2_train_mean, Q2_External_mean,
                importances_mean, importances_std, Feature, prediction,
                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)
    
outfile.close()
    #END
141/165:
import numpy as np

def Remove_useless_descriptor(df, threshold):
    
    des1 = len(df.columns) 
    
    h = df.columns.tolist()
    df = df.values.astype(np.float)
    df = np.array(df)

    STDEV = np.std(df, axis=0)
    idx = [idx for idx, val in enumerate(STDEV) if val > threshold]
    df2 = df[:,idx]
    hx = np.array(h)[idx]
    
    df = pd.DataFrame(df2, columns=[hx])
    
    des2 = len(df.columns)
    
    print('from Remove useless descriptor')
    print("The initial set of " + str(des1) + \
          " descriptors has been reduced to " + str(des2) + " descriptors.")
    
    return df, des1, des2
141/166:
import glob # to read multiple files 
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem
import os

%config InlineBackend.figure_format = 'retina'

! rm Result/Result_LogKp.csv

outfile = open('Result/Result_LogKp.csv', 'a')

print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\
                    'R2_train,R2_train_std,' + \
                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \
                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')

path = r'QSAR/'

for f in glob.glob(path + '*.csv'):
    df = pd.read_csv(f)
    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))
    df = df.fillna(method='ffill')
    Y = df["LogKp"].values.astype(np.float)
    data = df
    
    print ('\n\n************************************************************************************')
    print ('')
    print (f)
    print ('')
    
    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%
    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7

    h = data.columns.tolist()
    hx = np.array(h)

    data = data.values.astype(np.float)
    X = np.array(data)
    
    # Prepare empty lists to plot QSAR model
    R2_train = []
    RMSE_train = []
    Q2_CV = []
    RMSE_CV = []
    Q2_External = []
    RMSE_External = []
    importances_dict = defaultdict(list)
    
    # Prepare empty lists to plot the performance of accuracy.
    acclist_realRF                          = []
    acclist_realRF_predictTrain             = []
    acclist_predictionOnTest_scrambledtrain  = []
    acclist_predictionOnTrain_scrambledtrain = []
        
    for i in range(10):
        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)
            
    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \
                                                                        X_internal, X_external, Y_internal, Y_external)
    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \
        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\
        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, 
           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, 
           RMSE_External_std, X_internal, X_external, X, f, des1, des2)
    plot_model(f, X_internal, X_external, Y_internal, Y_external,
                R2_train_mean, Q2_External_mean,
                importances_mean, importances_std, Feature, prediction,
                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)
    
outfile.close()
    #END
142/1:
import pandas as pd
import numpy as np

df0 = pd.read_csv('model/ER_alpha_Train_final.csv', header = 0)
142/2: len(df0)
142/3: df0.tail(2)
142/4: df = df0.drop(['STATUS.1'], axis=1)
142/5: df = df[['chemblId','pIC50']]
142/6: df.head()
142/7:
path = r'Fingerprint/'

Fp1  = pd.read_csv(path + 'Train_ER_alpha-Fingerprinter.csv'               , header = 0)
Fp2  = pd.read_csv(path + 'Train_ER_alpha-ExtendedFingerprinter.csv'       , header = 0)
Fp3  = pd.read_csv(path + 'Train_ER_alpha-EStateFingerprinter.csv'         , header = 0)
Fp4  = pd.read_csv(path + 'Train_ER_alpha-GraphOnlyFingerprinter.csv'      , header = 0)
Fp5  = pd.read_csv(path + 'Train_ER_alpha-MACCSFingerprinter.csv'          , header = 0)
Fp6  = pd.read_csv(path + 'Train_ER_alpha-PubchemFingerprinter.csv'        , header = 0)
Fp7  = pd.read_csv(path + 'Train_ER_alpha-SubstructureFingerprinter.csv'   , header = 0)
Fp8  = pd.read_csv(path + 'Train_ER_alpha-SubstructureFingerprintCount.csv', header = 0)
Fp9  = pd.read_csv(path + 'Train_ER_alpha-KlekotaRothFingerprinter.csv'    , header = 0)
Fp10 = pd.read_csv(path + 'Train_ER_alpha-KlekotaRothFingerprintCount.csv' , header = 0)
Fp11 = pd.read_csv(path + 'Train_ER_alpha-AtomPairs2DFingerprinter.csv'    , header = 0)
Fp12 = pd.read_csv(path + 'Train_ER_alpha-AtomPairs2DFingerprintCount.csv' , header = 0)
142/8:
Fp1 = Fp1.rename(columns = {'Name':'chemblId'})
Fp2 = Fp2.rename(columns = {'Name':'chemblId'})
Fp3 = Fp3.rename(columns = {'Name':'chemblId'})
Fp4 = Fp4.rename(columns = {'Name':'chemblId'})
Fp5 = Fp5.rename(columns = {'Name':'chemblId'})
Fp6 = Fp6.rename(columns = {'Name':'chemblId'})
Fp7 = Fp7.rename(columns = {'Name':'chemblId'})
Fp8 = Fp8.rename(columns = {'Name':'chemblId'})
Fp9 = Fp9.rename(columns = {'Name':'chemblId'})
Fp10= Fp10.rename(columns = {'Name':'chemblId'})
Fp11= Fp11.rename(columns = {'Name':'chemblId'})
Fp12= Fp12.rename(columns = {'Name':'chemblId'})
142/9:
from sklearn import preprocessing

def normalized (Fp):
    
    chemblId = Fp.chemblId
    Fp_ix = Fp.ix[:,1:]
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp_ix)
    Fp_normalized = pd.DataFrame(np_scaled)
    Fp_normalized
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp_ix.columns)
    Fp_normalized['chemblId'] = chemblId
    
    return Fp_normalized
142/10:
Fp1_n  = normalized (Fp1 )
Fp2_n  = normalized (Fp2 )
Fp3_n  = normalized (Fp3 )
Fp4_n  = normalized (Fp4 )
Fp5_n  = normalized (Fp5 )
Fp6_n  = normalized (Fp6 )
Fp7_n  = normalized (Fp7 )
Fp8_n  = normalized (Fp8 )
Fp9_n  = normalized (Fp9 )
Fp10_n = normalized (Fp10)
Fp11_n = normalized (Fp11)
Fp12_n = normalized (Fp12)
141/167:
raw1 .to_csv('QSAR/Kp_Fingerprinter.csv'               , sep=',' ,index=False)
raw2 .to_csv('QSAR/Kp_ExtendedFingerprinter.csv'       , sep=',' ,index=True)
raw3 .to_csv('QSAR/Kp_EStateFingerprinter.csv'         , sep=',' ,index=True)
raw4 .to_csv('QSAR/Kp_GraphOnlyFingerprinter.csv'      , sep=',' ,index=True)
raw5 .to_csv('QSAR/Kp_MACCSFingerprinter.csv'          , sep=',' ,index=True)
raw6 .to_csv('QSAR/Kp_PubchemFingerprinter.csv'        , sep=',' ,index=True)
raw7 .to_csv('QSAR/Kp_SubstructureFingerprinter.csv'   , sep=',' ,index=True)
raw8 .to_csv('QSAR/Kp_SubstructureFingerprintCount.csv', sep=',' ,index=True)
raw9 .to_csv('QSAR/Kp_KlekotaRothFingerprinter.csv'    , sep=',' ,index=True)
raw10.to_csv('QSAR/Kp_KlekotaRothFingerprintCount.csv' , sep=',' ,index=True)
raw11.to_csv('QSAR/Kp_AtomPairs2DFingerprinter.csv'    , sep=',' ,index=True)
raw12.to_csv('QSAR/Kp_AtomPairs2DFingerprintCount.csv' , sep=',' ,index=True)
141/168:
import glob # to read multiple files 
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem
import os

%config InlineBackend.figure_format = 'retina'

! rm Result/Result_LogKp.csv

outfile = open('Result/Result_LogKp.csv', 'a')

print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\
                    'R2_train,R2_train_std,' + \
                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \
                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')

path = r'QSAR/'

for f in glob.glob(path + '*.csv'):
    df = pd.read_csv(f)
    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))
    df = df.fillna(method='ffill')
    Y = df["LogKp"].values.astype(np.float)
    data = df
    
    print ('\n\n************************************************************************************')
    print ('')
    print (f)
    print ('')
    
    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%
    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7

    h = data.columns.tolist()
    hx = np.array(h)

    data = data.values.astype(np.float)
    X = np.array(data)
    
    # Prepare empty lists to plot QSAR model
    R2_train = []
    RMSE_train = []
    Q2_CV = []
    RMSE_CV = []
    Q2_External = []
    RMSE_External = []
    importances_dict = defaultdict(list)
    
    # Prepare empty lists to plot the performance of accuracy.
    acclist_realRF                          = []
    acclist_realRF_predictTrain             = []
    acclist_predictionOnTest_scrambledtrain  = []
    acclist_predictionOnTrain_scrambledtrain = []
        
    for i in range(10):
        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)
            
    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \
                                                                        X_internal, X_external, Y_internal, Y_external)
    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \
        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\
        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, 
           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, 
           RMSE_External_std, X_internal, X_external, X, f, des1, des2)
    plot_model(f, X_internal, X_external, Y_internal, Y_external,
                R2_train_mean, Q2_External_mean,
                importances_mean, importances_std, Feature, prediction,
                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)
    
outfile.close()
    #END
141/169:
raw1 .to_csv('QSAR/Kp_Fingerprinter.csv'               , sep=',' ,index=False)
raw2 .to_csv('QSAR/Kp_ExtendedFingerprinter.csv'       , sep=',' ,index=True)
raw3 .to_csv('QSAR/Kp_EStateFingerprinter.csv'         , sep=',' ,index=True)
raw4 .to_csv('QSAR/Kp_GraphOnlyFingerprinter.csv'      , sep=',' ,index=True)
raw5 .to_csv('QSAR/Kp_MACCSFingerprinter.csv'          , sep=',' ,index=True)
raw6 .to_csv('QSAR/Kp_PubchemFingerprinter.csv'        , sep=',' ,index=True)
raw7 .to_csv('QSAR/Kp_SubstructureFingerprinter.csv'   , sep=',' ,index=False)
raw8 .to_csv('QSAR/Kp_SubstructureFingerprintCount.csv', sep=',' ,index=True)
raw9 .to_csv('QSAR/Kp_KlekotaRothFingerprinter.csv'    , sep=',' ,index=True)
raw10.to_csv('QSAR/Kp_KlekotaRothFingerprintCount.csv' , sep=',' ,index=True)
raw11.to_csv('QSAR/Kp_AtomPairs2DFingerprinter.csv'    , sep=',' ,index=True)
raw12.to_csv('QSAR/Kp_AtomPairs2DFingerprintCount.csv' , sep=',' ,index=True)
141/170:
import glob # to read multiple files 
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem
import os

%config InlineBackend.figure_format = 'retina'

! rm Result/Result_LogKp.csv

outfile = open('Result/Result_LogKp.csv', 'a')

print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\
                    'R2_train,R2_train_std,' + \
                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \
                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')

path = r'QSAR/'

for f in glob.glob(path + '*.csv'):
    df = pd.read_csv(f)
    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))
    df = df.fillna(method='ffill')
    Y = df["LogKp"].values.astype(np.float)
    data = df
    
    print ('\n\n************************************************************************************')
    print ('')
    print (f)
    print ('')
    
    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%
    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7

    h = data.columns.tolist()
    hx = np.array(h)

    data = data.values.astype(np.float)
    X = np.array(data)
    
    # Prepare empty lists to plot QSAR model
    R2_train = []
    RMSE_train = []
    Q2_CV = []
    RMSE_CV = []
    Q2_External = []
    RMSE_External = []
    importances_dict = defaultdict(list)
    
    # Prepare empty lists to plot the performance of accuracy.
    acclist_realRF                          = []
    acclist_realRF_predictTrain             = []
    acclist_predictionOnTest_scrambledtrain  = []
    acclist_predictionOnTrain_scrambledtrain = []
        
    for i in range(10):
        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)
            
    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \
                                                                        X_internal, X_external, Y_internal, Y_external)
    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \
        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\
        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, 
           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, 
           RMSE_External_std, X_internal, X_external, X, f, des1, des2)
    plot_model(f, X_internal, X_external, Y_internal, Y_external,
                R2_train_mean, Q2_External_mean,
                importances_mean, importances_std, Feature, prediction,
                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)
    
outfile.close()
    #END
141/171:
raw1 .to_csv('QSAR/Kp_Fingerprinter.csv'               , sep=',' ,index=False)
raw2 .to_csv('QSAR/Kp_ExtendedFingerprinter.csv'       , sep=',' ,index=False)
raw3 .to_csv('QSAR/Kp_EStateFingerprinter.csv'         , sep=',' ,index=False)
raw4 .to_csv('QSAR/Kp_GraphOnlyFingerprinter.csv'      , sep=',' ,index=False)
raw5 .to_csv('QSAR/Kp_MACCSFingerprinter.csv'          , sep=',' ,index=False)
raw6 .to_csv('QSAR/Kp_PubchemFingerprinter.csv'        , sep=',' ,index=False)
raw7 .to_csv('QSAR/Kp_SubstructureFingerprinter.csv'   , sep=',' ,index=False)
raw8 .to_csv('QSAR/Kp_SubstructureFingerprintCount.csv', sep=',' ,index=False)
raw9 .to_csv('QSAR/Kp_KlekotaRothFingerprinter.csv'    , sep=',' ,index=False)
raw10.to_csv('QSAR/Kp_KlekotaRothFingerprintCount.csv' , sep=',' ,index=False)
raw11.to_csv('QSAR/Kp_AtomPairs2DFingerprinter.csv'    , sep=',' ,index=False)
raw12.to_csv('QSAR/Kp_AtomPairs2DFingerprintCount.csv' , sep=',' ,index=False)
141/172:
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict

def build_model(X, Y, seed, hx, f):
    
    
    #Data split using 70/30 ratio
    X_internal, X_external, Y_internal, Y_external = train_test_split(X,
                                                        Y, test_size=0.3,
                                                        random_state=seed)

    # Training set
    rf = RandomForestRegressor(n_estimators=400, 
                               max_features='sqrt', 
                               min_samples_leaf = 1,
                               random_state = 13,
                               n_jobs=-1)
    
    rf.fit(X_internal,Y_internal)
    prediction = rf.predict(X_internal)
    
    # Cross-validation
    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)
    
    # External set  
    prediction_external = rf.predict(X_external)
    
    #print result from each seed    
    R2_train.append(r2_score(Y_internal, prediction))
    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))
    Q2_CV.append(r2_score(Y_internal, cv))
    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))
    Q2_External.append(r2_score(Y_external, prediction_external))
    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))
    
    #Feature Importance
    Feature = hx[:]
    feature_importance = rf.feature_importances_
    importances = 100.0 * (feature_importance / feature_importance.max()) #index
    
    for i, fx in enumerate(Feature):
        importances_dict[fx].append(importances[i])
    
    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict
141/173:
import glob # to read multiple files 
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem
import os

%config InlineBackend.figure_format = 'retina'

! rm Result/Result_LogKp.csv

outfile = open('Result/Result_LogKp.csv', 'a')

print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\
                    'R2_train,R2_train_std,' + \
                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \
                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')

path = r'QSAR/'

for f in glob.glob(path + '*.csv'):
    df = pd.read_csv(f)
    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))
    df = df.fillna(method='ffill')
    Y = df["LogKp"].values.astype(np.float)
    data = df
    
    print ('\n\n************************************************************************************')
    print ('')
    print (f)
    print ('')
    
    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%
    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7

    h = data.columns.tolist()
    hx = np.array(h)

    data = data.values.astype(np.float)
    X = np.array(data)
    
    # Prepare empty lists to plot QSAR model
    R2_train = []
    RMSE_train = []
    Q2_CV = []
    RMSE_CV = []
    Q2_External = []
    RMSE_External = []
    importances_dict = defaultdict(list)
    
    # Prepare empty lists to plot the performance of accuracy.
    acclist_realRF                          = []
    acclist_realRF_predictTrain             = []
    acclist_predictionOnTest_scrambledtrain  = []
    acclist_predictionOnTrain_scrambledtrain = []
        
    for i in range(10):
        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)
            
    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \
                                                                        X_internal, X_external, Y_internal, Y_external)
    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \
        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\
        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, 
           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, 
           RMSE_External_std, X_internal, X_external, X, f, des1, des2)
    plot_model(f, X_internal, X_external, Y_internal, Y_external,
                R2_train_mean, Q2_External_mean,
                importances_mean, importances_std, Feature, prediction,
                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)
    
outfile.close()
    #END
141/174:
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict

def build_model(X, Y, seed, hx, f):
    
    
    #Data split using 70/30 ratio
    X_internal, X_external, Y_internal, Y_external = train_test_split(X,
                                                        Y, test_size=0.3,
                                                        random_state=seed)

    # Training set
    rf = RandomForestRegressor(n_estimators=400, 
                               max_features='sqrt', 
                               min_samples_leaf = 1,
                               random_state = 13,
                               n_jobs=-1)
    
    rf.fit(X_internal,Y_internal)
    prediction = rf.predict(X_internal)
    
    # Cross-validation
    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)
    
    # External set  
    prediction_external = rf.predict(X_external)
    
    #print result from each seed    
    R2_train.append(r2_score(Y_internal, prediction))
    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))
    Q2_CV.append(r2_score(Y_internal, cv))
    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))
    Q2_External.append(r2_score(Y_external, prediction_external))
    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))
    
    #Feature Importance
    Feature = hx[:]
    feature_importance = rf.feature_importances_
    importances = 100.0 * (feature_importance / feature_importance.max()) #index
    
    for i, fx in enumerate(Feature):
        importances[fx].append(importances[i])
    
    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict
141/175:
import glob # to read multiple files 
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem
import os

%config InlineBackend.figure_format = 'retina'

! rm Result/Result_LogKp.csv

outfile = open('Result/Result_LogKp.csv', 'a')

print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\
                    'R2_train,R2_train_std,' + \
                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \
                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')

path = r'QSAR/'

for f in glob.glob(path + '*.csv'):
    df = pd.read_csv(f)
    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))
    df = df.fillna(method='ffill')
    Y = df["LogKp"].values.astype(np.float)
    data = df
    
    print ('\n\n************************************************************************************')
    print ('')
    print (f)
    print ('')
    
    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%
    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7

    h = data.columns.tolist()
    hx = np.array(h)

    data = data.values.astype(np.float)
    X = np.array(data)
    
    # Prepare empty lists to plot QSAR model
    R2_train = []
    RMSE_train = []
    Q2_CV = []
    RMSE_CV = []
    Q2_External = []
    RMSE_External = []
    importances_dict = defaultdict(list)
    
    # Prepare empty lists to plot the performance of accuracy.
    acclist_realRF                          = []
    acclist_realRF_predictTrain             = []
    acclist_predictionOnTest_scrambledtrain  = []
    acclist_predictionOnTrain_scrambledtrain = []
        
    for i in range(10):
        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)
            
    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \
                                                                        X_internal, X_external, Y_internal, Y_external)
    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \
        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\
        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, 
           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, 
           RMSE_External_std, X_internal, X_external, X, f, des1, des2)
    plot_model(f, X_internal, X_external, Y_internal, Y_external,
                R2_train_mean, Q2_External_mean,
                importances_mean, importances_std, Feature, prediction,
                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)
    
outfile.close()
    #END
141/176:
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict

def build_model(X, Y, seed, hx, f):
    
    
    #Data split using 70/30 ratio
    X_internal, X_external, Y_internal, Y_external = train_test_split(X,
                                                        Y, test_size=0.3,
                                                        random_state=seed)

    # Training set
    rf = RandomForestRegressor(n_estimators=400, 
                               max_features='sqrt', 
                               min_samples_leaf = 1,
                               random_state = 13,
                               n_jobs=-1)
    
    rf.fit(X_internal,Y_internal)
    prediction = rf.predict(X_internal)
    
    # Cross-validation
    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)
    
    # External set  
    prediction_external = rf.predict(X_external)
    
    #print result from each seed    
    R2_train.append(r2_score(Y_internal, prediction))
    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))
    Q2_CV.append(r2_score(Y_internal, cv))
    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))
    Q2_External.append(r2_score(Y_external, prediction_external))
    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))
    
    #Feature Importance
    Feature = hx[:]
    feature_importance = rf.feature_importances_
    importances = 100.0 * (feature_importance / feature_importance.max()) #index
    
    for i, fx in enumerate(Feature):
        importances_dict[fx].append(importances_dict[i])
    
    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict
141/177:
import glob # to read multiple files 
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem
import os

%config InlineBackend.figure_format = 'retina'

! rm Result/Result_LogKp.csv

outfile = open('Result/Result_LogKp.csv', 'a')

print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\
                    'R2_train,R2_train_std,' + \
                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \
                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')

path = r'QSAR/'

for f in glob.glob(path + '*.csv'):
    df = pd.read_csv(f)
    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))
    df = df.fillna(method='ffill')
    Y = df["LogKp"].values.astype(np.float)
    data = df
    
    print ('\n\n************************************************************************************')
    print ('')
    print (f)
    print ('')
    
    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%
    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7

    h = data.columns.tolist()
    hx = np.array(h)

    data = data.values.astype(np.float)
    X = np.array(data)
    
    # Prepare empty lists to plot QSAR model
    R2_train = []
    RMSE_train = []
    Q2_CV = []
    RMSE_CV = []
    Q2_External = []
    RMSE_External = []
    importances_dict = defaultdict(list)
    
    # Prepare empty lists to plot the performance of accuracy.
    acclist_realRF                          = []
    acclist_realRF_predictTrain             = []
    acclist_predictionOnTest_scrambledtrain  = []
    acclist_predictionOnTrain_scrambledtrain = []
        
    for i in range(10):
        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)
            
    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \
                                                                        X_internal, X_external, Y_internal, Y_external)
    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \
        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\
        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, 
           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, 
           RMSE_External_std, X_internal, X_external, X, f, des1, des2)
    plot_model(f, X_internal, X_external, Y_internal, Y_external,
                R2_train_mean, Q2_External_mean,
                importances_mean, importances_std, Feature, prediction,
                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)
    
outfile.close()
    #END
141/178:
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict

def build_model(X, Y, seed, hx, f):
    
    
    #Data split using 70/30 ratio
    X_internal, X_external, Y_internal, Y_external = train_test_split(X,
                                                        Y, test_size=0.3,
                                                        random_state=seed)

    # Training set
    rf = RandomForestRegressor(n_estimators=400, 
                               max_features='sqrt', 
                               min_samples_leaf = 1,
                               random_state = 13,
                               n_jobs=-1)
    
    rf.fit(X_internal,Y_internal)
    prediction = rf.predict(X_internal)
    
    # Cross-validation
    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)
    
    # External set  
    prediction_external = rf.predict(X_external)
    
    #print result from each seed    
    R2_train.append(r2_score(Y_internal, prediction))
    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))
    Q2_CV.append(r2_score(Y_internal, cv))
    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))
    Q2_External.append(r2_score(Y_external, prediction_external))
    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))
    
    #Feature Importance
    Feature = hx[:]
    feature_importance = rf.feature_importances_
    importances = 100.0 * (feature_importance / feature_importance.max()) #index
    
    for i, fx in enumerate(Feature):
        importances[fx].append(importances_dict[i])
    
    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict
141/179:
import glob # to read multiple files 
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem
import os

%config InlineBackend.figure_format = 'retina'

! rm Result/Result_LogKp.csv

outfile = open('Result/Result_LogKp.csv', 'a')

print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\
                    'R2_train,R2_train_std,' + \
                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \
                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')

path = r'QSAR/'

for f in glob.glob(path + '*.csv'):
    df = pd.read_csv(f)
    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))
    df = df.fillna(method='ffill')
    Y = df["LogKp"].values.astype(np.float)
    data = df
    
    print ('\n\n************************************************************************************')
    print ('')
    print (f)
    print ('')
    
    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%
    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7

    h = data.columns.tolist()
    hx = np.array(h)

    data = data.values.astype(np.float)
    X = np.array(data)
    
    # Prepare empty lists to plot QSAR model
    R2_train = []
    RMSE_train = []
    Q2_CV = []
    RMSE_CV = []
    Q2_External = []
    RMSE_External = []
    importances_dict = defaultdict(list)
    
    # Prepare empty lists to plot the performance of accuracy.
    acclist_realRF                          = []
    acclist_realRF_predictTrain             = []
    acclist_predictionOnTest_scrambledtrain  = []
    acclist_predictionOnTrain_scrambledtrain = []
        
    for i in range(10):
        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)
            
    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \
                                                                        X_internal, X_external, Y_internal, Y_external)
    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \
        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\
        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, 
           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, 
           RMSE_External_std, X_internal, X_external, X, f, des1, des2)
    plot_model(f, X_internal, X_external, Y_internal, Y_external,
                R2_train_mean, Q2_External_mean,
                importances_mean, importances_std, Feature, prediction,
                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)
    
outfile.close()
    #END
141/180:
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict

def build_model(X, Y, seed, hx, f):
    
    
    #Data split using 70/30 ratio
    X_internal, X_external, Y_internal, Y_external = train_test_split(X,
                                                        Y, test_size=0.3,
                                                        random_state=seed)

    # Training set
    rf = RandomForestRegressor(n_estimators=400, 
                               max_features='sqrt', 
                               min_samples_leaf = 1,
                               random_state = 13,
                               n_jobs=-1)
    
    rf.fit(X_internal,Y_internal)
    prediction = rf.predict(X_internal)
    
    # Cross-validation
    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)
    
    # External set  
    prediction_external = rf.predict(X_external)
    
    #print result from each seed    
    R2_train.append(r2_score(Y_internal, prediction))
    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))
    Q2_CV.append(r2_score(Y_internal, cv))
    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))
    Q2_External.append(r2_score(Y_external, prediction_external))
    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))
    
    #Feature Importance
    Feature = hx[:]
    feature_importance = rf.feature_importances_
    importances = 100.0 * (feature_importance / feature_importance.max()) #index
    
    for i, fx in enumerate(Feature):
        importances[fx].append(importances_dict[i]).astype(int)
    
    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict
141/181:
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict

def build_model(X, Y, seed, hx, f):
    
    
    #Data split using 70/30 ratio
    X_internal, X_external, Y_internal, Y_external = train_test_split(X,
                                                        Y, test_size=0.3,
                                                        random_state=seed)

    # Training set
    rf = RandomForestRegressor(n_estimators=400, 
                               max_features='sqrt', 
                               min_samples_leaf = 1,
                               random_state = 13,
                               n_jobs=-1)
    
    rf.fit(X_internal,Y_internal)
    prediction = rf.predict(X_internal)
    
    # Cross-validation
    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)
    
    # External set  
    prediction_external = rf.predict(X_external)
    
    #print result from each seed    
    R2_train.append(r2_score(Y_internal, prediction))
    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))
    Q2_CV.append(r2_score(Y_internal, cv))
    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))
    Q2_External.append(r2_score(Y_external, prediction_external))
    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))
    
    #Feature Importance
    Feature = hx[:]
    feature_importance = rf.feature_importances_
    importances = 100.0 * (feature_importance / feature_importance.max()) #index
    
    for i, fx in enumerate(Feature):
        importances[fx].astype(int).append(importances_dict[i])
    
    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict
141/182:
import glob # to read multiple files 
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem
import os

%config InlineBackend.figure_format = 'retina'

! rm Result/Result_LogKp.csv

outfile = open('Result/Result_LogKp.csv', 'a')

print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\
                    'R2_train,R2_train_std,' + \
                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \
                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')

path = r'QSAR/'

for f in glob.glob(path + '*.csv'):
    df = pd.read_csv(f)
    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))
    df = df.fillna(method='ffill')
    Y = df["LogKp"].values.astype(np.float)
    data = df
    
    print ('\n\n************************************************************************************')
    print ('')
    print (f)
    print ('')
    
    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%
    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7

    h = data.columns.tolist()
    hx = np.array(h)

    data = data.values.astype(np.float)
    X = np.array(data)
    
    # Prepare empty lists to plot QSAR model
    R2_train = []
    RMSE_train = []
    Q2_CV = []
    RMSE_CV = []
    Q2_External = []
    RMSE_External = []
    importances_dict = defaultdict(list)
    
    # Prepare empty lists to plot the performance of accuracy.
    acclist_realRF                          = []
    acclist_realRF_predictTrain             = []
    acclist_predictionOnTest_scrambledtrain  = []
    acclist_predictionOnTrain_scrambledtrain = []
        
    for i in range(10):
        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)
            
    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \
                                                                        X_internal, X_external, Y_internal, Y_external)
    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \
        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\
        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, 
           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, 
           RMSE_External_std, X_internal, X_external, X, f, des1, des2)
    plot_model(f, X_internal, X_external, Y_internal, Y_external,
                R2_train_mean, Q2_External_mean,
                importances_mean, importances_std, Feature, prediction,
                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)
    
outfile.close()
    #END
141/183:
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict

def build_model(X, Y, seed, hx, f):
    
    
    #Data split using 70/30 ratio
    X_internal, X_external, Y_internal, Y_external = train_test_split(X,
                                                        Y, test_size=0.3,
                                                        random_state=seed)

    # Training set
    rf = RandomForestRegressor(n_estimators=400, 
                               max_features='sqrt', 
                               min_samples_leaf = 1,
                               random_state = 13,
                               n_jobs=-1)
    
    rf.fit(X_internal,Y_internal)
    prediction = rf.predict(X_internal)
    
    # Cross-validation
    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)
    
    # External set  
    prediction_external = rf.predict(X_external)
    
    #print result from each seed    
    R2_train.append(r2_score(Y_internal, prediction))
    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))
    Q2_CV.append(r2_score(Y_internal, cv))
    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))
    Q2_External.append(r2_score(Y_external, prediction_external))
    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))
    
    #Feature Importance
    Feature = hx[:]
    feature_importance = rf.feature_importances_
    importances = 100.0 * (feature_importance / feature_importance.max()) #index
    
    for i, fx in enumerate(Feature):
        int(importances_dict[fx]).append(importances_dict[i])
    
    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict
141/184:
import glob # to read multiple files 
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem
import os

%config InlineBackend.figure_format = 'retina'

! rm Result/Result_LogKp.csv

outfile = open('Result/Result_LogKp.csv', 'a')

print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\
                    'R2_train,R2_train_std,' + \
                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \
                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')

path = r'QSAR/'

for f in glob.glob(path + '*.csv'):
    df = pd.read_csv(f)
    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))
    df = df.fillna(method='ffill')
    Y = df["LogKp"].values.astype(np.float)
    data = df
    
    print ('\n\n************************************************************************************')
    print ('')
    print (f)
    print ('')
    
    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%
    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7

    h = data.columns.tolist()
    hx = np.array(h)

    data = data.values.astype(np.float)
    X = np.array(data)
    
    # Prepare empty lists to plot QSAR model
    R2_train = []
    RMSE_train = []
    Q2_CV = []
    RMSE_CV = []
    Q2_External = []
    RMSE_External = []
    importances_dict = defaultdict(list)
    
    # Prepare empty lists to plot the performance of accuracy.
    acclist_realRF                          = []
    acclist_realRF_predictTrain             = []
    acclist_predictionOnTest_scrambledtrain  = []
    acclist_predictionOnTrain_scrambledtrain = []
        
    for i in range(10):
        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)
            
    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \
                                                                        X_internal, X_external, Y_internal, Y_external)
    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \
        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\
        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, 
           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, 
           RMSE_External_std, X_internal, X_external, X, f, des1, des2)
    plot_model(f, X_internal, X_external, Y_internal, Y_external,
                R2_train_mean, Q2_External_mean,
                importances_mean, importances_std, Feature, prediction,
                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)
    
outfile.close()
    #END
141/185:
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict

def build_model(X, Y, seed, hx, f):
    
    
    #Data split using 70/30 ratio
    X_internal, X_external, Y_internal, Y_external = train_test_split(X,
                                                        Y, test_size=0.3,
                                                        random_state=seed)

    # Training set
    rf = RandomForestRegressor(n_estimators=400, 
                               max_features='sqrt', 
                               min_samples_leaf = 1,
                               random_state = 13,
                               n_jobs=-1)
    
    rf.fit(X_internal,Y_internal)
    prediction = rf.predict(X_internal)
    
    # Cross-validation
    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)
    
    # External set  
    prediction_external = rf.predict(X_external)
    
    #print result from each seed    
    R2_train.append(r2_score(Y_internal, prediction))
    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))
    Q2_CV.append(r2_score(Y_internal, cv))
    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))
    Q2_External.append(r2_score(Y_external, prediction_external))
    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))
    
    #Feature Importance
    Feature = hx[:]
    feature_importance = rf.feature_importances_
    importances = 100.0 * (feature_importance / feature_importance.max()) #index
    
    for i, fx in enumerate(Feature):
        int(importances_dict[fx]).append(importances[i])
    
    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict
141/186:
import glob # to read multiple files 
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem
import os

%config InlineBackend.figure_format = 'retina'

! rm Result/Result_LogKp.csv

outfile = open('Result/Result_LogKp.csv', 'a')

print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\
                    'R2_train,R2_train_std,' + \
                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \
                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')

path = r'QSAR/'

for f in glob.glob(path + '*.csv'):
    df = pd.read_csv(f)
    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))
    df = df.fillna(method='ffill')
    Y = df["LogKp"].values.astype(np.float)
    data = df
    
    print ('\n\n************************************************************************************')
    print ('')
    print (f)
    print ('')
    
    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%
    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7

    h = data.columns.tolist()
    hx = np.array(h)

    data = data.values.astype(np.float)
    X = np.array(data)
    
    # Prepare empty lists to plot QSAR model
    R2_train = []
    RMSE_train = []
    Q2_CV = []
    RMSE_CV = []
    Q2_External = []
    RMSE_External = []
    importances_dict = defaultdict(list)
    
    # Prepare empty lists to plot the performance of accuracy.
    acclist_realRF                          = []
    acclist_realRF_predictTrain             = []
    acclist_predictionOnTest_scrambledtrain  = []
    acclist_predictionOnTrain_scrambledtrain = []
        
    for i in range(10):
        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)
            
    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \
                                                                        X_internal, X_external, Y_internal, Y_external)
    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \
        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\
        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, 
           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, 
           RMSE_External_std, X_internal, X_external, X, f, des1, des2)
    plot_model(f, X_internal, X_external, Y_internal, Y_external,
                R2_train_mean, Q2_External_mean,
                importances_mean, importances_std, Feature, prediction,
                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)
    
outfile.close()
    #END
141/187:
def print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, 
           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, 
           RMSE_External_std, X_internal, X_external, X, f, des1, des2 ):
    #outfile = open('all_output.csv', 'a')
    name = [os.path.basename(f)]
    print_out = name
    print_out = [name.replace('.smi','') for name in print_out]
    
    print (outfile, '%s,%d,%d,%d,%d,%0.4f,%0.4f,%0.4f,%0.4f,%d,%0.4f,%0.4f,%0.4f,%0.4f,%d,%0.4f,%0.4f,%0.4f,%0.4f' \
                                     % (print_out, len(X_internal),
                                        des1, des2, des4,
                                        R2_train_mean, R2_train_std,
                                        RMSE_train_mean, RMSE_train_std,
                                        len(X_internal),
                                        Q2_CV_mean, Q2_CV_std,
                                        RMSE_CV_mean, RMSE_CV_std,
                                        len(X_external),
                                        Q2_External_mean, Q2_External_std,
                                        RMSE_External_mean, RMSE_External_std,))
 
    print ('\nTraining set\n------------')
    print ('N: ' + (str(len(X_internal))))
    print ('R2: %0.4f'%(R2_train_mean))
    print ('std_R2: %0.4f'%(R2_train_std))
    print ('RMSE: %0.4f'%(RMSE_train_mean))
    print ('std_RMSE: %0.4f'%(RMSE_train_std))

    print ('\nCross-validation set\n------------')
    print ('N: ' + (str(len(X_internal))))
    print ('Q2: %0.4f'%(Q2_CV_mean))
    print ('std_Q2: %0.4f'%(Q2_CV_std))
    print ('RMSE: %0.4f'%(RMSE_CV_mean))
    print ('std_RMSE: %0.4f'%(RMSE_CV_std))

    print ('\nExternal set\n------------')
    print ('N: ' + (str(len(X_external))))
    print ('Q2_EXt: %0.4f'%(Q2_External_mean))
    print ('std_Q2_EXt: %0.4f'%(Q2_External_std))
    print ('RMSE: %0.4f'%(RMSE_External_mean))
    print ('std_RMSE: %0.4f'%(RMSE_External_std))
141/188:
def mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, importances_dict):
    R2_train_mean = np.mean(R2_train)
    RMSE_train_mean = np.mean(RMSE_train)
    Q2_CV_mean = np.mean(Q2_CV)
    RMSE_CV_mean = np.mean(RMSE_CV)
    Q2_External_mean = np.mean(Q2_External)
    RMSE_External_mean = np.mean(RMSE_External)
    importances_mean0 = {}
    for fx in importances_dict:
        importances_mean0[fx] = np.mean(importances_dict[fx])
    importances_mean = sorted([(k,v) for k,v in importances_mean0.iteritems()],
                                    key=lambda x: x[1], reverse=True)
    
    #predictionOnTest_mean = np.mean(acclist_predictionOnTest_scrambledtrain)
    #predictionOnTrain_mean = np.mean(acclist_predictionOnTrain_scrambledtrain)
    
    return R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \
            RMSE_External_mean, importances_mean
141/189:
from copy import deepcopy

def Y_scrambling(X_internal, X_external, Y_internal, Y_external):
    # Do the Y-scrambling. Loop over the actual learning for 100 times.
    for randomseedcounter in range(1,101):
        y_train_scrambled = deepcopy(Y_internal)
        X_train_scrambled = deepcopy(X_internal)
        np.random.shuffle(y_train_scrambled)
        np.random.shuffle(X_train_scrambled)

        # training was done on "scrambled" data - prediction on test set
        RF_scrambled         = RandomForestRegressor()
        RF_scrambled         = RF_scrambled.fit(X_internal,y_train_scrambled)
        y_predict_scrambled  = RF_scrambled.predict(X_external)
    
        acclist_predictionOnTest_scrambledtrain.append((RF_scrambled.score(X_external,Y_external))**2)
    
        # training was done on "scrambled" data - prediction on train set
        y_predict_scrambled_predictTrain  = RF_scrambled.predict(X_internal)
    
        acclist_predictionOnTrain_scrambledtrain.append((RF_scrambled.score(X_internal,Y_internal))**2)
        
        r2 = pd.DataFrame(acclist_predictionOnTrain_scrambledtrain, columns=['R2'])
        q2 = pd.DataFrame(acclist_predictionOnTest_scrambledtrain, columns=['Q2'])

        #result = pd.concat([r2, q2], axis=1, join='inner').to_csv(f+"_Y_scrambling.csv", header=False, index=False)
        
        
    return acclist_predictionOnTest_scrambledtrain,acclist_predictionOnTrain_scrambledtrain
141/190:
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict

def build_model(X, Y, seed, hx, f):
    
    
    #Data split using 70/30 ratio
    X_internal, X_external, Y_internal, Y_external = train_test_split(X,
                                                        Y, test_size=0.3,
                                                        random_state=seed)

    # Training set
    rf = RandomForestRegressor(n_estimators=400, 
                               max_features='sqrt', 
                               min_samples_leaf = 1,
                               random_state = 13,
                               n_jobs=-1)
    
    rf.fit(X_internal,Y_internal)
    prediction = rf.predict(X_internal)
    
    # Cross-validation
    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)
    
    # External set  
    prediction_external = rf.predict(X_external)
    
    #print result from each seed    
    R2_train.append(r2_score(Y_internal, prediction))
    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))
    Q2_CV.append(r2_score(Y_internal, cv))
    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))
    Q2_External.append(r2_score(Y_external, prediction_external))
    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))
    
    #Feature Importance
    Feature = hx[:]
    feature_importance = rf.feature_importances_
    importances = 100.0 * (feature_importance / feature_importance.max()) #index
    
    for i, fx in enumerate(Feature):
        importances_dict(fx).append(importances[i])
    
    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict
141/191:
import glob # to read multiple files 
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem
import os

%config InlineBackend.figure_format = 'retina'

! rm Result/Result_LogKp.csv

outfile = open('Result/Result_LogKp.csv', 'a')

print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\
                    'R2_train,R2_train_std,' + \
                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \
                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')

path = r'QSAR/'

for f in glob.glob(path + '*.csv'):
    df = pd.read_csv(f)
    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))
    df = df.fillna(method='ffill')
    Y = df["LogKp"].values.astype(np.float)
    data = df
    
    print ('\n\n************************************************************************************')
    print ('')
    print (f)
    print ('')
    
    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%
    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7

    h = data.columns.tolist()
    hx = np.array(h)

    data = data.values.astype(np.float)
    X = np.array(data)
    
    # Prepare empty lists to plot QSAR model
    R2_train = []
    RMSE_train = []
    Q2_CV = []
    RMSE_CV = []
    Q2_External = []
    RMSE_External = []
    importances_dict = defaultdict(list)
    
    # Prepare empty lists to plot the performance of accuracy.
    acclist_realRF                          = []
    acclist_realRF_predictTrain             = []
    acclist_predictionOnTest_scrambledtrain  = []
    acclist_predictionOnTrain_scrambledtrain = []
        
    for i in range(10):
        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)
            
    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \
                                                                        X_internal, X_external, Y_internal, Y_external)
    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \
        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\
        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, 
           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, 
           RMSE_External_std, X_internal, X_external, X, f, des1, des2)
    plot_model(f, X_internal, X_external, Y_internal, Y_external,
                R2_train_mean, Q2_External_mean,
                importances_mean, importances_std, Feature, prediction,
                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)
    
outfile.close()
    #END
141/192:
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict

def build_model(X, Y, seed, hx, f):
    
    
    #Data split using 70/30 ratio
    X_internal, X_external, Y_internal, Y_external = train_test_split(X,
                                                        Y, test_size=0.3,
                                                        random_state=seed)

    # Training set
    rf = RandomForestRegressor(n_estimators=400, 
                               max_features='sqrt', 
                               min_samples_leaf = 1,
                               random_state = 13,
                               n_jobs=-1)
    
    rf.fit(X_internal,Y_internal)
    prediction = rf.predict(X_internal)
    
    # Cross-validation
    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)
    
    # External set  
    prediction_external = rf.predict(X_external)
    
    #print result from each seed    
    R2_train.append(r2_score(Y_internal, prediction))
    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))
    Q2_CV.append(r2_score(Y_internal, cv))
    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))
    Q2_External.append(r2_score(Y_external, prediction_external))
    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))
    
    #Feature Importance
    Feature = hx[:]
    feature_importance = rf.feature_importances_
    importances = 100.0 * (feature_importance / feature_importance.max()) #index
    
    for i, fx in enumerate(Feature):
        importances_dict[fx] =  importances[i]
    
    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict
141/193:
import glob # to read multiple files 
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem
import os

%config InlineBackend.figure_format = 'retina'

! rm Result/Result_LogKp.csv

outfile = open('Result/Result_LogKp.csv', 'a')

print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\
                    'R2_train,R2_train_std,' + \
                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \
                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')

path = r'QSAR/'

for f in glob.glob(path + '*.csv'):
    df = pd.read_csv(f)
    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))
    df = df.fillna(method='ffill')
    Y = df["LogKp"].values.astype(np.float)
    data = df
    
    print ('\n\n************************************************************************************')
    print ('')
    print (f)
    print ('')
    
    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%
    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7

    h = data.columns.tolist()
    hx = np.array(h)

    data = data.values.astype(np.float)
    X = np.array(data)
    
    # Prepare empty lists to plot QSAR model
    R2_train = []
    RMSE_train = []
    Q2_CV = []
    RMSE_CV = []
    Q2_External = []
    RMSE_External = []
    importances_dict = defaultdict(list)
    
    # Prepare empty lists to plot the performance of accuracy.
    acclist_realRF                          = []
    acclist_realRF_predictTrain             = []
    acclist_predictionOnTest_scrambledtrain  = []
    acclist_predictionOnTrain_scrambledtrain = []
        
    for i in range(10):
        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)
            
    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \
                                                                        X_internal, X_external, Y_internal, Y_external)
    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \
        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\
        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, 
           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, 
           RMSE_External_std, X_internal, X_external, X, f, des1, des2)
    plot_model(f, X_internal, X_external, Y_internal, Y_external,
                R2_train_mean, Q2_External_mean,
                importances_mean, importances_std, Feature, prediction,
                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)
    
outfile.close()
    #END
141/194:
import matplotlib.pyplot as plt
from matplotlib.ticker import MultipleLocator
import pylab as py 
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import summary_table

def plot_model (f, X_internal, X_external, Y_internal, Y_external,
                R2_train_mean, Q2_External_mean,
                importances_mean, importances_std, Feature, prediction, 
                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain):
    
    # Prepare plot
    m  = rf.fit(X_internal,Y_internal)
    cm = plt.cm.RdBu
    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)

    fig_size = plt.rcParams["figure.figsize"]
    fig_size[0]=5
    fig_size[1]=5

    # Train Set
    x_train = np.array(Y_internal)
    y_train = m.predict(X_internal).flatten()
    py.scatter(x_train, y_train, s=50, marker='.', alpha=0.3,
            c='g', cmap=cm ,edgecolors='g')
                 #label=r"$R^{2}_{Tr}$ = %.4f" % R2_train_mean)
    
    
    # CV Set
    np.array(cv)
    x_test = np.array(Y_internal)
    y_test = cv
    py.scatter(x_test, y_test, s=50, marker='.', alpha=0.3,
            c='b', cmap=cm ,edgecolors='b') 
                 #label=r"$Q^{2}_{Ext}$ = %.4f" % Q2_External_mean)
        #2SD line
    X = sm.add_constant(x_test)
    res = sm.OLS(y_test, X).fit()

    st, data, ss2 = summary_table(res, alpha=0.05)
    fittedvalues = data[:,2]
        #predict_mean_se  = data[:,3]
    predict_mean_ci_low, predict_mean_ci_upp = data[:,4:6].T
    predict_ci_low, predict_ci_upp = data[:,6:8].T
        #2SD line
    plt.plot(X, predict_ci_low, '--b', linewidth=0.5, alpha=0.5)
    plt.plot(X, predict_ci_upp, '--b', linewidth=0.5, alpha=0.5)
        
    # External Set
    x_test = np.array(Y_external)
    y_test = m.predict(X_external).flatten()
    py.scatter(x_test, y_test, s=50, marker='.', alpha=0.3,
            c='r', cmap=cm ,edgecolors='r') 
                 #label=r"$Q^{2}_{Ext}$ = %.4f" % Q2_External_mean)
    
    
    #py.plot(x_train, np.polyval(np.polyfit(x_train,y_train,1), x_train), '--r') #mean line
    plt.legend(loc=2,prop={'size':6})
    plt.xlabel("Experimental $pIC_{50}$ values", fontsize=10)
    plt.ylabel("Predicted $pIC_{50}$ values", fontsize=10)
        
    min_axis = np.min(np.concatenate([Y_internal, prediction], axis=0))
    max_axis = np.max(np.concatenate([Y_internal, prediction], axis=0))
    plt.xlim([(min_axis*0.9),(max_axis*1.05)])
    plt.ylim([(min_axis*0.9),(max_axis*1.05)])
    plt.tick_params(axis='both', which='major', labelsize=14)

    
    # Save plot to file
    plt.savefig(f+'_compair_Fp.pdf', dpi=300)
    plt.show()
    
    # Y-scrambling plot
    py.scatter(Q2_CV_mean, R2_train_mean, s=100, marker='.', alpha=0.3,
            c='b', cmap=cm ,edgecolors='b') 
            
    py.scatter(acclist_predictionOnTest_scrambledtrain,acclist_predictionOnTrain_scrambledtrain, \
               s=100, marker='.', alpha=0.3, c='r', cmap=cm ,edgecolors='r')

    plt.legend(loc=2,prop={'size':6})
    plt.xlabel("$Q^{2}$", fontsize=10)
    plt.ylabel("$R^{2}$", fontsize=10)
    plt.tick_params(axis='both', which='major', labelsize=14)
    
    plt.ylim([0, 1])
    plt.xlim([0, 1])
    plt.axhline(0.5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
    plt.axvline(0.5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
    
    plt.savefig(f+'_Y_scrambling.pdf', dpi=300)

    plt.show()

    #Feature Importance 
    fig_size = plt.rcParams["figure.figsize"]
    fig_size[0]=5
    fig_size[1]=10

    barlist = plt.barh(range(20), [x[1] for x in importances_mean[:20]], 
          color="g", xerr=[x[1] for x in importances_std[:20]], align="center", \
                       error_kw=dict(ecolor='gray', lw=2, capsize=5, capthick=2))

    print (str("top10"), [x[0] for x in importances_mean[:10]])
    print ('')
    print (str("top20"), [x[0] for x in importances_mean[:20]])
    print ('')
    print (str("top30"), [x[0] for x in importances_mean[:30]])
    print ('')
    print (str("top40"), [x[0] for x in importances_mean[:40]])
    print ('')
    print (str("top50"), [x[0] for x in importances_mean[:50]])
    
    plt.yticks(range(20), [x[0] for x in importances_mean[:20]])
    plt.ylim([-1, 20])
    
    plt.xlabel(r"$\bf{Gini}$" + " "+ r"$\bf{index}$", fontsize=12)
    ax = plt.gca()
    ax.invert_yaxis()
    plt.tight_layout(pad=2.0, w_pad=0.7, h_pad=2.0)
    plt.savefig(f+'_Feature_importances.pdf', dpi=300)
    plt.show()
141/195:
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict

def build_model(X, Y, seed, hx, f):
    
    
    #Data split using 70/30 ratio
    X_internal, X_external, Y_internal, Y_external = train_test_split(X,
                                                        Y, test_size=0.3,
                                                        random_state=seed)

    # Training set
    rf = RandomForestRegressor(n_estimators=400, 
                               max_features='sqrt', 
                               min_samples_leaf = 1,
                               random_state = 13,
                               n_jobs=-1)
    
    rf.fit(X_internal,Y_internal)
    prediction = rf.predict(X_internal)
    
    # Cross-validation
    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)
    
    # External set  
    prediction_external = rf.predict(X_external)
    
    #print result from each seed    
    R2_train.append(r2_score(Y_internal, prediction))
    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))
    Q2_CV.append(r2_score(Y_internal, cv))
    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))
    Q2_External.append(r2_score(Y_external, prediction_external))
    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))
    
    #Feature Importance
    Feature = hx[:]
    feature_importance = rf.feature_importances_
    importances = 100.0 * (feature_importance / feature_importance.max()) #index
    
    for i, fx in enumerate(Feature):
        importances[fx].append(importances[i])
    
    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict
141/196:
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict

def build_model(X, Y, seed, hx, f):
    
    
    #Data split using 70/30 ratio
    X_internal, X_external, Y_internal, Y_external = train_test_split(X,
                                                        Y, test_size=0.3,
                                                        random_state=seed)

    # Training set
    rf = RandomForestRegressor(n_estimators=400, 
                               max_features='sqrt', 
                               min_samples_leaf = 1,
                               random_state = 13,
                               n_jobs=-1)
    
    rf.fit(X_internal,Y_internal)
    prediction = rf.predict(X_internal)
    
    # Cross-validation
    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)
    
    # External set  
    prediction_external = rf.predict(X_external)
    
    #print result from each seed    
    R2_train.append(r2_score(Y_internal, prediction))
    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))
    Q2_CV.append(r2_score(Y_internal, cv))
    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))
    Q2_External.append(r2_score(Y_external, prediction_external))
    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))
    
    #Feature Importance
    Feature = hx[:]
    feature_importance = rf.feature_importances_
    importances = 100.0 * (feature_importance / feature_importance.max()) #index
    
    for i, fx in enumerate(Feature):
        importances[fx].astype(np.float)
        importances[fx].append(importances[i])
    
    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict
141/197:
import glob # to read multiple files 
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem
import os

%config InlineBackend.figure_format = 'retina'

! rm Result/Result_LogKp.csv

outfile = open('Result/Result_LogKp.csv', 'a')

print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\
                    'R2_train,R2_train_std,' + \
                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \
                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')

path = r'QSAR/'

for f in glob.glob(path + '*.csv'):
    df = pd.read_csv(f)
    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))
    df = df.fillna(method='ffill')
    Y = df["LogKp"].values.astype(np.float)
    data = df
    
    print ('\n\n************************************************************************************')
    print ('')
    print (f)
    print ('')
    
    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%
    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7

    h = data.columns.tolist()
    hx = np.array(h)

    data = data.values.astype(np.float)
    X = np.array(data)
    
    # Prepare empty lists to plot QSAR model
    R2_train = []
    RMSE_train = []
    Q2_CV = []
    RMSE_CV = []
    Q2_External = []
    RMSE_External = []
    importances_dict = defaultdict(list)
    
    # Prepare empty lists to plot the performance of accuracy.
    acclist_realRF                          = []
    acclist_realRF_predictTrain             = []
    acclist_predictionOnTest_scrambledtrain  = []
    acclist_predictionOnTrain_scrambledtrain = []
        
    for i in range(10):
        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)
            
    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \
                                                                        X_internal, X_external, Y_internal, Y_external)
    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \
        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\
        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, 
           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, 
           RMSE_External_std, X_internal, X_external, X, f, des1, des2)
    plot_model(f, X_internal, X_external, Y_internal, Y_external,
                R2_train_mean, Q2_External_mean,
                importances_mean, importances_std, Feature, prediction,
                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)
    
outfile.close()
    #END
141/198:
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict

def build_model(X, Y, seed, hx, f):
    
    
    #Data split using 70/30 ratio
    X_internal, X_external, Y_internal, Y_external = train_test_split(X,
                                                        Y, test_size=0.3,
                                                        random_state=seed)

    # Training set
    rf = RandomForestRegressor(n_estimators=400, 
                               max_features='sqrt', 
                               min_samples_leaf = 1,
                               random_state = 13,
                               n_jobs=-1)
    
    rf.fit(X_internal,Y_internal)
    prediction = rf.predict(X_internal)
    
    # Cross-validation
    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)
    
    # External set  
    prediction_external = rf.predict(X_external)
    
    #print result from each seed    
    R2_train.append(r2_score(Y_internal, prediction))
    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))
    Q2_CV.append(r2_score(Y_internal, cv))
    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))
    Q2_External.append(r2_score(Y_external, prediction_external))
    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))
    
    #Feature Importance
    Feature = hx[:]
    feature_importance = rf.feature_importances_
    importances = 100.0 * (feature_importance / feature_importance.max()) #index
    
    for i, fx in enumerate(Feature):
        importances[fx].astype(int)
        importances[fx].append(importances[i])
    
    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict
141/199:
import glob # to read multiple files 
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem
import os

%config InlineBackend.figure_format = 'retina'

! rm Result/Result_LogKp.csv

outfile = open('Result/Result_LogKp.csv', 'a')

print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\
                    'R2_train,R2_train_std,' + \
                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \
                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')

path = r'QSAR/'

for f in glob.glob(path + '*.csv'):
    df = pd.read_csv(f)
    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))
    df = df.fillna(method='ffill')
    Y = df["LogKp"].values.astype(np.float)
    data = df
    
    print ('\n\n************************************************************************************')
    print ('')
    print (f)
    print ('')
    
    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%
    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7

    h = data.columns.tolist()
    hx = np.array(h)

    data = data.values.astype(np.float)
    X = np.array(data)
    
    # Prepare empty lists to plot QSAR model
    R2_train = []
    RMSE_train = []
    Q2_CV = []
    RMSE_CV = []
    Q2_External = []
    RMSE_External = []
    importances_dict = defaultdict(list)
    
    # Prepare empty lists to plot the performance of accuracy.
    acclist_realRF                          = []
    acclist_realRF_predictTrain             = []
    acclist_predictionOnTest_scrambledtrain  = []
    acclist_predictionOnTrain_scrambledtrain = []
        
    for i in range(10):
        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)
            
    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \
                                                                        X_internal, X_external, Y_internal, Y_external)
    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \
        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\
        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, 
           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, 
           RMSE_External_std, X_internal, X_external, X, f, des1, des2)
    plot_model(f, X_internal, X_external, Y_internal, Y_external,
                R2_train_mean, Q2_External_mean,
                importances_mean, importances_std, Feature, prediction,
                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)
    
outfile.close()
    #END
141/200:
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict

def build_model(X, Y, seed, hx, f):
    
    
    #Data split using 70/30 ratio
    X_internal, X_external, Y_internal, Y_external = train_test_split(X,
                                                        Y, test_size=0.3,
                                                        random_state=seed)

    # Training set
    rf = RandomForestRegressor(n_estimators=400, 
                               max_features='sqrt', 
                               min_samples_leaf = 1,
                               random_state = 13,
                               n_jobs=-1)
    
    rf.fit(X_internal,Y_internal)
    prediction = rf.predict(X_internal)
    
    # Cross-validation
    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)
    
    # External set  
    prediction_external = rf.predict(X_external)
    
    #print result from each seed    
    R2_train.append(r2_score(Y_internal, prediction))
    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))
    Q2_CV.append(r2_score(Y_internal, cv))
    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))
    Q2_External.append(r2_score(Y_external, prediction_external))
    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))
    
    #Feature Importance
    Feature = hx[:]
    feature_importance = rf.feature_importances_
    importances = 100.0 * (feature_importance / feature_importance.max()) #index
    
    for i, fx in enumerate(Feature):
        importances_dict[fx] = importances[i]
    
    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict
141/201:
import glob # to read multiple files 
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem
import os

%config InlineBackend.figure_format = 'retina'

! rm Result/Result_LogKp.csv

outfile = open('Result/Result_LogKp.csv', 'a')

print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\
                    'R2_train,R2_train_std,' + \
                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \
                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')

path = r'QSAR/'

for f in glob.glob(path + '*.csv'):
    df = pd.read_csv(f)
    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))
    df = df.fillna(method='ffill')
    Y = df["LogKp"].values.astype(np.float)
    data = df
    
    print ('\n\n************************************************************************************')
    print ('')
    print (f)
    print ('')
    
    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%
    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7

    h = data.columns.tolist()
    hx = np.array(h)

    data = data.values.astype(np.float)
    X = np.array(data)
    
    # Prepare empty lists to plot QSAR model
    R2_train = []
    RMSE_train = []
    Q2_CV = []
    RMSE_CV = []
    Q2_External = []
    RMSE_External = []
    importances_dict = defaultdict(list)
    
    # Prepare empty lists to plot the performance of accuracy.
    acclist_realRF                          = []
    acclist_realRF_predictTrain             = []
    acclist_predictionOnTest_scrambledtrain  = []
    acclist_predictionOnTrain_scrambledtrain = []
        
    for i in range(10):
        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)
            
    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \
                                                                        X_internal, X_external, Y_internal, Y_external)
    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \
        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\
        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, 
           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, 
           RMSE_External_std, X_internal, X_external, X, f, des1, des2)
    plot_model(f, X_internal, X_external, Y_internal, Y_external,
                R2_train_mean, Q2_External_mean,
                importances_mean, importances_std, Feature, prediction,
                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)
    
outfile.close()
    #END
141/202:
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict

def build_model(X, Y, seed, hx, f):
    
    
    #Data split using 70/30 ratio
    X_internal, X_external, Y_internal, Y_external = train_test_split(X,
                                                        Y, test_size=0.3,
                                                        random_state=seed)

    # Training set
    rf = RandomForestRegressor(n_estimators=400, 
                               max_features='sqrt', 
                               min_samples_leaf = 1,
                               random_state = 13,
                               n_jobs=-1)
    
    rf.fit(X_internal,Y_internal)
    prediction = rf.predict(X_internal)
    
    # Cross-validation
    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)
    
    # External set  
    prediction_external = rf.predict(X_external)
    
    #print result from each seed    
    R2_train.append(r2_score(Y_internal, prediction))
    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))
    Q2_CV.append(r2_score(Y_internal, cv))
    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))
    Q2_External.append(r2_score(Y_external, prediction_external))
    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))
    
    #Feature Importance
    Feature = hx[:]
    feature_importance = rf.feature_importances_
    importances = 100.0 * (feature_importance / feature_importance.max()) #index
    
    for i, fx in enumerate(Feature):
        print(type(importances_dict))
        importances_dict[fx] = importances[i]
       
    
    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict
141/203:
import glob # to read multiple files 
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem
import os

%config InlineBackend.figure_format = 'retina'

! rm Result/Result_LogKp.csv

outfile = open('Result/Result_LogKp.csv', 'a')

print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\
                    'R2_train,R2_train_std,' + \
                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \
                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')

path = r'QSAR/'

for f in glob.glob(path + '*.csv'):
    df = pd.read_csv(f)
    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))
    df = df.fillna(method='ffill')
    Y = df["LogKp"].values.astype(np.float)
    data = df
    
    print ('\n\n************************************************************************************')
    print ('')
    print (f)
    print ('')
    
    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%
    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7

    h = data.columns.tolist()
    hx = np.array(h)

    data = data.values.astype(np.float)
    X = np.array(data)
    
    # Prepare empty lists to plot QSAR model
    R2_train = []
    RMSE_train = []
    Q2_CV = []
    RMSE_CV = []
    Q2_External = []
    RMSE_External = []
    importances_dict = defaultdict(list)
    
    # Prepare empty lists to plot the performance of accuracy.
    acclist_realRF                          = []
    acclist_realRF_predictTrain             = []
    acclist_predictionOnTest_scrambledtrain  = []
    acclist_predictionOnTrain_scrambledtrain = []
        
    for i in range(10):
        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)
            
    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \
                                                                        X_internal, X_external, Y_internal, Y_external)
    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \
        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\
        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, 
           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, 
           RMSE_External_std, X_internal, X_external, X, f, des1, des2)
    plot_model(f, X_internal, X_external, Y_internal, Y_external,
                R2_train_mean, Q2_External_mean,
                importances_mean, importances_std, Feature, prediction,
                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)
    
outfile.close()
    #END
141/204:
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict

def build_model(X, Y, seed, hx, f):
    
    
    #Data split using 70/30 ratio
    X_internal, X_external, Y_internal, Y_external = train_test_split(X,
                                                        Y, test_size=0.3,
                                                        random_state=seed)

    # Training set
    rf = RandomForestRegressor(n_estimators=400, 
                               max_features='sqrt', 
                               min_samples_leaf = 1,
                               random_state = 13,
                               n_jobs=-1)
    
    rf.fit(X_internal,Y_internal)
    prediction = rf.predict(X_internal)
    
    # Cross-validation
    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)
    
    # External set  
    prediction_external = rf.predict(X_external)
    
    #print result from each seed    
    R2_train.append(r2_score(Y_internal, prediction))
    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))
    Q2_CV.append(r2_score(Y_internal, cv))
    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))
    Q2_External.append(r2_score(Y_external, prediction_external))
    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))
    
    #Feature Importance
    Feature = hx[:]
    feature_importance = rf.feature_importances_
    importances = 100.0 * (feature_importance / feature_importance.max()) #index
    
    for i, fx in enumerate(Feature):
        
        importances_dict[fx] += importances[i]
       
    
    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict
141/205:
import glob # to read multiple files 
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem
import os

%config InlineBackend.figure_format = 'retina'

! rm Result/Result_LogKp.csv

outfile = open('Result/Result_LogKp.csv', 'a')

print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\
                    'R2_train,R2_train_std,' + \
                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \
                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')

path = r'QSAR/'

for f in glob.glob(path + '*.csv'):
    df = pd.read_csv(f)
    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))
    df = df.fillna(method='ffill')
    Y = df["LogKp"].values.astype(np.float)
    data = df
    
    print ('\n\n************************************************************************************')
    print ('')
    print (f)
    print ('')
    
    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%
    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7

    h = data.columns.tolist()
    hx = np.array(h)

    data = data.values.astype(np.float)
    X = np.array(data)
    
    # Prepare empty lists to plot QSAR model
    R2_train = []
    RMSE_train = []
    Q2_CV = []
    RMSE_CV = []
    Q2_External = []
    RMSE_External = []
    importances_dict = defaultdict(list)
    
    # Prepare empty lists to plot the performance of accuracy.
    acclist_realRF                          = []
    acclist_realRF_predictTrain             = []
    acclist_predictionOnTest_scrambledtrain  = []
    acclist_predictionOnTrain_scrambledtrain = []
        
    for i in range(10):
        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)
            
    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \
                                                                        X_internal, X_external, Y_internal, Y_external)
    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \
        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\
        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, 
           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, 
           RMSE_External_std, X_internal, X_external, X, f, des1, des2)
    plot_model(f, X_internal, X_external, Y_internal, Y_external,
                R2_train_mean, Q2_External_mean,
                importances_mean, importances_std, Feature, prediction,
                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)
    
outfile.close()
    #END
141/206:
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict

def build_model(X, Y, seed, hx, f):
    
    
    #Data split using 70/30 ratio
    X_internal, X_external, Y_internal, Y_external = train_test_split(X,
                                                        Y, test_size=0.3,
                                                        random_state=seed)

    # Training set
    rf = RandomForestRegressor(n_estimators=400, 
                               max_features='sqrt', 
                               min_samples_leaf = 1,
                               random_state = 13,
                               n_jobs=-1)
    
    rf.fit(X_internal,Y_internal)
    prediction = rf.predict(X_internal)
    
    # Cross-validation
    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)
    
    # External set  
    prediction_external = rf.predict(X_external)
    
    #print result from each seed    
    R2_train.append(r2_score(Y_internal, prediction))
    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))
    Q2_CV.append(r2_score(Y_internal, cv))
    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))
    Q2_External.append(r2_score(Y_external, prediction_external))
    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))
    
    #Feature Importance
    Feature = hx[:]
    feature_importance = rf.feature_importances_
    importances = 100.0 * (feature_importance / feature_importance.max()) #index
    
    for i, fx in enumerate(Feature):
        
        importances_dict[fx] += importances
       
    
    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict
141/207:
import glob # to read multiple files 
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem
import os

%config InlineBackend.figure_format = 'retina'

! rm Result/Result_LogKp.csv

outfile = open('Result/Result_LogKp.csv', 'a')

print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\
                    'R2_train,R2_train_std,' + \
                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \
                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')

path = r'QSAR/'

for f in glob.glob(path + '*.csv'):
    df = pd.read_csv(f)
    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))
    df = df.fillna(method='ffill')
    Y = df["LogKp"].values.astype(np.float)
    data = df
    
    print ('\n\n************************************************************************************')
    print ('')
    print (f)
    print ('')
    
    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%
    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7

    h = data.columns.tolist()
    hx = np.array(h)

    data = data.values.astype(np.float)
    X = np.array(data)
    
    # Prepare empty lists to plot QSAR model
    R2_train = []
    RMSE_train = []
    Q2_CV = []
    RMSE_CV = []
    Q2_External = []
    RMSE_External = []
    importances_dict = defaultdict(list)
    
    # Prepare empty lists to plot the performance of accuracy.
    acclist_realRF                          = []
    acclist_realRF_predictTrain             = []
    acclist_predictionOnTest_scrambledtrain  = []
    acclist_predictionOnTrain_scrambledtrain = []
        
    for i in range(10):
        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)
            
    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \
                                                                        X_internal, X_external, Y_internal, Y_external)
    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \
        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\
        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, 
           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, 
           RMSE_External_std, X_internal, X_external, X, f, des1, des2)
    plot_model(f, X_internal, X_external, Y_internal, Y_external,
                R2_train_mean, Q2_External_mean,
                importances_mean, importances_std, Feature, prediction,
                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)
    
outfile.close()
    #END
141/208:
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict

def build_model(X, Y, seed, hx, f):
    
    
    #Data split using 70/30 ratio
    X_internal, X_external, Y_internal, Y_external = train_test_split(X,
                                                        Y, test_size=0.3,
                                                        random_state=seed)

    # Training set
    rf = RandomForestRegressor(n_estimators=400, 
                               max_features='sqrt', 
                               min_samples_leaf = 1,
                               random_state = 13,
                               n_jobs=-1)
    
    rf.fit(X_internal,Y_internal)
    prediction = rf.predict(X_internal)
    
    # Cross-validation
    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)
    
    # External set  
    prediction_external = rf.predict(X_external)
    
    #print result from each seed    
    R2_train.append(r2_score(Y_internal, prediction))
    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))
    Q2_CV.append(r2_score(Y_internal, cv))
    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))
    Q2_External.append(r2_score(Y_external, prediction_external))
    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))
    
    #Feature Importance
    Feature = hx[:]
    feature_importance = rf.feature_importances_
    importances = 100.0 * (feature_importance / feature_importance.max()) #index
    
    for i, fx in enumerate(Feature):
        
        importances_dict[fx] = importances(i)
       
    
    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict
141/209:
import glob # to read multiple files 
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem
import os

%config InlineBackend.figure_format = 'retina'

! rm Result/Result_LogKp.csv

outfile = open('Result/Result_LogKp.csv', 'a')

print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\
                    'R2_train,R2_train_std,' + \
                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \
                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')

path = r'QSAR/'

for f in glob.glob(path + '*.csv'):
    df = pd.read_csv(f)
    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))
    df = df.fillna(method='ffill')
    Y = df["LogKp"].values.astype(np.float)
    data = df
    
    print ('\n\n************************************************************************************')
    print ('')
    print (f)
    print ('')
    
    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%
    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7

    h = data.columns.tolist()
    hx = np.array(h)

    data = data.values.astype(np.float)
    X = np.array(data)
    
    # Prepare empty lists to plot QSAR model
    R2_train = []
    RMSE_train = []
    Q2_CV = []
    RMSE_CV = []
    Q2_External = []
    RMSE_External = []
    importances_dict = defaultdict(list)
    
    # Prepare empty lists to plot the performance of accuracy.
    acclist_realRF                          = []
    acclist_realRF_predictTrain             = []
    acclist_predictionOnTest_scrambledtrain  = []
    acclist_predictionOnTrain_scrambledtrain = []
        
    for i in range(10):
        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)
            
    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \
                                                                        X_internal, X_external, Y_internal, Y_external)
    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \
        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\
        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, 
           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, 
           RMSE_External_std, X_internal, X_external, X, f, des1, des2)
    plot_model(f, X_internal, X_external, Y_internal, Y_external,
                R2_train_mean, Q2_External_mean,
                importances_mean, importances_std, Feature, prediction,
                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)
    
outfile.close()
    #END
141/210:
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict

def build_model(X, Y, seed, hx, f):
    
    
    #Data split using 70/30 ratio
    X_internal, X_external, Y_internal, Y_external = train_test_split(X,
                                                        Y, test_size=0.3,
                                                        random_state=seed)

    # Training set
    rf = RandomForestRegressor(n_estimators=400, 
                               max_features='sqrt', 
                               min_samples_leaf = 1,
                               random_state = 13,
                               n_jobs=-1)
    
    rf.fit(X_internal,Y_internal)
    prediction = rf.predict(X_internal)
    
    # Cross-validation
    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)
    
    # External set  
    prediction_external = rf.predict(X_external)
    
    #print result from each seed    
    R2_train.append(r2_score(Y_internal, prediction))
    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))
    Q2_CV.append(r2_score(Y_internal, cv))
    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))
    Q2_External.append(r2_score(Y_external, prediction_external))
    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))
    
    #Feature Importance
    Feature = hx[:]
    feature_importance = rf.feature_importances_
    importances = 100.0 * (feature_importance / feature_importance.max()) #index
    
    for i, fx in enumerate(Feature):
        
        importances_dict[fx].append(importances[i])
       
    
    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict
141/211:
import glob # to read multiple files 
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem
import os

%config InlineBackend.figure_format = 'retina'

! rm Result/Result_LogKp.csv

outfile = open('Result/Result_LogKp.csv', 'a')

print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\
                    'R2_train,R2_train_std,' + \
                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \
                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')

path = r'QSAR/'

for f in glob.glob(path + '*.csv'):
    df = pd.read_csv(f)
    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))
    df = df.fillna(method='ffill')
    Y = df["LogKp"].values.astype(np.float)
    data = df
    
    print ('\n\n************************************************************************************')
    print ('')
    print (f)
    print ('')
    
    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%
    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7

    h = data.columns.tolist()
    hx = np.array(h)

    data = data.values.astype(np.float)
    X = np.array(data)
    
    # Prepare empty lists to plot QSAR model
    R2_train = []
    RMSE_train = []
    Q2_CV = []
    RMSE_CV = []
    Q2_External = []
    RMSE_External = []
    importances_dict = defaultdict(list)
    
    # Prepare empty lists to plot the performance of accuracy.
    acclist_realRF                          = []
    acclist_realRF_predictTrain             = []
    acclist_predictionOnTest_scrambledtrain  = []
    acclist_predictionOnTrain_scrambledtrain = []
        
    for i in range(10):
        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)
            
    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \
                                                                        X_internal, X_external, Y_internal, Y_external)
    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \
        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\
        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, 
           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, 
           RMSE_External_std, X_internal, X_external, X, f, des1, des2)
    plot_model(f, X_internal, X_external, Y_internal, Y_external,
                R2_train_mean, Q2_External_mean,
                importances_mean, importances_std, Feature, prediction,
                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)
    
outfile.close()
    #END
141/212:
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict

def build_model(X, Y, seed, hx, f):
    
    
    #Data split using 70/30 ratio
    X_internal, X_external, Y_internal, Y_external = train_test_split(X,
                                                        Y, test_size=0.3,
                                                        random_state=seed)

    # Training set
    rf = RandomForestRegressor(n_estimators=400, 
                               max_features='sqrt', 
                               min_samples_leaf = 1,
                               random_state = 13,
                               n_jobs=-1)
    
    rf.fit(X_internal,Y_internal)
    prediction = rf.predict(X_internal)
    
    # Cross-validation
    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)
    
    # External set  
    prediction_external = rf.predict(X_external)
    
    #print result from each seed    
    R2_train.append(r2_score(Y_internal, prediction))
    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))
    Q2_CV.append(r2_score(Y_internal, cv))
    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))
    Q2_External.append(r2_score(Y_external, prediction_external))
    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))
    
    #Feature Importance
    Feature = hx[:]
    feature_importance = rf.feature_importances_
    importances = 100.0 * (feature_importance / feature_importance.max()) #index
    
    for i, fx in enumerate(Feature):
        
        hash(tuple(importances_dict[fx])).append(importances[i])
       
    
    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict
141/213:
import glob # to read multiple files 
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem
import os

%config InlineBackend.figure_format = 'retina'

! rm Result/Result_LogKp.csv

outfile = open('Result/Result_LogKp.csv', 'a')

print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\
                    'R2_train,R2_train_std,' + \
                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \
                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')

path = r'QSAR/'

for f in glob.glob(path + '*.csv'):
    df = pd.read_csv(f)
    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))
    df = df.fillna(method='ffill')
    Y = df["LogKp"].values.astype(np.float)
    data = df
    
    print ('\n\n************************************************************************************')
    print ('')
    print (f)
    print ('')
    
    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%
    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7

    h = data.columns.tolist()
    hx = np.array(h)

    data = data.values.astype(np.float)
    X = np.array(data)
    
    # Prepare empty lists to plot QSAR model
    R2_train = []
    RMSE_train = []
    Q2_CV = []
    RMSE_CV = []
    Q2_External = []
    RMSE_External = []
    importances_dict = defaultdict(list)
    
    # Prepare empty lists to plot the performance of accuracy.
    acclist_realRF                          = []
    acclist_realRF_predictTrain             = []
    acclist_predictionOnTest_scrambledtrain  = []
    acclist_predictionOnTrain_scrambledtrain = []
        
    for i in range(10):
        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)
            
    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \
                                                                        X_internal, X_external, Y_internal, Y_external)
    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \
        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\
        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, 
           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, 
           RMSE_External_std, X_internal, X_external, X, f, des1, des2)
    plot_model(f, X_internal, X_external, Y_internal, Y_external,
                R2_train_mean, Q2_External_mean,
                importances_mean, importances_std, Feature, prediction,
                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)
    
outfile.close()
    #END
141/214:
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict

def build_model(X, Y, seed, hx, f):
    
    
    #Data split using 70/30 ratio
    X_internal, X_external, Y_internal, Y_external = train_test_split(X,
                                                        Y, test_size=0.3,
                                                        random_state=seed)

    # Training set
    rf = RandomForestRegressor(n_estimators=400, 
                               max_features='sqrt', 
                               min_samples_leaf = 1,
                               random_state = 13,
                               n_jobs=-1)
    
    rf.fit(X_internal,Y_internal)
    prediction = rf.predict(X_internal)
    
    # Cross-validation
    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)
    
    # External set  
    prediction_external = rf.predict(X_external)
    
    #print result from each seed    
    R2_train.append(r2_score(Y_internal, prediction))
    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))
    Q2_CV.append(r2_score(Y_internal, cv))
    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))
    Q2_External.append(r2_score(Y_external, prediction_external))
    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))
    
    #Feature Importance
    Feature = hx[:]
    feature_importance = rf.feature_importances_
    importances = 100.0 * (feature_importance / feature_importance.max()) #index
    
    for i, fx in enumerate(Feature):
        
        hash(tuple(importances_dict))
        importances_dict[fx].append(importances[i])
       
    
    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict
141/215:
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict

def build_model(X, Y, seed, hx, f):
    
    
    #Data split using 70/30 ratio
    X_internal, X_external, Y_internal, Y_external = train_test_split(X,
                                                        Y, test_size=0.3,
                                                        random_state=seed)

    # Training set
    rf = RandomForestRegressor(n_estimators=400, 
                               max_features='sqrt', 
                               min_samples_leaf = 1,
                               random_state = 13,
                               n_jobs=-1)
    
    rf.fit(X_internal,Y_internal)
    prediction = rf.predict(X_internal)
    
    # Cross-validation
    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)
    
    # External set  
    prediction_external = rf.predict(X_external)
    
    #print result from each seed    
    R2_train.append(r2_score(Y_internal, prediction))
    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))
    Q2_CV.append(r2_score(Y_internal, cv))
    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))
    Q2_External.append(r2_score(Y_external, prediction_external))
    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))
    
    #Feature Importance
    Feature = hx[:]
    feature_importance = rf.feature_importances_
    importances = 100.0 * (feature_importance / feature_importance.max()) #index
    
    for i, fx in enumerate(Feature):
        
        importances_dict = hash(tuple(importances_dict))
        importances_dict[fx].append(importances[i])
       
    
    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict
141/216:
import glob # to read multiple files 
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem
import os

%config InlineBackend.figure_format = 'retina'

! rm Result/Result_LogKp.csv

outfile = open('Result/Result_LogKp.csv', 'a')

print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\
                    'R2_train,R2_train_std,' + \
                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \
                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')

path = r'QSAR/'

for f in glob.glob(path + '*.csv'):
    df = pd.read_csv(f)
    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))
    df = df.fillna(method='ffill')
    Y = df["LogKp"].values.astype(np.float)
    data = df
    
    print ('\n\n************************************************************************************')
    print ('')
    print (f)
    print ('')
    
    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%
    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7

    h = data.columns.tolist()
    hx = np.array(h)

    data = data.values.astype(np.float)
    X = np.array(data)
    
    # Prepare empty lists to plot QSAR model
    R2_train = []
    RMSE_train = []
    Q2_CV = []
    RMSE_CV = []
    Q2_External = []
    RMSE_External = []
    importances_dict = defaultdict(list)
    
    # Prepare empty lists to plot the performance of accuracy.
    acclist_realRF                          = []
    acclist_realRF_predictTrain             = []
    acclist_predictionOnTest_scrambledtrain  = []
    acclist_predictionOnTrain_scrambledtrain = []
        
    for i in range(10):
        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)
            
    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \
                                                                        X_internal, X_external, Y_internal, Y_external)
    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \
        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\
        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, 
           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, 
           RMSE_External_std, X_internal, X_external, X, f, des1, des2)
    plot_model(f, X_internal, X_external, Y_internal, Y_external,
                R2_train_mean, Q2_External_mean,
                importances_mean, importances_std, Feature, prediction,
                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)
    
outfile.close()
    #END
141/217:
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict

def build_model(X, Y, seed, hx, f):
    
    
    #Data split using 70/30 ratio
    X_internal, X_external, Y_internal, Y_external = train_test_split(X,
                                                        Y, test_size=0.3,
                                                        random_state=seed)

    # Training set
    rf = RandomForestRegressor(n_estimators=400, 
                               max_features='sqrt', 
                               min_samples_leaf = 1,
                               random_state = 13,
                               n_jobs=-1)
    
    rf.fit(X_internal,Y_internal)
    prediction = rf.predict(X_internal)
    
    # Cross-validation
    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)
    
    # External set  
    prediction_external = rf.predict(X_external)
    
    #print result from each seed    
    R2_train.append(r2_score(Y_internal, prediction))
    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))
    Q2_CV.append(r2_score(Y_internal, cv))
    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))
    Q2_External.append(r2_score(Y_external, prediction_external))
    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))
    
    #Feature Importance
    Feature = hx[:]
    feature_importance = rf.feature_importances_
    importances = 100.0 * (feature_importance / feature_importance.max()) #index
    
    for i, fx in enumerate(Feature):
        
        importances_dict = hash(importances_dict)
        importances_dict[fx].append(importances[i])
       
    
    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict
141/218:
import glob # to read multiple files 
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem
import os

%config InlineBackend.figure_format = 'retina'

! rm Result/Result_LogKp.csv

outfile = open('Result/Result_LogKp.csv', 'a')

print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\
                    'R2_train,R2_train_std,' + \
                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \
                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')

path = r'QSAR/'

for f in glob.glob(path + '*.csv'):
    df = pd.read_csv(f)
    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))
    df = df.fillna(method='ffill')
    Y = df["LogKp"].values.astype(np.float)
    data = df
    
    print ('\n\n************************************************************************************')
    print ('')
    print (f)
    print ('')
    
    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%
    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7

    h = data.columns.tolist()
    hx = np.array(h)

    data = data.values.astype(np.float)
    X = np.array(data)
    
    # Prepare empty lists to plot QSAR model
    R2_train = []
    RMSE_train = []
    Q2_CV = []
    RMSE_CV = []
    Q2_External = []
    RMSE_External = []
    importances_dict = defaultdict(list)
    
    # Prepare empty lists to plot the performance of accuracy.
    acclist_realRF                          = []
    acclist_realRF_predictTrain             = []
    acclist_predictionOnTest_scrambledtrain  = []
    acclist_predictionOnTrain_scrambledtrain = []
        
    for i in range(10):
        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)
            
    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \
                                                                        X_internal, X_external, Y_internal, Y_external)
    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \
        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\
        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, 
           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, 
           RMSE_External_std, X_internal, X_external, X, f, des1, des2)
    plot_model(f, X_internal, X_external, Y_internal, Y_external,
                R2_train_mean, Q2_External_mean,
                importances_mean, importances_std, Feature, prediction,
                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)
    
outfile.close()
    #END
141/219:
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict

def build_model(X, Y, seed, hx, f):
    
    
    #Data split using 70/30 ratio
    X_internal, X_external, Y_internal, Y_external = train_test_split(X,
                                                        Y, test_size=0.3,
                                                        random_state=seed)

    # Training set
    rf = RandomForestRegressor(n_estimators=400, 
                               max_features='sqrt', 
                               min_samples_leaf = 1,
                               random_state = 13,
                               n_jobs=-1)
    
    rf.fit(X_internal,Y_internal)
    prediction = rf.predict(X_internal)
    
    # Cross-validation
    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)
    
    # External set  
    prediction_external = rf.predict(X_external)
    
    #print result from each seed    
    R2_train.append(r2_score(Y_internal, prediction))
    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))
    Q2_CV.append(r2_score(Y_internal, cv))
    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))
    Q2_External.append(r2_score(Y_external, prediction_external))
    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))
    
    #Feature Importance
    Feature = hx[:]
    feature_importance = rf.feature_importances_
    importances = 100.0 * (feature_importance / feature_importance.max()) #index
    
    for i, fx in enumerate(Feature):
        
        hash(importances_dict[fx]).append(importances[i])
       
    
    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict
141/220:
import glob # to read multiple files 
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem
import os

%config InlineBackend.figure_format = 'retina'

! rm Result/Result_LogKp.csv

outfile = open('Result/Result_LogKp.csv', 'a')

print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\
                    'R2_train,R2_train_std,' + \
                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \
                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')

path = r'QSAR/'

for f in glob.glob(path + '*.csv'):
    df = pd.read_csv(f)
    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))
    df = df.fillna(method='ffill')
    Y = df["LogKp"].values.astype(np.float)
    data = df
    
    print ('\n\n************************************************************************************')
    print ('')
    print (f)
    print ('')
    
    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%
    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7

    h = data.columns.tolist()
    hx = np.array(h)

    data = data.values.astype(np.float)
    X = np.array(data)
    
    # Prepare empty lists to plot QSAR model
    R2_train = []
    RMSE_train = []
    Q2_CV = []
    RMSE_CV = []
    Q2_External = []
    RMSE_External = []
    importances_dict = defaultdict(list)
    
    # Prepare empty lists to plot the performance of accuracy.
    acclist_realRF                          = []
    acclist_realRF_predictTrain             = []
    acclist_predictionOnTest_scrambledtrain  = []
    acclist_predictionOnTrain_scrambledtrain = []
        
    for i in range(10):
        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)
            
    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \
                                                                        X_internal, X_external, Y_internal, Y_external)
    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \
        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\
        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, 
           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, 
           RMSE_External_std, X_internal, X_external, X, f, des1, des2)
    plot_model(f, X_internal, X_external, Y_internal, Y_external,
                R2_train_mean, Q2_External_mean,
                importances_mean, importances_std, Feature, prediction,
                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)
    
outfile.close()
    #END
141/221:
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict

def build_model(X, Y, seed, hx, f):
    
    
    #Data split using 70/30 ratio
    X_internal, X_external, Y_internal, Y_external = train_test_split(X,
                                                        Y, test_size=0.3,
                                                        random_state=seed)

    # Training set
    rf = RandomForestRegressor(n_estimators=400, 
                               max_features='sqrt', 
                               min_samples_leaf = 1,
                               random_state = 13,
                               n_jobs=-1)
    
    rf.fit(X_internal,Y_internal)
    prediction = rf.predict(X_internal)
    
    # Cross-validation
    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)
    
    # External set  
    prediction_external = rf.predict(X_external)
    
    #print result from each seed    
    R2_train.append(r2_score(Y_internal, prediction))
    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))
    Q2_CV.append(r2_score(Y_internal, cv))
    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))
    Q2_External.append(r2_score(Y_external, prediction_external))
    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))
    
    #Feature Importance
    Feature = hx[:]
    feature_importance = rf.feature_importances_
    importances = 100.0 * (feature_importance / feature_importance.max()) #index
    
       
    
    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict
141/222:
import glob # to read multiple files 
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem
import os

%config InlineBackend.figure_format = 'retina'

! rm Result/Result_LogKp.csv

outfile = open('Result/Result_LogKp.csv', 'a')

print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\
                    'R2_train,R2_train_std,' + \
                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \
                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')

path = r'QSAR/'

for f in glob.glob(path + '*.csv'):
    df = pd.read_csv(f)
    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))
    df = df.fillna(method='ffill')
    Y = df["LogKp"].values.astype(np.float)
    data = df
    
    print ('\n\n************************************************************************************')
    print ('')
    print (f)
    print ('')
    
    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%
    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7

    h = data.columns.tolist()
    hx = np.array(h)

    data = data.values.astype(np.float)
    X = np.array(data)
    
    # Prepare empty lists to plot QSAR model
    R2_train = []
    RMSE_train = []
    Q2_CV = []
    RMSE_CV = []
    Q2_External = []
    RMSE_External = []
    importances_dict = defaultdict(list)
    
    # Prepare empty lists to plot the performance of accuracy.
    acclist_realRF                          = []
    acclist_realRF_predictTrain             = []
    acclist_predictionOnTest_scrambledtrain  = []
    acclist_predictionOnTrain_scrambledtrain = []
        
    for i in range(10):
        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)
            
    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \
                                                                        X_internal, X_external, Y_internal, Y_external)
    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \
        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\
        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, 
           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, 
           RMSE_External_std, X_internal, X_external, X, f, des1, des2)
    plot_model(f, X_internal, X_external, Y_internal, Y_external,
                R2_train_mean, Q2_External_mean,
                importances_mean, importances_std, Feature, prediction,
                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)
    
outfile.close()
    #END
141/223:
def mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, importances_dict):
    R2_train_mean = np.mean(R2_train)
    RMSE_train_mean = np.mean(RMSE_train)
    Q2_CV_mean = np.mean(Q2_CV)
    RMSE_CV_mean = np.mean(RMSE_CV)
    Q2_External_mean = np.mean(Q2_External)
    RMSE_External_mean = np.mean(RMSE_External)
    importances_mean0 = {}
    for fx in importances_dict:
        importances_mean0[fx] = np.mean(importances_dict[fx])
    importances_mean = sorted([(k,v) for k,v in importances_mean0.items()],
                                    key=lambda x: x[1], reverse=True)
    
    #predictionOnTest_mean = np.mean(acclist_predictionOnTest_scrambledtrain)
    #predictionOnTrain_mean = np.mean(acclist_predictionOnTrain_scrambledtrain)
    
    return R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \
            RMSE_External_mean, importances_mean
141/224:
import glob # to read multiple files 
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem
import os

%config InlineBackend.figure_format = 'retina'

! rm Result/Result_LogKp.csv

outfile = open('Result/Result_LogKp.csv', 'a')

print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\
                    'R2_train,R2_train_std,' + \
                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \
                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')

path = r'QSAR/'

for f in glob.glob(path + '*.csv'):
    df = pd.read_csv(f)
    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))
    df = df.fillna(method='ffill')
    Y = df["LogKp"].values.astype(np.float)
    data = df
    
    print ('\n\n************************************************************************************')
    print ('')
    print (f)
    print ('')
    
    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%
    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7

    h = data.columns.tolist()
    hx = np.array(h)

    data = data.values.astype(np.float)
    X = np.array(data)
    
    # Prepare empty lists to plot QSAR model
    R2_train = []
    RMSE_train = []
    Q2_CV = []
    RMSE_CV = []
    Q2_External = []
    RMSE_External = []
    importances_dict = defaultdict(list)
    
    # Prepare empty lists to plot the performance of accuracy.
    acclist_realRF                          = []
    acclist_realRF_predictTrain             = []
    acclist_predictionOnTest_scrambledtrain  = []
    acclist_predictionOnTrain_scrambledtrain = []
        
    for i in range(10):
        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)
            
    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \
                                                                        X_internal, X_external, Y_internal, Y_external)
    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \
        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\
        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, 
           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, 
           RMSE_External_std, X_internal, X_external, X, f, des1, des2)
    plot_model(f, X_internal, X_external, Y_internal, Y_external,
                R2_train_mean, Q2_External_mean,
                importances_mean, importances_std, Feature, prediction,
                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)
    
outfile.close()
    #END
141/225:
import numpy as np

def Remove_useless_descriptor(df, threshold):
    
    des1 = len(df.columns) 
    
    h = df.columns.tolist()
    df = df.values.astype(np.float)
    df = np.array(df)

    STDEV = np.std(df, axis=0)
    idx = [idx for idx, val in enumerate(STDEV) if val > threshold]
    df2 = df[:,idx]
    hx = np.array(h)[idx]
    
    df = pd.DataFrame(df2, columns=[hx])
    
    des2 = len(df.columns)
    
    print('from Remove useless descriptor')
    print("The initial set of " + str(des1) + \
          " descriptors has been reduced to " + str(des2) + " descriptors.")
    
    return df, des1, des2
141/226:
from scipy import stats

def correlation(df, threshold):

    des3 = len(df.columns) 
    corr = stats.pearsonr
    col_corr = set() # Set of all the names of deleted columns
    corr_matrix = df.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if corr_matrix.iloc[i, j] >= threshold:
                colname = corr_matrix.columns[i] # getting the name of column
                col_corr.add(colname)
                if colname in df.columns:
                    del df[colname] # deleting the column from the dataset
    des4 = len(df.columns) 

    print ('from Remove correlation')
    print ("The initial set of " + str(des3) + ' descriptors'+ 
           " has been reduced to " + str(des4) + " descriptors.")

    return df, des3, des4
141/227:
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict

def build_model(X, Y, seed, hx, f):
    
    
    #Data split using 70/30 ratio
    X_internal, X_external, Y_internal, Y_external = train_test_split(X,
                                                        Y, test_size=0.3,
                                                        random_state=seed)

    # Training set
    rf = RandomForestRegressor(n_estimators=400, 
                               max_features='sqrt', 
                               min_samples_leaf = 1,
                               random_state = 13,
                               n_jobs=-1)
    
    rf.fit(X_internal,Y_internal)
    prediction = rf.predict(X_internal)
    
    # Cross-validation
    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)
    
    # External set  
    prediction_external = rf.predict(X_external)
    
    #print result from each seed    
    R2_train.append(r2_score(Y_internal, prediction))
    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))
    Q2_CV.append(r2_score(Y_internal, cv))
    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))
    Q2_External.append(r2_score(Y_external, prediction_external))
    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))
    
    #Feature Importance
    Feature = hx[:]
    feature_importance = rf.feature_importances_
    importances = 100.0 * (feature_importance / feature_importance.max()) #index
    
    for i, fx in enumerate(Feature):
        
        hash(importances_dict[fx]).append(importances[i])
       
    
    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict
141/228:
from copy import deepcopy

def Y_scrambling(X_internal, X_external, Y_internal, Y_external):
    # Do the Y-scrambling. Loop over the actual learning for 100 times.
    for randomseedcounter in range(1,101):
        y_train_scrambled = deepcopy(Y_internal)
        X_train_scrambled = deepcopy(X_internal)
        np.random.shuffle(y_train_scrambled)
        np.random.shuffle(X_train_scrambled)

        # training was done on "scrambled" data - prediction on test set
        RF_scrambled         = RandomForestRegressor()
        RF_scrambled         = RF_scrambled.fit(X_internal,y_train_scrambled)
        y_predict_scrambled  = RF_scrambled.predict(X_external)
    
        acclist_predictionOnTest_scrambledtrain.append((RF_scrambled.score(X_external,Y_external))**2)
    
        # training was done on "scrambled" data - prediction on train set
        y_predict_scrambled_predictTrain  = RF_scrambled.predict(X_internal)
    
        acclist_predictionOnTrain_scrambledtrain.append((RF_scrambled.score(X_internal,Y_internal))**2)
        
        r2 = pd.DataFrame(acclist_predictionOnTrain_scrambledtrain, columns=['R2'])
        q2 = pd.DataFrame(acclist_predictionOnTest_scrambledtrain, columns=['Q2'])

        #result = pd.concat([r2, q2], axis=1, join='inner').to_csv(f+"_Y_scrambling.csv", header=False, index=False)
        
        
    return acclist_predictionOnTest_scrambledtrain,acclist_predictionOnTrain_scrambledtrain
141/229:
def mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, importances_dict):
    R2_train_mean = np.mean(R2_train)
    RMSE_train_mean = np.mean(RMSE_train)
    Q2_CV_mean = np.mean(Q2_CV)
    RMSE_CV_mean = np.mean(RMSE_CV)
    Q2_External_mean = np.mean(Q2_External)
    RMSE_External_mean = np.mean(RMSE_External)
    importances_mean0 = {}
    for fx in importances_dict:
        importances_mean0[fx] = np.mean(importances_dict[fx])
    importances_mean = sorted([(k,v) for k,v in importances_mean0.items()],
                                    key=lambda x: x[1], reverse=True)
    
    #predictionOnTest_mean = np.mean(acclist_predictionOnTest_scrambledtrain)
    #predictionOnTrain_mean = np.mean(acclist_predictionOnTrain_scrambledtrain)
    
    return R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \
            RMSE_External_mean, importances_mean
141/230:
def std(R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, importances_dict):
    R2_train_std = np.std(R2_train)
    RMSE_train_std = np.std(RMSE_train)
    Q2_CV_std = np.std(Q2_CV)
    RMSE_CV_std = np.std(RMSE_CV)
    Q2_External_std = np.std(Q2_External)
    RMSE_External_std = np.std(RMSE_External)
    importances_std0 = {}
    for fx in importances_dict:
        importances_std0[fx] = np.std(importances_dict[fx])
    importances_std = sorted([(k,v) for k,v in importances_std0.items()],
                                    key=lambda x: x[1], reverse=True)
    #predictionOnTest_std = np.std(acclist_predictionOnTest_scrambledtrain)
    #predictionOnTrain_std = np.std(acclist_predictionOnTrain_scrambledtrain)
    
    return R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\
            RMSE_External_std, importances_std
141/231:
def print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, 
           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, 
           RMSE_External_std, X_internal, X_external, X, f, des1, des2 ):
    #outfile = open('all_output.csv', 'a')
    name = [os.path.basename(f)]
    print_out = name
    print_out = [name.replace('.smi','') for name in print_out]
    
    print (outfile, '%s,%d,%d,%d,%d,%0.4f,%0.4f,%0.4f,%0.4f,%d,%0.4f,%0.4f,%0.4f,%0.4f,%d,%0.4f,%0.4f,%0.4f,%0.4f' \
                                     % (print_out, len(X_internal),
                                        des1, des2, des4,
                                        R2_train_mean, R2_train_std,
                                        RMSE_train_mean, RMSE_train_std,
                                        len(X_internal),
                                        Q2_CV_mean, Q2_CV_std,
                                        RMSE_CV_mean, RMSE_CV_std,
                                        len(X_external),
                                        Q2_External_mean, Q2_External_std,
                                        RMSE_External_mean, RMSE_External_std,))
 
    print ('\nTraining set\n------------')
    print ('N: ' + (str(len(X_internal))))
    print ('R2: %0.4f'%(R2_train_mean))
    print ('std_R2: %0.4f'%(R2_train_std))
    print ('RMSE: %0.4f'%(RMSE_train_mean))
    print ('std_RMSE: %0.4f'%(RMSE_train_std))

    print ('\nCross-validation set\n------------')
    print ('N: ' + (str(len(X_internal))))
    print ('Q2: %0.4f'%(Q2_CV_mean))
    print ('std_Q2: %0.4f'%(Q2_CV_std))
    print ('RMSE: %0.4f'%(RMSE_CV_mean))
    print ('std_RMSE: %0.4f'%(RMSE_CV_std))

    print ('\nExternal set\n------------')
    print ('N: ' + (str(len(X_external))))
    print ('Q2_EXt: %0.4f'%(Q2_External_mean))
    print ('std_Q2_EXt: %0.4f'%(Q2_External_std))
    print ('RMSE: %0.4f'%(RMSE_External_mean))
    print ('std_RMSE: %0.4f'%(RMSE_External_std))
141/232:
import matplotlib.pyplot as plt
from matplotlib.ticker import MultipleLocator
import pylab as py 
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import summary_table

def plot_model (f, X_internal, X_external, Y_internal, Y_external,
                R2_train_mean, Q2_External_mean,
                importances_mean, importances_std, Feature, prediction, 
                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain):
    
    # Prepare plot
    m  = rf.fit(X_internal,Y_internal)
    cm = plt.cm.RdBu
    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)

    fig_size = plt.rcParams["figure.figsize"]
    fig_size[0]=5
    fig_size[1]=5

    # Train Set
    x_train = np.array(Y_internal)
    y_train = m.predict(X_internal).flatten()
    py.scatter(x_train, y_train, s=50, marker='.', alpha=0.3,
            c='g', cmap=cm ,edgecolors='g')
                 #label=r"$R^{2}_{Tr}$ = %.4f" % R2_train_mean)
    
    
    # CV Set
    np.array(cv)
    x_test = np.array(Y_internal)
    y_test = cv
    py.scatter(x_test, y_test, s=50, marker='.', alpha=0.3,
            c='b', cmap=cm ,edgecolors='b') 
                 #label=r"$Q^{2}_{Ext}$ = %.4f" % Q2_External_mean)
        #2SD line
    X = sm.add_constant(x_test)
    res = sm.OLS(y_test, X).fit()

    st, data, ss2 = summary_table(res, alpha=0.05)
    fittedvalues = data[:,2]
        #predict_mean_se  = data[:,3]
    predict_mean_ci_low, predict_mean_ci_upp = data[:,4:6].T
    predict_ci_low, predict_ci_upp = data[:,6:8].T
        #2SD line
    plt.plot(X, predict_ci_low, '--b', linewidth=0.5, alpha=0.5)
    plt.plot(X, predict_ci_upp, '--b', linewidth=0.5, alpha=0.5)
        
    # External Set
    x_test = np.array(Y_external)
    y_test = m.predict(X_external).flatten()
    py.scatter(x_test, y_test, s=50, marker='.', alpha=0.3,
            c='r', cmap=cm ,edgecolors='r') 
                 #label=r"$Q^{2}_{Ext}$ = %.4f" % Q2_External_mean)
    
    
    #py.plot(x_train, np.polyval(np.polyfit(x_train,y_train,1), x_train), '--r') #mean line
    plt.legend(loc=2,prop={'size':6})
    plt.xlabel("Experimental $pIC_{50}$ values", fontsize=10)
    plt.ylabel("Predicted $pIC_{50}$ values", fontsize=10)
        
    min_axis = np.min(np.concatenate([Y_internal, prediction], axis=0))
    max_axis = np.max(np.concatenate([Y_internal, prediction], axis=0))
    plt.xlim([(min_axis*0.9),(max_axis*1.05)])
    plt.ylim([(min_axis*0.9),(max_axis*1.05)])
    plt.tick_params(axis='both', which='major', labelsize=14)

    
    # Save plot to file
    plt.savefig(f+'_compair_Fp.pdf', dpi=300)
    plt.show()
    
    # Y-scrambling plot
    py.scatter(Q2_CV_mean, R2_train_mean, s=100, marker='.', alpha=0.3,
            c='b', cmap=cm ,edgecolors='b') 
            
    py.scatter(acclist_predictionOnTest_scrambledtrain,acclist_predictionOnTrain_scrambledtrain, \
               s=100, marker='.', alpha=0.3, c='r', cmap=cm ,edgecolors='r')

    plt.legend(loc=2,prop={'size':6})
    plt.xlabel("$Q^{2}$", fontsize=10)
    plt.ylabel("$R^{2}$", fontsize=10)
    plt.tick_params(axis='both', which='major', labelsize=14)
    
    plt.ylim([0, 1])
    plt.xlim([0, 1])
    plt.axhline(0.5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
    plt.axvline(0.5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)
    
    plt.savefig(f+'_Y_scrambling.pdf', dpi=300)

    plt.show()

    #Feature Importance 
    fig_size = plt.rcParams["figure.figsize"]
    fig_size[0]=5
    fig_size[1]=10

    barlist = plt.barh(range(20), [x[1] for x in importances_mean[:20]], 
          color="g", xerr=[x[1] for x in importances_std[:20]], align="center", \
                       error_kw=dict(ecolor='gray', lw=2, capsize=5, capthick=2))

    print (str("top10"), [x[0] for x in importances_mean[:10]])
    print ('')
    print (str("top20"), [x[0] for x in importances_mean[:20]])
    print ('')
    print (str("top30"), [x[0] for x in importances_mean[:30]])
    print ('')
    print (str("top40"), [x[0] for x in importances_mean[:40]])
    print ('')
    print (str("top50"), [x[0] for x in importances_mean[:50]])
    
    plt.yticks(range(20), [x[0] for x in importances_mean[:20]])
    plt.ylim([-1, 20])
    
    plt.xlabel(r"$\bf{Gini}$" + " "+ r"$\bf{index}$", fontsize=12)
    ax = plt.gca()
    ax.invert_yaxis()
    plt.tight_layout(pad=2.0, w_pad=0.7, h_pad=2.0)
    plt.savefig(f+'_Feature_importances.pdf', dpi=300)
    plt.show()
141/233:
import glob # to read multiple files 
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem
import os

%config InlineBackend.figure_format = 'retina'

! rm Result/Result_LogKp.csv

outfile = open('Result/Result_LogKp.csv', 'a')

print (outfile, 'Filename,N_train,Descriptors,Remove STDEV,Remove correlation,'+\
                    'R2_train,R2_train_std,' + \
                    'MAE_train,MAE_train_std,N_CV,Q2_CV,Q2_CV_std,MAE_CV,' + \
                    'MAE_CV_std,N_External,Q2_External,Q2_External_std,MAE_External,MAE_External_std')

path = r'QSAR/'

for f in glob.glob(path + '*.csv'):
    df = pd.read_csv(f)
    df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))
    df = df.fillna(method='ffill')
    Y = df["LogKp"].values.astype(np.float)
    data = df
    
    print ('\n\n************************************************************************************')
    print ('')
    print (f)
    print ('')
    
    data, des1, des2 = Remove_useless_descriptor(data, 0.05)  # Remove correlation cut off 95%
    data, des3, des4 = correlation(data, 0.7)  # Remove correlation cut off 0.7

    h = data.columns.tolist()
    hx = np.array(h)

    data = data.values.astype(np.float)
    X = np.array(data)
    
    # Prepare empty lists to plot QSAR model
    R2_train = []
    RMSE_train = []
    Q2_CV = []
    RMSE_CV = []
    Q2_External = []
    RMSE_External = []
    importances_dict = defaultdict(list)
    
    # Prepare empty lists to plot the performance of accuracy.
    acclist_realRF                          = []
    acclist_realRF_predictTrain             = []
    acclist_predictionOnTest_scrambledtrain  = []
    acclist_predictionOnTrain_scrambledtrain = []
        
    for i in range(10):
        R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict = build_model(X, Y, i, hx, f)
            
    acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain = Y_scrambling( \
                                                                        X_internal, X_external, Y_internal, Y_external)
    R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, \
        RMSE_External_mean, importances_mean = mean(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std,\
        RMSE_External_std, importances_std = std(R2_train, RMSE_train, Q2_CV, RMSE_CV, \
                                 Q2_External, RMSE_External, importances_dict)
    print_output(R2_train_mean, RMSE_train_mean, Q2_CV_mean, RMSE_CV_mean, Q2_External_mean, 
           RMSE_External_mean, R2_train_std, RMSE_train_std, Q2_CV_std, RMSE_CV_std, Q2_External_std, 
           RMSE_External_std, X_internal, X_external, X, f, des1, des2)
    plot_model(f, X_internal, X_external, Y_internal, Y_external,
                R2_train_mean, Q2_External_mean,
                importances_mean, importances_std, Feature, prediction,
                acclist_predictionOnTest_scrambledtrain, acclist_predictionOnTrain_scrambledtrain)
    
outfile.close()
    #END
143/1:
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict

def build_model(X, Y, seed, hx, f):
    
    
    #Data split using 70/30 ratio
    X_internal, X_external, Y_internal, Y_external = train_test_split(X,
                                                        Y, test_size=0.3,
                                                        random_state=seed)

    # Training set
    rf = RandomForestRegressor(n_estimators=400, 
                               max_features='sqrt', 
                               min_samples_leaf = 1,
                               random_state = 13,
                               n_jobs=-1)
    
    rf.fit(X_internal,Y_internal)
    prediction = rf.predict(X_internal)
    
    # Cross-validation
    cv = cross_val_predict(rf, X_internal, Y_internal, cv=10, n_jobs=-1)
    
    # External set  
    prediction_external = rf.predict(X_external)
    
    #print result from each seed    
    R2_train.append(r2_score(Y_internal, prediction))
    RMSE_train.append(np.sqrt(mean_absolute_error(Y_internal, prediction)))
    Q2_CV.append(r2_score(Y_internal, cv))
    RMSE_CV.append(np.sqrt(mean_absolute_error(Y_internal, cv)))
    Q2_External.append(r2_score(Y_external, prediction_external))
    RMSE_External.append((mean_absolute_error(Y_external, prediction_external)))
    
    #Feature Importance
    Feature = hx[:]
    feature_importance = rf.feature_importances_
    importances = 100.0 * (feature_importance / feature_importance.max()) #index
    
    for i, fx in enumerate(Feature):
        
        hash(importances_dict[fx]).append(importances[i])
       
    
    return R2_train, RMSE_train, Q2_CV, RMSE_CV, Q2_External, RMSE_External, Feature, \
           X_internal, X_external, Y_internal, Y_external, rf, prediction, importances_dict
143/2:
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
143/3:
from sklearn.linear_model import LinearRegression
lr = LinearRegression()
scores = cross_val_score(lr, X, y, cv=kf)
print(scores)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
143/4:
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate, cross_val_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
143/5: df = pd.read_csv('QSAR/Kp_Fingerprinter.csv')
143/6:
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate, cross_val_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import svm
143/7: df = pd.read_csv('QSAR/Kp_Fingerprinter.csv')
143/8: df = pd.read_csv(r'QSAR/Kp_Fingerprinter.csv')
143/9:
df = pd.read_csv(r'QSAR/Kp_Fingerprinter.csv')
df
143/10:
df = pd.read_csv(r'QSAR/Kp_Fingerprinter.csv')
df
143/11:
df = pd.read_csv(r'QSAR\Kp_Fingerprinter.csv')
df
143/12: pwd
146/1:
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate, cross_val_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import svm
146/2: pwd
146/3:
df = pd.read_csv(r'QSAR/Kp_Fingerprinter.csv')
df
146/4: X = df.drop('LogKp')
146/5: X = df.drop('LogKp', axis=1)
146/6: X
146/7: y = df('LogKp', axis=1)
146/8: y = df('LogKp')
146/9: y = df[]'LogKp']
146/10: y = df['LogKp']
146/11: y
146/12: y.to_frame
146/13: y.to_frame()
146/14: y
146/15: y = y.to_frame()
146/16: y
146/17:
from sklearn.linear_model import LinearRegression
lr = LinearRegression()
scores = cross_val_score(lr, X, y, cv=kf)
print(scores)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
146/18:
from sklearn.linear_model import LinearRegression
lr = LinearRegression()
scores = cross_val_score(lr, X, y, cv=10)
print(scores)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
146/19:
from sklearn.cross_decomposition import PLSRegression
pls2 = PLSRegression(n_components=2)
scores = cross_val_score(pls2, X, y, cv=kf)
scores
146/20:
from sklearn.cross_decomposition import PLSRegression
pls2 = PLSRegression(n_components=2)
scores = cross_val_score(pls2, X, y, cv=10)
scores
146/21:
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(n_estimators=200)
scores = cross_val_score(model, X, y, cv=10)
scores
146/22:
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(n_estimators=150)
scores = cross_val_score(model, X, y, cv=10)
scores
152/1:
import numpy as np

def Remove_useless_descriptor(df, threshold):
    
    des1 = len(df.columns) 
    
    h = df.columns.tolist()
    df = df.values.astype(np.float)
    df = np.array(df)

    STDEV = np.std(df, axis=0)
    idx = [idx for idx, val in enumerate(STDEV) if val > threshold]
    df2 = df[:,idx]
    hx = np.array(h)[idx]
    
    df = pd.DataFrame(df2, columns=[hx])
    
    des2 = len(df.columns)
    
    print('from Remove useless descriptor')
    print("The initial set of " + str(des1) + \
          " descriptors has been reduced to " + str(des2) + " descriptors.")
    
    return df, des1, des2
155/1:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
155/2:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
155/3: pwd
155/4:
df = pd.read_csv("df6_lipinski.csv")
df
155/5: df1 = df.drop(['Unnamed: 0', 'smiles'], axis=True)
155/6: df1
155/7: df1 = df.drop(['Unnamed: 0', 'smiles', 'Cells'], axis=True)
155/8:
from sklearn.model_selection import KFold, cross_val_score, cross_val_predict
kf =KFold(n_splits=10, shuffle=False)
155/9:
cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1
155/10: df1
155/11: df1 = df.drop(['Unnamed: 0', 'smiles', 'Cells', 'compound'], axis=True)
155/12: df1
155/13: X = df1.drop['LogKp']
155/14: X = df1.drop[LogKp]
155/15: X = df1.drop(['LogKp'])
155/16: X = df1.drop(['LogKp'], axis=1)
155/17:
X = df1.drop(['LogKp'], axis=1)
X
155/18: y = df1['LogKp']
155/19:
y = df1['LogKp']
y
155/20:
y = df1['LogKp']
y.to_frame()
155/21:
y = df1['LogKp']
y.to_frame()
t
155/22:
y = df1['LogKp']
y.to_frame()
y
155/23:
y = df1['LogKp']
y = y.to_frame()
y
155/24:
from sklearn.model_to_frametion import KFold, cross_val_score, cross_val_predict
kf =KFold(n_splits=10, shuffle=False)
155/25:
from sklearn.model_to_selection import KFold, cross_val_score, cross_val_predict
kf =KFold(n_splits=10, shuffle=False)
155/26:
from sklearn.model_selection import KFold, cross_val_score, cross_val_predict
kf =KFold(n_splits=10, shuffle=False)
155/27:
cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1
155/28:
from sklearn.ensemble import RandomForestRegressor
RF = RandomForestRegressor()
155/29:
from sklearn.ensemble import RandomForestRegressor
RF = RandomForestRegressor(n_estimators=150)
155/30:
from sklearn.ensemble import RandomForestRegressor
RF = RandomForestRegressor(n_estimators=150)
y_pred = cross_val_predict(RF, X, y, cv=kf)
155/31: y.shape
155/32:
y = df1['LogKp']
y
155/33: y.shape
155/34:
from sklearn.ensemble import RandomForestRegressor
RF = RandomForestRegressor(n_estimators=150)
y_pred = cross_val_predict(RF, X, y, cv=kf)
155/35:
from sklearn.metrics import r2_score
r2_score(y, y_pred)
155/36: plt.scatter(y, y_pred)
155/37:
X = df1.drop(['LogKp'], axis=1)
X.shape
155/38:
from sklearn.ensemble import RandomForestRegressor
RF = RandomForestRegressor(n_estimators=400)
y_pred = cross_val_predict(RF, X, y, cv=kf)
155/39:
from sklearn.metrics import r2_score
#Rsqure
r2_score(y, y_pred)
155/40: plt.scatter(y, y_pred)
155/41:
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
RMSE=np.sqrt(mean_absolute_error(y, y_pred))
155/42: RMSE
155/43:
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
RMSE=np.sqrt(mean_squared_error(y, y_pred))
155/44: RMSE
155/45:
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
RMSE=mean_squared_error(y, y_pred)
155/46: RMSE
155/47:
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
RMSE = mean_squared_error(y, y_pred)
155/48: RMSE
155/49:
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(y, y_pred)
155/50: RMSE_pred
157/1:
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
RMSE_RF = mean_squared_error(y, y_pred)
157/2:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
157/3: pwd
157/4:
df = pd.read_csv("df6_lipinski.csv")
df
157/5: df1 = df.drop(['Unnamed: 0', 'smiles', 'Cells', 'compound'], axis=True)
157/6: df1
157/7:
X = df1.drop(['LogKp'], axis=1)
X.shape
157/8:
y = df1['LogKp']
y
157/9: y.shape
157/10:
from sklearn.model_selection import KFold, cross_val_score, cross_val_predict
kf =KFold(n_splits=10, shuffle=False)
157/11:
cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1
157/12:
from sklearn.ensemble import RandomForestRegressor
RF = RandomForestRegressor(n_estimators=400)
y_pred = cross_val_predict(RF, X, y, cv=kf)
157/13:
from sklearn.metrics import r2_score
#Rsqure
r2_score(y, y_pred)
157/14: plt.scatter(y, y_pred)
157/15:
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
RMSE_RF = mean_squared_error(y, y_pred)
157/16: RMSE_RF
157/17: from sklearn.svm import svm
157/18:     from sklearn.svm import SVM
157/19: from sklearn.svm import SVM
157/20:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
157/21: X = normalized(X)
157/22:
X = normalized(X)
X
157/23:
from sklearn.model_selection import KFold, cross_val_score, cross_val_predict
kf =KFold(n_splits=10, shuffle=False)
157/24:
cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1
157/25:
from sklearn.ensemble import RandomForestRegressor
RF = RandomForestRegressor(n_estimators=400)
y_pred = cross_val_predict(RF, X, y, cv=kf)
157/26:
from sklearn.metrics import r2_score
#Rsqure
r2_score(y, y_pred)
157/27: plt.scatter(y, y_pred)
157/28:
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
RMSE_RF = mean_squared_error(y, y_pred)
157/29: RMSE_RF
157/30: from sklearn.svm import SVC
157/31: from sklearn.svm import SVR
157/32:
from sklearn.svm import SVR
SVR = sklearn.svm.SVR()
157/33:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
157/34: y_pred = cross_val_predict(SVR, X, y, cv=kf)
157/35: r2_score(y, y_pred)
157/36: plt.scatter(y, y_pred)
157/37:
X = df1.drop(['LogKp'], axis=1)
X.shape
157/38:
y = df1['LogKp']
y
157/39: y.shape
157/40:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
157/41: X
157/42:
Xn = normalized(X)
Xn
157/43: y_pred = cross_val_predict(SVR, X, y, cv=kf)
157/44: r2_score(y, y_pred)
157/45: plt.scatter(y, y_pred)
157/46: X
157/47:
from sklearn.cross_decomposition import PLSRegression
pls2 = PLSRegression(n_components=10)
157/48:
from sklearn.cross_decomposition import PLSRegression
pls = PLSRegression(n_components=10)
157/49: y_pred = cross_val_predict(pls, X, y, cv=kf)
157/50: r2_score(y, y_pred)
157/51: plt.scatter(y, y_pred)
157/52:
#only LogP
X = df1(['LogP'], axis=1)
157/53:
#only LogP
X = df1['LogP']
157/54:
#only LogP
XlogP = df1['LogP']
157/55:
X = df1.drop(['LogKp'], axis=1)
X.shape
157/56:
RF = RandomForestRegressor(n_estimators=400)
y_pred = cross_val_predict(RF, XlogP, y, cv=kf)
r2_score = score(y, y_pred)
157/57:
#only LogP
XlogP = df1['LogP']
XlogP
157/58:
#only LogP
XlogP = df1['LogP']
XlogP = XlogP.to_frame
157/59:
RF = RandomForestRegressor(n_estimators=400)
y_pred = cross_val_predict(RF, XlogP, y, cv=kf)
r2_score = score(y, y_pred)
157/60:
#only LogP
XlogP = df1['LogP']
XlogP = XlogP.to_frame
XlogP
157/61:
#only LogP
XlogP = df1['LogP']
XlogP = XlogP.to_frame()
XlogP
157/62:
RF = RandomForestRegressor(n_estimators=400)
y_pred = cross_val_predict(RF, XlogP, y, cv=kf)
r2_score = score(y, y_pred)
157/63:
RF = RandomForestRegressor(n_estimators=400)
y_pred = cross_val_predict(RF, XlogP, y, cv=kf)
r2_score = score(y, y_pred)
r2_score
157/64:
RF = RandomForestRegressor(n_estimators=400)
y_pred = cross_val_predict(RF, XlogP, y, cv=kf)
r2_score(y, y_pred)
r2_score
157/65:
RF = RandomForestRegressor(n_estimators=400)
y_pred = cross_val_predict(RF, XlogP, y, cv=kf)
r2_score(y, y_pred)
157/66: plt.scatter(y, y_pred)
155/51:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
155/52:
df = pd.read_csv('QSAR/Kp_AtomPairs2DFingerprinter.csv')
df
155/53:
X = df1.drop(['LogKp'], axis=1)
X.shape
155/54:
y = df1['LogKp']
y
155/55: y.shape
155/56:
from sklearn.model_selection import KFold, cross_val_score, cross_val_predict
kf =KFold(n_splits=10, shuffle=False)
155/57:
cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1
155/58:
from sklearn.ensemble import RandomForestRegressor
RF = RandomForestRegressor(n_estimators=400)
y_pred = cross_val_predict(RF, X, y, cv=kf)
155/59:
from sklearn.metrics import r2_score
#Rsqure
r2_score(y, y_pred)
155/60: plt.scatter(y, y_pred)
155/61:
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(y, y_pred)
155/62: RMSE_pred
155/63: df1 = df.drop(['Unnamed: 0', 'smiles', 'Cells', 'compound'], axis=True)
155/64:
df = pd.read_csv('QSAR/Kp_AtomPairs2DFingerprinter.csv')
df
155/65:
X = df.drop(['LogKp'], axis=1)
X.shape
155/66:
y = df['LogKp']
y
155/67: y.shape
155/68:
from sklearn.model_selection import KFold, cross_val_score, cross_val_predict
kf =KFold(n_splits=10, shuffle=False)
155/69:
cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1
155/70:
from sklearn.ensemble import RandomForestRegressor
RF = RandomForestRegressor(n_estimators=400)
y_pred = cross_val_predict(RF, X, y, cv=kf)
155/71:
from sklearn.metrics import r2_score
#Rsqure
r2_score(y, y_pred)
155/72: plt.scatter(y, y_pred)
155/73:
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(y, y_pred)
155/74: RMSE_pred
157/67:
RF = RandomForestRegressor(n_estimators=400)
y_pred = cross_val_predict(RF, XlogP, y, cv=kf)
r2_score(y, y_pred)
print (plt.scatter(y, y_pred))
157/68:
RF = RandomForestRegressor(n_estimators=400)
y_pred = cross_val_predict(RF, XlogP, y, cv=kf)
RMSE_RF = mean_squared_error(y, y_pred)
print (RMSE_RF)
print (r2_score(y, y_pred)
print (plt.scatter(y, y_pred))
157/69:
RF = RandomForestRegressor(n_estimators=400)
y_pred = cross_val_predict(RF, XlogP, y, cv=kf)
RMSE_RF = mean_squared_error(y, y_pred)
print (RMSE_RF)
print (r2_score(y, y_pred)
print (plt.scatter(y, y_pred))
157/70:
RF = RandomForestRegressor(n_estimators=400)
y_pred = cross_val_predict(RF, XlogP, y, cv=kf)
RMSE_RF = mean_squared_error(y, y_pred)
print (RMSE_RF)
print (r2_score(y, y_pred)
    print (plt.scatter(y, y_pred))
157/71:
RF = RandomForestRegressor(n_estimators=400)
y_pred = cross_val_predict(RF, XlogP, y, cv=kf)
RMSE_RF = mean_squared_error(y, y_pred)
    print (RMSE_RF)
    print (r2_score(y, y_pred)
    print (plt.scatter(y, y_pred))
157/72:
RF = RandomForestRegressor(n_estimators=400)
y_pred = cross_val_predict(RF, XlogP, y, cv=kf)
RMSE_RF = mean_squared_error(y, y_pred)

print (RMSE_RF)
print (r2_score(y, y_pred)
print (plt.scatter(y, y_pred))
157/73:
RF = RandomForestRegressor(n_estimators=400)
y_pred = cross_val_predict(RF, XlogP, y, cv=kf)
RMSE_RF = mean_squared_error(y, y_pred)

print (RMSE_RF)
print (r2_score(y, y_pred)
       
print (plt.scatter(y, y_pred))
157/74:
RF = RandomForestRegressor(n_estimators=400)
y_pred = cross_val_predict(RF, XlogP, y, cv=kf)
RMSE_RF = mean_squared_error(y, y_pred)

print (RMSE_RF)
print (r2_score(y, y_pred)
157/75:
RF = RandomForestRegressor(n_estimators=400)
y_pred = cross_val_predict(RF, XlogP, y, cv=kf)
RMSE_RF = mean_squared_error(y, y_pred)

    print (plt.scatter(y, y_pred))
157/76:
RF = RandomForestRegressor(n_estimators=400)
y_pred = cross_val_predict(RF, XlogP, y, cv=kf)
RMSE_RF = mean_squared_error(y, y_pred)

print (plt.scatter(y, y_pred))
157/77:
RF = RandomForestRegressor(n_estimators=400)
y_pred = cross_val_predict(RF, XlogP, y, cv=kf)
RMSE_RF = mean_squared_error(y, y_pred)
r2_score = r2_score(y, y_pred)
print (plt.scatter(y, y_pred), RMSE_RF, r2_score)
157/78:
RF = RandomForestRegressor(n_estimators=400)
y_pred = cross_val_predict(RF, XlogP, y, cv=kf)
RMSE_RF = mean_squared_error(y, y_pred)
r2_score = r2_score(y, y_pred)
print (plt.scatter(y, y_pred) \
       , RMSE_RF \
       , r2_score)
157/79:
RF = RandomForestRegressor(n_estimators=400)
y_pred = cross_val_predict(RF, XlogP, y, cv=kf)
RMSE_RF = mean_squared_error(y, y_pred)
plt.scatter(y, y_pred)
RMSE_RF
157/80:
RF = RandomForestRegressor(n_estimators=400)
y_pred = cross_val_predict(RF, XlogP, y, cv=kf)
RMSE_RF = mean_squared_error(y, y_pred)
r2_score = r2_score(y, y_pred)
plt.scatter(y, y_pred)
print('%4f'.RMSE_RF, 'RMSE', '%4f'.r2_score)
157/81:
RF = RandomForestRegressor(n_estimators=400)
y_pred = cross_val_predict(RF, XlogP, y, cv=kf)
RMSE_RF = mean_squared_error(y, y_pred)
r2_score(y, y_pred)
plt.scatter(y, y_pred)
print('%4f'.RMSE_RF, 'RMSE', '%4f'.r2_score)
157/82:
RF = RandomForestRegressor(n_estimators=400)
y_pred = cross_val_predict(RF, XlogP, y, cv=kf)
RMSE_RF = mean_squared_error(y, y_pred)
plt.scatter(y, y_pred)
print('%4f'.RMSE_RF, 'RMSE', '%4f'.r2_score)
157/83:
RF = RandomForestRegressor(n_estimators=400)
y_pred = cross_val_predict(RF, XlogP, y, cv=kf)
RMSE_RF = mean_squared_error(y, y_pred)
plt.scatter(y, y_pred)
print('{4f}'.RMSE_RF, 'RMSE', '{%4f}'.r2_score)
157/84: r2_score(y, y_pred)
155/75:
from sklearn.cross_decomposition import PLSRegression
pls = PLSRegression(n_components=10)
y_pred = cross_val_predict(pls, X, y, cv=kf)
r2_score(y, y_pred)
155/76: plt.scatter(y, y_pred)
155/77:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
y_pred = cross_val_predict(SVR, X, y, cv=kf)
r2_score(y, y_pred)
155/78: plt.scatter(y, y_pred)
155/79:
from sklearn.svm import SVR
SVR = SVR(kernel='poly')
y_pred = cross_val_predict(SVR, X, y, cv=kf)
r2_score(y, y_pred)
155/80:
from sklearn.svm import SVR
SVR = SVR(kernel='linear')
y_pred = cross_val_predict(SVR, X, y, cv=kf)
r2_score(y, y_pred)
155/81: plt.scatter(y, y_pred)
155/82:
from sklearn.svm import SVR
SVR = SVR(kernel='sigmoid')
y_pred = cross_val_predict(SVR, X, y, cv=kf)
r2_score(y, y_pred)
155/83: plt.scatter(y, y_pred)
155/84:
from sklearn.ensemble import RandomForestRegressor
RF = RandomForestRegressor(n_estimators=600)
y_pred = cross_val_predict(RF, X, y, cv=kf)
155/85:
from sklearn.metrics import r2_score
#Rsqure
r2_score(y, y_pred)
155/86: plt.scatter(y, y_pred)
157/85:
RF = RandomForestRegressor(n_estimators=400)
y_pred = cross_val_predict(RF, XlogP, y, cv=kf)
RMSE_RF = mean_squared_error(y, y_pred)
r2_score(y, y_pred)
158/1:
df = pd.read_csv('QSAR/Kp_AtomPairs2DFingerprintCount.csv')
df
158/2:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
158/3: pwd
158/4:
df = pd.read_csv('QSAR/Kp_AtomPairs2DFingerprintCount.csv')
df
158/5:
X = df.drop(['LogKp'], axis=1)
X.shape
158/6:
y = df['LogKp']
y
158/7: y.shape
158/8:
from sklearn.model_selection import KFold, cross_val_score, cross_val_predict
kf =KFold(n_splits=10, shuffle=False)
158/9:
cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1
158/10:
from sklearn.ensemble import RandomForestRegressor
RF = RandomForestRegressor(n_estimators=600)
y_pred = cross_val_predict(RF, X, y, cv=kf)
158/11:
from sklearn.metrics import r2_score
#Rsqure
r2_score(y, y_pred)
158/12: plt.scatter(y, y_pred)
158/13:
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(y, y_pred)
158/14: RMSE_pred
158/15:
from sklearn.cross_decomposition import PLSRegression
pls = PLSRegression(n_components=10)
y_pred = cross_val_predict(pls, X, y, cv=kf)
r2_score(y, y_pred)
158/16: plt.scatter(y, y_pred)
158/17:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
y_pred = cross_val_predict(SVR, X, y, cv=kf)
r2_score(y, y_pred)
158/18: plt.scatter(y, y_pred)
158/19:
from sklearn.svm import SVR
SVR = SVR(kernel='sigmoid')
y_pred = cross_val_predict(SVR, X, y, cv=kf)
r2_score(y, y_pred)
158/20: plt.scatter(y, y_pred)
159/1:
df = pd.read_csv('QSAR/Kp_MACCSFingerprinter.csv')
df
159/2:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
159/3: pwd
159/4:
df = pd.read_csv('QSAR/Kp_MACCSFingerprinter.csv')
df
159/5:
X = df.drop(['LogKp'], axis=1)
X.shape
159/6:
y = df['LogKp']
y
159/7: y.shape
159/8:
from sklearn.model_selection import KFold, cross_val_score, cross_val_predict
kf =KFold(n_splits=10, shuffle=False)
159/9:
cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1
159/10:
from sklearn.ensemble import RandomForestRegressor
RF = RandomForestRegressor(n_estimators=600)
y_pred = cross_val_predict(RF, X, y, cv=kf)
159/11:
from sklearn.metrics import r2_score
#Rsqure
r2_score(y, y_pred)
159/12: plt.scatter(y, y_pred)
159/13:
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(y, y_pred)
159/14: RMSE_pred
159/15:
from sklearn.cross_decomposition import PLSRegression
pls = PLSRegression(n_components=10)
y_pred = cross_val_predict(pls, X, y, cv=kf)
r2_score(y, y_pred)
159/16: plt.scatter(y, y_pred)
159/17:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
y_pred = cross_val_predict(SVR, X, y, cv=kf)
r2_score(y, y_pred)
159/18: plt.scatter(y, y_pred)
159/19:
from sklearn.svm import SVR
SVR = SVR(kernel='sigmoid')
y_pred = cross_val_predict(SVR, X, y, cv=kf)
r2_score(y, y_pred)
159/20: plt.scatter(y, y_pred)
159/21:
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
X_new = SelectKBest(chi2, k=2).fit_transform(X, y)
159/22:
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
X_new = SelectKBest(chi2, k=ๅจ).fit_transform(X, y)
159/23:
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
X_new = SelectKBest(chi2, k=10).fit_transform(X, y)
159/24:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
159/25: pwd
159/26:
df = pd.read_csv('QSAR/Kp_MACCSFingerprinter.csv')
df
159/27:
X = df.drop(['LogKp'], axis=1)
X.shape
159/28:
y = df['LogKp']
y
159/29: y.shape
159/30:
from sklearn.model_selection import KFold, cross_val_score, cross_val_predict
kf =KFold(n_splits=10, shuffle=False)
159/31:
cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1
159/32:
from sklearn.ensemble import RandomForestRegressor
RF = RandomForestRegressor(n_estimators=600)
y_pred = cross_val_predict(RF, X, y, cv=kf)
159/33:
from sklearn.metrics import r2_score
#Rsqure
r2_score(y, y_pred)
159/34: plt.scatter(y, y_pred)
159/35:
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(y, y_pred)
159/36: RMSE_pred
159/37:
from sklearn.cross_decomposition import PLSRegression
pls = PLSRegression(n_components=10)
y_pred = cross_val_predict(pls, X, y, cv=kf)
r2_score(y, y_pred)
159/38: plt.scatter(y, y_pred)
159/39:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
y_pred = cross_val_predict(SVR, X, y, cv=kf)
r2_score(y, y_pred)
159/40: plt.scatter(y, y_pred)
159/41:
from sklearn.svm import SVR
SVR = SVR(kernel='sigmoid')
y_pred = cross_val_predict(SVR, X, y, cv=kf)
r2_score(y, y_pred)
159/42: plt.scatter(y, y_pred)
159/43:
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
X_new = SelectKBest(chi2, k=10).fit_transform(X, y)
159/44:
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
X = X.astype(float)
y = y.astype(float)
X_new = SelectKBest(chi2, k=10).fit_transform(X, y)
159/45:
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
X = X.astype(float)
y = y.astype(float)
SelectKBest(chi2, k=10).fit_transform(X, y)
159/46:
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

SelectKBest(chi2, k=10).fit_transform(X, y)
159/47:
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
159/48: SelectKBest(chi2)
159/49: SelectKBest(chi2, X, y)
159/50: SelectKBest(chi2).fit_transform(X, y)
159/51: SelectKBest(chi2).fit_transform(X)
159/52: SelectKBest(chi2).fit_transform(X, y)
159/53: X.sh
159/54: X.shape
159/55: y.shape
159/56: SelectKBest(ch2).fit_transform(X, y)
159/57:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(.8 * (1 - .8)))
159/58:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(.8 * (1 - .8)))
sel.fit_transform(X)
159/59:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(.8 * (1 - .8)))
des2 = sel.fit_transform(X)
159/60: des2
159/61: des2.to_frame()
159/62:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(.8 * (1 - .8)))
X = sel.fit_transform(X)
159/63: X
159/64:
from sklearn.svm import SVR
SVR = SVR(kernel='sigmoid')
y_pred = cross_val_predict(SVR, X, y, cv=kf)
r2_score(y, y_pred)
159/65: plt.scatter(y, y_pred)
159/66: X.shape
159/67: plt.scatter(y, y_pred)
159/68:
from sklearn.svm import SVR
SVR = SVR(kernel='sigmoid')
y_pred = cross_val_predict(RF, X, y, cv=kf)
r2_score(y, y_pred)
159/69: plt.scatter(y, y_pred)
159/70:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3
                                                    random_state=29)
159/71:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,
                                                    random_state=29)
159/72:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,
                                                    random_state=10)
159/73: pls.fit_transform(X_train, y_train)
159/74:
import matplotlib.pyplot as plt

coefs = pd.DataFrame(
   RF.coef_,
   columns=['Coefficients'], index=X_train.columns
)

coefs.plot(kind='barh', figsize=(9, 7))
plt.title('Ridge model')
plt.axvline(x=0, color='.5')
plt.subplots_adjust(left=.3)
159/75:
import matplotlib.pyplot as plt


coefs = pd.DataFrame(
   RF.feature_importances_,
   columns=['Coefficients'], index=X_train.columns
)

coefs.plot(kind='barh', figsize=(9, 7))
plt.title('Ridge model')
plt.axvline(x=0, color='.5')
plt.subplots_adjust(left=.3)
159/76:
import matplotlib.pyplot as plt
X_internal, X_external, Y_internal, Y_external = train_test_split(X,
                                                        Y, test_size=0.3,
                                                        random_state=seed)

coefs = pd.DataFrame(
   RF.feature_importances_,
   columns=['Coefficients'], index=X_train.columns
)

coefs.plot(kind='barh', figsize=(9, 7))
plt.title('Ridge model')
plt.axvline(x=0, color='.5')
plt.subplots_adjust(left=.3)
159/77:
import matplotlib.pyplot as plt
X_internal, X_external, Y_internal, Y_external = train_test_split(X,
                                                        y, test_size=0.3,
                                                        random_state=seed)

coefs = pd.DataFrame(
   RF.feature_importances_,
   columns=['Coefficients'], index=X_train.columns
)

coefs.plot(kind='barh', figsize=(9, 7))
plt.title('Ridge model')
plt.axvline(x=0, color='.5')
plt.subplots_adjust(left=.3)
159/78:
import matplotlib.pyplot as plt
X_internal, X_external, Y_internal, Y_external = train_test_split(X,
                                                        y, test_size=0.3)

coefs = pd.DataFrame(
   RF.feature_importances_,
   columns=['Coefficients'], index=X_train.columns
)

coefs.plot(kind='barh', figsize=(9, 7))
plt.title('Ridge model')
plt.axvline(x=0, color='.5')
plt.subplots_adjust(left=.3)
159/79:
import matplotlib.pyplot as plt
X_internal, X_external, Y_internal, Y_external = train_test_split(X,
                                                        y, test_size=0.3)
RF.fit(X_internal, Y_internal)
coefs = pd.DataFrame(
   RF.feature_importances_,
   columns=['Coefficients'], index=X_train.columns
)

coefs.plot(kind='barh', figsize=(9, 7))
plt.title('Ridge model')
plt.axvline(x=0, color='.5')
plt.subplots_adjust(left=.3)
159/80: RF.feature_importances_
159/81:
def Remove_useless_descriptor(df, threshold):
    
    des1 = len(df.columns) 
    
    h = df.columns.tolist()
    df = df.values.astype(np.float)
    df = np.array(df)

    STDEV = np.std(df, axis=0)
    idx = [idx for idx, val in enumerate(STDEV) if val > threshold]
    df2 = df[:,idx]
    hx = np.array(h)[idx]
    
    df = pd.DataFrame(df2, columns=[hx])
    
    des2 = len(df.columns)
    
    print('from Remove useless descriptor')
    print("The initial set of " + str(des1) + \
          " descriptors has been reduced to " + str(des2) + " descriptors.")
    
    return df, des1, des2
159/82: Remove_useless_descriptor(df, 0.05)
159/83: df 2 =Remove_useless_descriptor(df, 0.05)
159/84: df2 = Remove_useless_descriptor(df, 0.05)
159/85: df2
159/86: df2.to_frame
159/87: df2
159/88: df3 = pd.DataFrame(df2)
159/89: df3
159/90:
def Remove_useless_descriptor(df, threshold):
    
    des1 = len(df.columns) 
    
    h = df.columns.tolist()
    df = df.values.astype(np.float)
    df = np.array(df)

    STDEV = np.std(df, axis=0)
    idx = [idx for idx, val in enumerate(STDEV) if val > threshold]
    df2 = df[:,idx]
    hx = np.array(h)[idx]
    
    df = pd.DataFrame(df, columns=[hx])
    
    des2 = len(df.columns)
    
    print('from Remove useless descriptor')
    print("The initial set of " + str(des1) + \
          " descriptors has been reduced to " + str(des2) + " descriptors.")
    
    return df, des1, des2
159/91: RF.feature_importances_
159/92: df2 = Remove_useless_descriptor(df, 0.05)
159/93:
def Remove_useless_descriptor(df, threshold):
    
    des1 = len(df.columns) 
    
    h = df.columns.tolist()
    df = df.values.astype(np.float)
    df = np.array(df)

    STDEV = np.std(df, axis=0)
    idx = [idx for idx, val in enumerate(STDEV) if val > threshold]
    df2 = df[:,idx]
    hx = np.array(h)[idx]
    
    df = pd.DataFrame(df2, columns=[hx])
    
    des2 = len(df.columns)
    
    print('from Remove useless descriptor')
    print("The initial set of " + str(des1) + \
          " descriptors has been reduced to " + str(des2) + " descriptors.")
    
    return df, des1, des2
159/94: df2 = Remove_useless_descriptor(df, 0.05)
159/95: df2
159/96:
def Remove_useless_descriptor(df, threshold):
    
    des1 = len(df.columns) 
    
    h = df.columns.tolist()
    df = df.values.astype(np.float)
    df = np.array(df)

    STDEV = np.std(df, axis=0)
    idx = [idx for idx, val in enumerate(STDEV) if val > threshold]
    df2 = df[:,idx]
    hx = np.array(h)[idx]
    
    df = pd.DataFrame([df2], columns=[hx])
    
    des2 = len(df.columns)
    
    print('from Remove useless descriptor')
    print("The initial set of " + str(des1) + \
          " descriptors has been reduced to " + str(des2) + " descriptors.")
    
    return df, des1, des2
159/97: RF.feature_importances_
159/98: df2 = Remove_useless_descriptor(df, 0.05)
159/99:
def Remove_useless_descriptor(df, threshold):
    
    des1 = len(df.columns) 
    
    h = df.columns.tolist()
    df = df.values.astype(np.float)
    df = np.array(df)

    STDEV = np.std(df, axis=0)
    idx = [idx for idx, val in enumerate(STDEV) if val > threshold]
    df2 = df[:,idx]
    hx = np.array(h)[idx]
    
    df = pd.DataFrame(df2, columns=[hx])
    
    des2 = len(df.columns)
    
    print('from Remove useless descriptor')
    print("The initial set of " + str(des1) + \
          " descriptors has been reduced to " + str(des2) + " descriptors.")
    
    return df, des1, des2
159/100: RF.feature_importances_
159/101: df2 = Remove_useless_descriptor(df, 0.05)
159/102: df2.
159/103: df2
159/104: df2 = pd.DataFrame(df2, columns=[hx]
159/105: df2 = pd.DataFrame(df2, columns=df[h]
159/106: df2 = pd.DataFrame(df2, columns=df[h])
159/107: df2 = pd.DataFrame(df2, columns=df.head())
159/108:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05)
X = sel.fit_transform(X)
159/109: X.shape
159/110: y.shape
159/111:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05)
X = sel.fit_transform(X)
159/112:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
159/113: pwd
159/114:
df = pd.read_csv('QSAR/Kp_MACCSFingerprinter.csv')
df
159/115:
X = df.drop(['LogKp'], axis=1)
X.shape
159/116:
y = df['LogKp']
y
159/117: y.shape
159/118:
from sklearn.model_selection import KFold, cross_val_score, cross_val_predict
kf =KFold(n_splits=10, shuffle=False)
159/119:
cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1
159/120:
from sklearn.ensemble import RandomForestRegressor
RF = RandomForestRegressor(n_estimators=600)
y_pred = cross_val_predict(RF, X, y, cv=kf)
159/121:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05)
X = sel.fit_transform(X)
159/122:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05)
X =sel.fit_transform(X)
159/123:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05)
                        sel.fit_transform(X)
159/124:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05)
sel.fit_transform(X)
159/125:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05)
159/126:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
X = sel.fit_transform(X)
159/127:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
X = sel.fit_transform(X)
X.shape
159/128:
from sklearn.model_selection import KFold, cross_val_score, cross_val_predict
kf =KFold(n_splits=10, shuffle=False)
159/129:
cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1
159/130:
from sklearn.ensemble import RandomForestRegressor
RF = RandomForestRegressor(n_estimators=600)
y_pred = cross_val_predict(RF, X, y, cv=kf)
159/131:
from sklearn.metrics import r2_score
#Rsqure
r2_score(y, y_pred)
159/132: plt.scatter(y, y_pred)
159/133:
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(y, y_pred)
159/134: RMSE_pred
159/135:
from sklearn.cross_decomposition import PLSRegression
pls = PLSRegression(n_components=10)
y_pred = cross_val_predict(pls, X, y, cv=kf)
r2_score(y, y_pred)
159/136: plt.scatter(y, y_pred)
159/137:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
y_pred = cross_val_predict(SVR, X, y, cv=kf)
r2_score(y, y_pred)
159/138: plt.scatter(y, y_pred)
159/139:
from sklearn.svm import SVR
SVR = SVR(kernel='sigmoid')
y_pred = cross_val_predict(SVR, X, y, cv=kf)
r2_score(y, y_pred)
159/140: plt.scatter(y, y_pred)
159/141:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05)
X = sel.fit_transform(X)
159/142: X.shape
159/143:
from sklearn.svm import SVR
SVR = SVR(kernel='sigmoid')
y_pred = cross_val_predict(RF, X, y, cv=kf)
r2_score(y, y_pred)
159/144: plt.scatter(y, y_pred)
160/1:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
160/2: pwd
160/3:
df = pd.read_csv('QSAR/Kp_AtomPairs2DFingerprinter.csv')
df
160/4:
X = df.drop(['LogKp'], axis=1)
X.shape
160/5:
y = df['LogKp']
y
160/6: y.shape
160/7:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
X = sel.fit_transform(X)
X.shape
160/8:
from sklearn.model_selection import KFold, cross_val_score, cross_val_predict
kf =KFold(n_splits=10, shuffle=False)
160/9:
cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1
160/10:
from sklearn.ensemble import RandomForestRegressor
RF = RandomForestRegressor(n_estimators=600)
y_pred = cross_val_predict(RF, X, y, cv=kf)
160/11:
from sklearn.metrics import r2_score
#Rsqure
r2_score(y, y_pred)
160/12: plt.scatter(y, y_pred)
160/13:
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(y, y_pred)
160/14: RMSE_pred
160/15:
from sklearn.cross_decomposition import PLSRegression
pls = PLSRegression(n_components=10)
y_pred = cross_val_predict(pls, X, y, cv=kf)
r2_score(y, y_pred)
160/16: plt.scatter(y, y_pred)
160/17:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
y_pred = cross_val_predict(SVR, X, y, cv=kf)
r2_score(y, y_pred)
160/18: plt.scatter(y, y_pred)
160/19:
from sklearn.svm import SVR
SVR = SVR(kernel='sigmoid')
y_pred = cross_val_predict(SVR, X, y, cv=kf)
r2_score(y, y_pred)
160/20: plt.scatter(y, y_pred)
160/21: from sklearn.feature_selection import VarianceThreshold
161/1:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
161/2: pwd
161/3:
df = pd.read_csv('QSAR/Kp_AtomPairs2DFingerprintCount.csv')
df
161/4:
X = df.drop(['LogKp'], axis=1)
X.shape
161/5:
y = df['LogKp']
y
161/6: y.shape
161/7:
X = df.drop(['LogKp'], axis=1)
X.shape
161/8:
y = df['LogKp']
y
161/9: y.shape
161/10:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
X = sel.fit_transform(X)
X.shape
161/11:
from sklearn.model_selection import KFold, cross_val_score, cross_val_predict
kf =KFold(n_splits=10, shuffle=False)
161/12:
cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1
161/13:
from sklearn.ensemble import RandomForestRegressor
RF = RandomForestRegressor(n_estimators=600)
y_pred = cross_val_predict(RF, X, y, cv=kf)
161/14:
from sklearn.metrics import r2_score
#Rsqure
r2_score(y, y_pred)
161/15: plt.scatter(y, y_pred)
161/16:
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(y, y_pred)
161/17: RMSE_pred
161/18:
from sklearn.cross_decomposition import PLSRegression
pls = PLSRegression(n_components=10)
y_pred = cross_val_predict(pls, X, y, cv=kf)
r2_score(y, y_pred)
161/19: plt.scatter(y, y_pred)
161/20:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
y_pred = cross_val_predict(SVR, X, y, cv=kf)
r2_score(y, y_pred)
161/21: plt.scatter(y, y_pred)
161/22:
from sklearn.svm import SVR
SVR = SVR(kernel='sigmoid')
y_pred = cross_val_predict(SVR, X, y, cv=kf)
r2_score(y, y_pred)
161/23: plt.scatter(y, y_pred)
161/24:
from sklearn.ensemble import RandomForestRegressor
RF = RandomForestRegressor(n_estimators=10)
y_pred = cross_val_predict(RF, X, y, cv=kf)
161/25:
from sklearn.metrics import r2_score
#Rsqure
r2_score(y, y_pred)
161/26: plt.scatter(y, y_pred)
161/27:
from sklearn.ensemble import RandomForestRegressor
RF = RandomForestRegressor(n_estimators=100)
y_pred = cross_val_predict(RF, X, y, cv=kf)
161/28:
from sklearn.metrics import r2_score
#Rsqure
r2_score(y, y_pred)
161/29: plt.scatter(y, y_pred)
162/1:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
162/2: pwd
162/3:
df = pd.read_csv('QSAR/Kp_Fingerprinter.csv')
df
162/4:
X = df.drop(['LogKp'], axis=1)
X.shape
162/5:
y = df['LogKp']
y
162/6: y.shape
162/7:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
X = sel.fit_transform(X)
X.shape
162/8:
from sklearn.model_selection import KFold, cross_val_score, cross_val_predict
kf =KFold(n_splits=10, shuffle=False)
162/9:
cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1
162/10:
from sklearn.ensemble import RandomForestRegressor
RF = RandomForestRegressor(n_estimators=600)
y_pred = cross_val_predict(RF, X, y, cv=kf)
162/11:
from sklearn.metrics import r2_score
#Rsqure
r2_score(y, y_pred)
162/12: plt.scatter(y, y_pred)
162/13:
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(y, y_pred)
162/14: RMSE_pred
162/15:
from sklearn.cross_decomposition import PLSRegression
pls = PLSRegression(n_components=10)
y_pred = cross_val_predict(pls, X, y, cv=kf)
r2_score(y, y_pred)
162/16: plt.scatter(y, y_pred)
162/17:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
y_pred = cross_val_predict(SVR, X, y, cv=kf)
r2_score(y, y_pred)
162/18: plt.scatter(y, y_pred)
162/19:
from sklearn.svm import SVR
SVR = SVR(kernel='sigmoid')
y_pred = cross_val_predict(SVR, X, y, cv=kf)
r2_score(y, y_pred)
162/20: plt.scatter(y, y_pred)
162/21:
from sklearn.svm import SVR
SVR = SVR(kernel='sigmoid')
y_pred = cross_val_predict(RF, X, y, cv=kf)
r2_score(y, y_pred)
159/145:
from sklearn.svm import SVR
SVR = SVR(kernel='linear')
y_pred = cross_val_predict(RF, X, y, cv=kf)
r2_score(y, y_pred)
159/146: plt.scatter(y, y_pred)
162/22: plt.scatter(y, y_pred)
162/23:
from sklearn.svm import SVR
SVR = SVR(kernel='poly')
y_pred = cross_val_predict(RF, X, y, cv=kf)
r2_score(y, y_pred)
163/1:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
163/2: pwd
163/3:
df = pd.read_csv('QSAR/Kp_ExtendedFingerprinter.csv')
df
163/4:
X = df.drop(['LogKp'], axis=1)
X.shape
163/5:
y = df['LogKp']
y
163/6: y.shape
163/7:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
X = sel.fit_transform(X)
X.shape
163/8:
from sklearn.model_selection import KFold, cross_val_score, cross_val_predict
kf =KFold(n_splits=10, shuffle=False)
163/9:
cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1
163/10:
from sklearn.ensemble import RandomForestRegressor
RF = RandomForestRegressor(n_estimators=600)
y_pred = cross_val_predict(RF, X, y, cv=kf)
162/24: plt.scatter(y, y_pred)
163/11:
from sklearn.metrics import r2_score
#Rsqure
r2_score(y, y_pred)
163/12: plt.scatter(y, y_pred)
163/13:
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(y, y_pred)
163/14: RMSE_pred
163/15:
from sklearn.cross_decomposition import PLSRegression
pls = PLSRegression(n_components=10)
y_pred = cross_val_predict(pls, X, y, cv=kf)
r2_score(y, y_pred)
163/16: plt.scatter(y, y_pred)
163/17:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
y_pred = cross_val_predict(SVR, X, y, cv=kf)
r2_score(y, y_pred)
163/18: plt.scatter(y, y_pred)
163/19:
from sklearn.svm import SVR
SVR = SVR(kernel='sigmoid')
y_pred = cross_val_predict(SVR, X, y, cv=kf)
r2_score(y, y_pred)
163/20: plt.scatter(y, y_pred)
163/21:
from sklearn.svm import SVR
SVR = SVR(kernel='poly')
y_pred = cross_val_predict(RF, X, y, cv=kf)
r2_score(y, y_pred)
163/22: plt.scatter(y, y_pred)
164/1:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
164/2: pwd
164/3:
df = pd.read_csv('QSAR/Kp_EStateFingerprinter.csv')
df
164/4:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
164/5: pwd
164/6:
df = pd.read_csv('QSAR/Kp_EStateFingerprinter.csv')
df
164/7:
X = df.drop(['LogKp'], axis=1)
X.shape
164/8:
y = df['LogKp']
y
164/9: y.shape
164/10:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
X = sel.fit_transform(X)
X.shape
164/11:
from sklearn.model_selection import KFold, cross_val_score, cross_val_predict
kf =KFold(n_splits=10, shuffle=False)
164/12:
cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1
164/13:
from sklearn.ensemble import RandomForestRegressor
RF = RandomForestRegressor(n_estimators=600)
y_pred = cross_val_predict(RF, X, y, cv=kf)
164/14:
from sklearn.metrics import r2_score
#Rsqure
r2_score(y, y_pred)
164/15: plt.scatter(y, y_pred)
164/16:
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(y, y_pred)
164/17: RMSE_pred
164/18:
from sklearn.cross_decomposition import PLSRegression
pls = PLSRegression(n_components=10)
y_pred = cross_val_predict(pls, X, y, cv=kf)
r2_score(y, y_pred)
164/19: plt.scatter(y, y_pred)
164/20:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
y_pred = cross_val_predict(SVR, X, y, cv=kf)
r2_score(y, y_pred)
164/21: plt.scatter(y, y_pred)
164/22:
from sklearn.svm import SVR
SVR = SVR(kernel='sigmoid')
y_pred = cross_val_predict(SVR, X, y, cv=kf)
r2_score(y, y_pred)
164/23: plt.scatter(y, y_pred)
164/24:
from sklearn.svm import SVR
SVR = SVR(kernel='poly')
y_pred = cross_val_predict(RF, X, y, cv=kf)
r2_score(y, y_pred)
164/25: plt.scatter(y, y_pred)
165/1:
df = pd.read_csv('QSAR/Kp_GraphOnlyFingerprinter.csv')
df
165/2:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
165/3: pwd
165/4:
df = pd.read_csv('QSAR/Kp_GraphOnlyFingerprinter.csv')
df
165/5:
X = df.drop(['LogKp'], axis=1)
X.shape
165/6:
y = df['LogKp']
y
165/7: y.shape
165/8:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
X = sel.fit_transform(X)
X.shape
165/9:
from sklearn.model_selection import KFold, cross_val_score, cross_val_predict
kf =KFold(n_splits=10, shuffle=False)
165/10:
cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1
165/11:
from sklearn.ensemble import RandomForestRegressor
RF = RandomForestRegressor(n_estimators=600)
y_pred = cross_val_predict(RF, X, y, cv=kf)
165/12:
from sklearn.metrics import r2_score
#Rsqure
r2_score(y, y_pred)
165/13: plt.scatter(y, y_pred)
165/14:
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(y, y_pred)
165/15: RMSE_pred
165/16:
from sklearn.cross_decomposition import PLSRegression
pls = PLSRegression(n_components=10)
y_pred = cross_val_predict(pls, X, y, cv=kf)
r2_score(y, y_pred)
165/17: plt.scatter(y, y_pred)
165/18:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
y_pred = cross_val_predict(SVR, X, y, cv=kf)
r2_score(y, y_pred)
165/19: plt.scatter(y, y_pred)
165/20:
from sklearn.svm import SVR
SVR = SVR(kernel='sigmoid')
y_pred = cross_val_predict(SVR, X, y, cv=kf)
r2_score(y, y_pred)
165/21: plt.scatter(y, y_pred)
165/22:
from sklearn.svm import SVR
SVR = SVR(kernel='poly')
y_pred = cross_val_predict(RF, X, y, cv=kf)
r2_score(y, y_pred)
165/23: plt.scatter(y, y_pred)
166/1:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
166/2: pwd
166/3:
df = pd.read_csv('QSAR/Kp_PubchemFingerprinter.csv')
df
166/4:
X = df.drop(['LogKp'], axis=1)
X.shape
166/5:
y = df['LogKp']
y
166/6: y.shape
166/7:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
X = sel.fit_transform(X)
X.shape
166/8:
from sklearn.model_selection import KFold, cross_val_score, cross_val_predict
kf =KFold(n_splits=10, shuffle=False)
166/9:
cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1
166/10:
from sklearn.ensemble import RandomForestRegressor
RF = RandomForestRegressor(n_estimators=600)
y_pred = cross_val_predict(RF, X, y, cv=kf)
166/11:
from sklearn.metrics import r2_score
#Rsqure
r2_score(y, y_pred)
166/12: plt.scatter(y, y_pred)
166/13:
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(y, y_pred)
166/14: RMSE_pred
166/15:
from sklearn.cross_decomposition import PLSRegression
pls = PLSRegression(n_components=10)
y_pred = cross_val_predict(pls, X, y, cv=kf)
r2_score(y, y_pred)
166/16: plt.scatter(y, y_pred)
166/17:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
y_pred = cross_val_predict(SVR, X, y, cv=kf)
r2_score(y, y_pred)
166/18: plt.scatter(y, y_pred)
166/19:
from sklearn.svm import SVR
SVR = SVR(kernel='sigmoid')
y_pred = cross_val_predict(SVR, X, y, cv=kf)
r2_score(y, y_pred)
166/20: plt.scatter(y, y_pred)
166/21:
from sklearn.svm import SVR
SVR = SVR(kernel='poly')
y_pred = cross_val_predict(RF, X, y, cv=kf)
r2_score(y, y_pred)
165/24:
from sklearn.linear_model import LinearRegression
Lr = LinearRegression()
y_pred = cross_val_predict(Lr, X, y, cv=kf)
r2_score(y, y_pred)
165/25: plt.scatter(y, y_pred)
166/22: plt.scatter(y, y_pred)
166/23: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
166/24: Fit = RF.fit(X_train, y_train)
166/25:
# Get numerical feature importances
importances = list(RF.feature_importances_)# List of tuples with variable and importance
feature_importances = [(feature, round(importance, 2)) 
for feature, importance in zip(feature_list, importances)]

# Sort the feature importances by most important first

feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)

# Print out the feature and importances 
[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];
166/26: RF.fit(X_train, y_train)
166/27: Fit = RF.fit(X_train, y_train)
166/28:
Fit = RF.fit(X_train, y_train)
Fit
166/29:
RF.fit(X_train, y_train)
r2 = RF.score(X_train,y_train)
r2
166/30:
result = permutation_importance(RF, X_test, y_test, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()

fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=X_test.columns[sorted_idx])
ax.set_title("Permutation Importances (test set)")
fig.tight_layout()
plt.show()
166/31:
import matplotlib.pyplot as plt
import numpy as np
from sklearn.datasets import fetch_openml
from sklearn.ensemble import RandomForestClassifier
from sklearn.impute import SimpleImputer
from sklearn.inspection import permutation_importance
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
result = permutation_importance(RF, X_test, y_test, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()

fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=X_test.columns[sorted_idx])
ax.set_title("Permutation Importances (test set)")
fig.tight_layout()
plt.show()
166/32:
import matplotlib.pyplot as plt
import numpy as np
from sklearn.datasets import fetch_openml
from sklearn.ensemble import RandomForestClassifier
from sklearn.impute import SimpleImputer
from sklearn.inspection import permutation_importance
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder

result = permutation_importance(RF, X_test, y_test, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()

fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=X.columns[sorted_idx])
ax.set_title("Permutation Importances (test set)")
fig.tight_layout()
plt.show()
166/33: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
166/34:
RF.fit(X_train, y_train)
r2 = RF.score(X_train,y_train)
r2
166/35: y_pred = RF.predict(X_test)
166/36:
y_pred = RF.predict(X_test)
r2_score(y,y_pred)
166/37:
y_pred = RF.predict(X_test)
r2_score(y, _pred)
166/38:
y_pred = RF.predict(X_test)
r2_score(y, y_pred)
166/39: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
166/40:
RF.fit(X_train, y_train)
r2 = RF.score(X_train,y_train)
r2
166/41:
y_pred = RF.predict(X_test)
r2_score(y, y_pred)
166/42:
y_pred = RF.predict(X_test)
y_pred
166/43:
y_pred = RF.predict(X_test)
y_pred.sha
166/44:
y_pred = RF.predict(X_test)
y_pred.shape
166/45:
y_pred = RF.predict(X_train)
y_pred.shape
166/46:
y_pred = RF.predict(X_test)
y_pred.shape
166/47:
y_pred = RF.predict(y_test)
y_pred.shape
166/48:
y_pred = RF.predict(X_test)
y_pred.shape
166/49: r2_score(y_test, y_pred)
166/50:
import matplotlib.pyplot as plt
impshape numpy as np
from sklearn.datasets import fetch_openml
from sklearn.ensemble import RandomForestClassifier
from sklearn.impute import SimpleImputer
from sklearn.inspection import permutation_importance
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder

result = permutation_importance(RF, X_test, y_test, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()

fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=X.columns[sorted_idx])
ax.set_title("Permutation Importances (test set)")
fig.tight_layout()
plt.show()
166/51:
from sklearn.inspection import permutation_importance
from sklearn.compose import ColumnTransformer

result = permutation_importance(RF, X_test, y_test, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()

fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=X.columns[sorted_idx])
ax.set_title("Permutation Importances (test set)")
fig.tight_layout()
plt.show()
168/1:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
168/2: pwd
168/3:
df = pd.read_csv('QSAR/Kp_SubstructureFingerprinter.csv')
df
168/4:
X = df.drop(['LogKp'], axis=1)
X.shape
168/5:
y = df['LogKp']
y
168/6: y.shape
168/7:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
X = sel.fit_transform(X)
X.shape
168/8:
from sklearn.model_selection import KFold, cross_val_score, cross_val_predict
kf =KFold(n_splits=10, shuffle=False)
168/9:
cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1
168/10:
from sklearn.ensemble import RandomForestRegressor
RF = RandomForestRegressor(n_estimators=600)
y_pred = cross_val_predict(RF, X, y, cv=kf)
168/11:
from sklearn.metrics import r2_score
#Rsqure
r2_score(y, y_pred)
168/12: plt.scatter(y, y_pred)
168/13:
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(y, y_pred)
168/14: RMSE_pred
168/15:
from sklearn.cross_decomposition import PLSRegression
pls = PLSRegression(n_components=10)
y_pred = cross_val_predict(pls, X, y, cv=kf)
r2_score(y, y_pred)
168/16: plt.scatter(y, y_pred)
168/17:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
y_pred = cross_val_predict(SVR, X, y, cv=kf)
r2_score(y, y_pred)
168/18: plt.scatter(y, y_pred)
168/19:
from sklearn.svm import SVR
SVR = SVR(kernel='sigmoid')
y_pred = cross_val_predict(SVR, X, y, cv=kf)
r2_score(y, y_pred)
168/20: plt.scatter(y, y_pred)
168/21:
from sklearn.svm import SVR
SVR = SVR(kernel='poly')
y_pred = cross_val_predict(RF, X, y, cv=kf)
r2_score(y, y_pred)
168/22: plt.scatter(y, y_pred)
168/23: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
168/24:
RF.fit(X_train, y_train)
r2 = RF.score(X_train,y_train)
r2
168/25:
y_pred = RF.predict(X_test)
y_pred.shape
168/26: r2_score(y_test, y_pred)
168/27:
from sklearn.inspection import permutation_importance
from sklearn.compose import ColumnTransformer

result = permutation_importance(RF, X_test, y_test, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()

fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=X.columns[sorted_idx])
ax.set_title("Permutation Importances (test set)")
fig.tight_layout()
plt.show()
168/28:
from sklearn.inspection import permutation_importance
from sklearn.compose import ColumnTransformer

result = permutation_importance(RF, X_test, y_test, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()

fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=X_test.columns[sorted_idx])
ax.set_title("Permutation Importances (test set)")
fig.tight_layout()
plt.show()
168/29:
from sklearn.inspection import permutation_importance
from sklearn.compose import ColumnTransformer

result = permutation_importance(RF, X_test, y_test, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()

fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False)
ax.set_title("Permutation Importances (test set)")
fig.tight_layout()
plt.show()
168/30:
from sklearn.inspection import permutation_importance
from sklearn.compose import ColumnTransformer

result = permutation_importance(RF, X_test, y_test, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()

fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=X_test.head())
ax.set_title("Permutation Importances (test set)")
fig.tight_layout()
plt.show()
168/31:
from sklearn.inspection import permutation_importance
from sklearn.compose import ColumnTransformer

result = permutation_importance(RF, X_test, y_test, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()

fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=X_test.[sorted_idx])
ax.set_title("Permutation Importances (test set)")
fig.tight_layout()
plt.show()
168/32:
from sklearn.inspection import permutation_importance
from sklearn.compose import ColumnTransformer

result = permutation_importance(RF, X_test, y_test, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()
168/33:
fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=X.[sorted_idx])
ax.set_title("Permutation Importances (test set)")
fig.tight_layout()
plt.show()
168/34:
fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=X.c[sorted_idx])
ax.set_title("Permutation Importances (test set)")
fig.tight_layout()
plt.show()
168/35:
fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=X.column[sorted_idx])
ax.set_title("Permutation Importances (test set)")
fig.tight_layout()
plt.show()
168/36:
fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=X.columns[sorted_idx])
ax.set_title("Permutation Importances (test set)")
fig.tight_layout()
plt.show()
168/37:
X = df.drop(['LogKp'], axis=1)
X.shape
168/38:
X = df.drop(['LogKp'], axis=1)
X.shape
X
168/39:
X = df.drop(['LogKp'], axis=1)
X.shape
168/40:
fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=X.columns[sorted_idx])
ax.set_title("Permutation Importances (test set)")
fig.tight_layout()
plt.show()
168/41:
from sklearn.inspection import permutation_importance
from sklearn.compose import ColumnTransformer

result = permutation_importance(RF, X_test, y_test, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()
fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=X.columns[sorted_idx])
ax.set_title("Permutation Importances (test set)")
fig.tight_layout()
plt.show()
169/1:
df = pd.read_csv('QSAR/Kp_SubstructureFingerprintCount.csv')
df
169/2:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
169/3: pwd
169/4:
df = pd.read_csv('QSAR/Kp_SubstructureFingerprintCount.csv')
df
169/5:
X = df.drop(['LogKp'], axis=1)
X.shape
169/6:
y = df['LogKp']
y
169/7: y.shape
169/8:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
X = sel.fit_transform(X)
X.shape
169/9:
from sklearn.model_selection import KFold, cross_val_score, cross_val_predict
kf =KFold(n_splits=10, shuffle=False)
169/10:
cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1
169/11:
from sklearn.ensemble import RandomForestRegressor
RF = RandomForestRegressor(n_estimators=600)
y_pred = cross_val_predict(RF, X, y, cv=kf)
169/12:
from sklearn.metrics import r2_score
#Rsqure
r2_score(y, y_pred)
169/13: plt.scatter(y, y_pred)
169/14:
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(y, y_pred)
169/15: RMSE_pred
169/16:
from sklearn.cross_decomposition import PLSRegression
pls = PLSRegression(n_components=10)
y_pred = cross_val_predict(pls, X, y, cv=kf)
r2_score(y, y_pred)
169/17: plt.scatter(y, y_pred)
169/18:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
y_pred = cross_val_predict(SVR, X, y, cv=kf)
r2_score(y, y_pred)
169/19: plt.scatter(y, y_pred)
169/20:
from sklearn.svm import SVR
SVR = SVR(kernel='sigmoid')
y_pred = cross_val_predict(SVR, X, y, cv=kf)
r2_score(y, y_pred)
169/21: plt.scatter(y, y_pred)
169/22:
from sklearn.svm import SVR
SVR = SVR(kernel='poly')
y_pred = cross_val_predict(RF, X, y, cv=kf)
r2_score(y, y_pred)
169/23: plt.scatter(y, y_pred)
169/24: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
169/25:
RF.fit(X_train, y_train)
r2 = RF.score(X_train,y_train)
r2
169/26:
y_pred = RF.predict(X_test)
y_pred.shape
169/27: r2_score(y_test, y_pred)
169/28:
from sklearn.inspection import permutation_importance
from sklearn.compose import ColumnTransformer

result = permutation_importance(RF, X_test, y_test, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()
170/1:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
170/2: pwd
170/3:
df = pd.read_csv('QSAR/Kp_KlekotaRothFingerprinter.csv')
df
170/4:
X = df.drop(['LogKp'], axis=1)
X.shape
170/5:
y = df['LogKp']
y
170/6: y.shape
170/7:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
X = sel.fit_transform(X)
X.shape
170/8:
from sklearn.model_selection import KFold, cross_val_score, cross_val_predict
kf =KFold(n_splits=10, shuffle=False)
170/9:
cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1
170/10:
from sklearn.ensemble import RandomForestRegressor
RF = RandomForestRegressor(n_estimators=600)
y_pred = cross_val_predict(RF, X, y, cv=kf)
169/29:
fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=df.columns[sorted_idx])
ax.set_title("Permutation Importances (test set)")
fig.tight_layout()
plt.show()
171/1:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
170/11:
from sklearn.metrics import r2_score
#Rsqure
r2_score(y, y_pred)
170/12: plt.scatter(y, y_pred)
170/13:
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(y, y_pred)
170/14: RMSE_pred
170/15:
from sklearn.cross_decomposition import PLSRegression
pls = PLSRegression(n_components=10)
y_pred = cross_val_predict(pls, X, y, cv=kf)
r2_score(y, y_pred)
170/16: plt.scatter(y, y_pred)
170/17:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
y_pred = cross_val_predict(SVR, X, y, cv=kf)
r2_score(y, y_pred)
170/18: plt.scatter(y, y_pred)
171/2: pwd
171/3:
df = pd.read_csv('QSAR/Kp_KlekotaRothFingerprintCount.csv')
df
170/19:
from sklearn.svm import SVR
SVR = SVR(kernel='sigmoid')
y_pred = cross_val_predict(SVR, X, y, cv=kf)
r2_score(y, y_pred)
170/20: plt.scatter(y, y_pred)
170/21:
from sklearn.svm import SVR
SVR = SVR(kernel='poly')
y_pred = cross_val_predict(RF, X, y, cv=kf)
r2_score(y, y_pred)
171/4:
X = df.drop(['LogKp'], axis=1)
X.shape
171/5:
y = df['LogKp']
y
171/6: y.shape
171/7:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
X = sel.fit_transform(X)
X.shape
171/8:
from sklearn.model_selection import KFold, cross_val_score, cross_val_predict
kf =KFold(n_splits=10, shuffle=False)
171/9:
cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1
171/10:
from sklearn.ensemble import RandomForestRegressor
RF = RandomForestRegressor(n_estimators=600)
y_pred = cross_val_predict(RF, X, y, cv=kf)
171/11:
from sklearn.metrics import r2_score
#Rsqure
r2_score(y, y_pred)
171/12: plt.scatter(y, y_pred)
171/13:
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(y, y_pred)
171/14: RMSE_pred
171/15:
from sklearn.cross_decomposition import PLSRegression
pls = PLSRegression(n_components=10)
y_pred = cross_val_predict(pls, X, y, cv=kf)
r2_score(y, y_pred)
171/16: plt.scatter(y, y_pred)
171/17:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
y_pred = cross_val_predict(SVR, X, y, cv=kf)
r2_score(y, y_pred)
171/18: plt.scatter(y, y_pred)
171/19:
from sklearn.svm import SVR
SVR = SVR(kernel='sigmoid')
y_pred = cross_val_predict(SVR, X, y, cv=kf)
r2_score(y, y_pred)
171/20: plt.scatter(y, y_pred)
171/21:
from sklearn.svm import SVR
SVR = SVR(kernel='poly')
y_pred = cross_val_predict(RF, X, y, cv=kf)
r2_score(y, y_pred)
170/22: plt.scatter(y, y_pred)
170/23: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
170/24:
RF.fit(X_train, y_train)
r2 = RF.score(X_train,y_train)
r2
170/25:
y_pred = RF.predict(X_test)
y_pred.shape
170/26: r2_score(y_test, y_pred)
170/27:
from sklearn.inspection import permutation_importance
from sklearn.compose import ColumnTransformer

result = permutation_importance(RF, X_test, y_test, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()
fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=X.columns[sorted_idx])
ax.set_title("Permutation Importances (test set)")
fig.tight_layout()
plt.show()
171/22: plt.scatter(y, y_pred)
171/23: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
171/24:
RF.fit(X_train, y_train)
r2 = RF.score(X_train,y_train)
r2
171/25:
y_pred = RF.predict(X_test)
y_pred.shape
171/26: r2_score(y_test, y_pred)
171/27:
from sklearn.inspection import permutation_importance
from sklearn.compose import ColumnTransformer

result = permutation_importance(RF, X_test, y_test, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()
fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=X.columns[sorted_idx])
ax.set_title("Permutation Importances (test set)")
fig.tight_layout()
plt.show()
170/28:
X = df.drop(['LogKp'], axis=1)
X.shape
170/29:
from sklearn.inspection import permutation_importance
from sklearn.compose import ColumnTransformer

result = permutation_importance(RF, X_test, y_test, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()
fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=X.columns[sorted_idx])
ax.set_title("Permutation Importances (test set)")
fig.tight_layout()
plt.show()
171/28:
from sklearn.inspection import permutation_importance
from sklearn.compose import ColumnTransformer

result = permutation_importance(RF, X_test, y_test, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()
fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=X.columns[sorted_idx])
ax.set_title("Permutation Importances (test set)")
fig.tight_layout()
plt.show()
168/42: result.importances[sorted_idx]
168/43: result.importances[sorted_idx][:20]
168/44: result.importances[sorted_idx][:19]
168/45: result.importances[sorted_idx].[:19]
168/46: result.importances[sorted_idx]
170/30:
from sklearn.inspection import permutation_importance
from sklearn.compose import ColumnTransformer

result = permutation_importance(RF, X_test, y_test, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()
fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=df.columns[sorted_idx])
ax.set_title("Permutation Importances (test set)")
fig.tight_layout()
plt.show()
171/29:
from sklearn.inspection import permutation_importance
from sklearn.compose import ColumnTransformer

result = permutation_importance(RF, X_test, y_test, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()
fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=X.columns[sorted_idx])
ax.set_title("Permutation Importances (test set)")
fig.tight_layout()
plt.show()
166/52:
from sklearn.inspection import permutation_importance
from sklearn.compose import ColumnTransformer

result = permutation_importance(RF, X_test, y_test, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()

fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=df.columns[sorted_idx])
ax.set_title("Permutation Importances (test set)")
fig.tight_layout()
plt.show()
168/47: X.columns[sorted_idx]
168/48: List_importance = X.columns[sorted_idx]
168/49: List_importance[:20]
168/50:
List_importance = X.columns[sorted_idx]
List_importance[:20]
169/30:
List_importance = df.columns[sorted_idx]
List_importance[:20]
169/31:
List_importance = X.columns[sorted_idx]
List_importance[:20]
168/51: List_importance = X.columns[sorted_idx]
168/52: List_importance
168/53: result
168/54: result.shape
168/55: result.importances.shape
168/56: result.importances
168/57: pd.DataFrame(result.importances)
168/58: list(result.importances)
168/59: pd.DataFrame(result.importances, [:0])
168/60: result.importances.shape
168/61: result.importances.to_csv('test')
168/62: result.importances.reshape[:-1]
168/63: result.importances.reshape(:1)
168/64: result.importances.reshape[:1]
168/65: result.importances.reshape(0,1)
168/66: result.importances.reshape(1,0)
168/67: result.importances.reshape(0,-1)
168/68: result.importances
168/69: X.columns[sorted_idx]
168/70: result.importances[sorted_idx]
168/71: result.importances
168/72: X.columns
168/73: result.importances.shape
168/74: X.columns.shape
168/75: X.columns[sorted_idx].shape
168/76: result.importances.shape
168/77: result.importances[sorted_idx]
168/78: result.importances[sorted_idx].shape
168/79: result.importances[sorted_idx]
168/80: X.columns[sorted_idx][:19]
168/81: X.columns[sorted_idx][:20]
168/82:
from sklearn.inspection import permutation_importance
from sklearn.compose import ColumnTransformer

result = permutation_importance(RF, X_test, y_test, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()
fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx][:20],
           vert=False, labels=X.columns[sorted_idx][:20])
ax.set_title("Permutation Importances (test set)")
fig.tight_layout()
plt.show()
168/83:
from sklearn.inspection import permutation_importance
from sklearn.compose import ColumnTransformer

result = permutation_importance(RF, X_test, y_test, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()
fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T[:20],
           vert=False, labels=X.columns[sorted_idx][:20])
ax.set_title("Permutation Importances (test set)")
fig.tight_layout()
plt.show()
168/84:
from sklearn.inspection import permutation_importance
from sklearn.compose import ColumnTransformer

result = permutation_importance(RF, X_test, y_test, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()
fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=X.columns[sorted_idx])
ax.set_title("Permutation Importances (test set)")
fig.tight_layout()
plt.show()
168/85:
for i in r.importances_mean.argsort()[::-1]:
...     if r.importances_mean[i] - 2 * r.importances_std[i] > 0:
...         print(f"{diabetes.feature_names[i]:<8}"
...               f"{r.importances_mean[i]:.3f}"
...               f" +/- {r.importances_std[i]:.3f}")
168/86:
for i in r.importances_mean.argsort()[::-1]:
    if r.importances_mean[i] - 2 * r.importances_std[i] > 0:
        print(f"{diabetes.feature_names[i]:<8}"
               f"{r.importances_mean[i]:.3f}"
               f" +/- {r.importances_std[i]:.3f}")
168/87:
for i in result .importances_mean.argsort()[::-1]:
    if result .importances_mean[i] - 2 * r.importances_std[i] > 0:
        print(f"{diabetes.feature_names[i]:<8}"
               f"{r.importances_mean[i]:.3f}"
               f" +/- {r.importances_std[i]:.3f}")
168/88:
for i in result .importances_mean.argsort()[::-1]:
    if result .importances_mean[i] - 2 * result .importances_std[i] > 0:
        print(f"{diabetes.feature_names[i]:<8}"
               f"{r.importances_mean[i]:.3f}"
               f" +/- {r.importances_std[i]:.3f}")
168/89:
for i in result .importances_mean.argsort()[::-1]:
    if result .importances_mean[i] - 2 * result .importances_std[i] > 0:
        print(f"{X.feature_names[i]:<8}"
               f"{r.importances_mean[i]:.3f}"
               f" +/- {r.importances_std[i]:.3f}")
168/90:
for i in result .importances_mean.argsort()[::-1]:
    if result .importances_mean[i] - 2 * result .importances_std[i] > 0:
        print(f"{X.columns[i]:<8}"
               f"{r.importances_mean[i]:.3f}"
               f" +/- {r.importances_std[i]:.3f}")
168/91:
for i in result .importances_mean.argsort()[::-1]:
    if result .importances_mean[i] - 2 * result .importances_std[i] > 0:
        print(f"{X.columns[i]:<8}"
               f"{result.importances_mean[i]:.3f}"
               f" +/- {result.importances_std[i]:.3f}")
168/92:
for i in result .importances_mean.argsort()[::-1]:
    if result .importances_mean[i] - 2 * result .importances_std[i] > 0:
        print(f"{X.columns[i]:<20}"
               f"{result.importances_mean[i]:.3f}"
               f" +/- {result.importances_std[i]:.3f}")
168/93:
for i in result .importances_mean.argsort()[::-1]:
    if result .importances_mean[i] - 2 * result .importances_std[i] > 0:
        print(f"{X.columns[i]:<8}"
               f"{result.importances_mean[i]:.3f}"
               f" +/- {result.importances_std[i]:.3f}")
168/94:
for i in result .importances_mean.argsort()[::-1]:
    if result .importances_mean[i] - 2 * result .importances_std[i] > 0.05:
        print(f"{X.columns[i]:<8}"
               f"{result.importances_mean[i]:.3f}"
               f" +/- {result.importances_std[i]:.3f}")
168/95:
for i in result .importances_mean.argsort()[::-1]:
    if result .importances_mean[i] - 2 * result .importances_std[i] > 0:
        print(f"{X.columns[i]:<8}"
               f"{result.importances_mean[i]:.3f}"
               f" +/- {result.importances_std[i]:.3f}")
169/32:
fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=df.columns[sorted_idx])
ax.set_title("Permutation Importances (test set)")
ax.set_ylim(:20)
fig.tight_layout()
plt.show()
169/33:
fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=df.columns[sorted_idx])
ax.set_title("Permutation Importances (test set)")
ax.set_ylim([:20])
fig.tight_layout()
plt.show()
169/34:
fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=df.columns[sorted_idx])
ax.set_title("Permutation Importances (test set)")
ax.set_ylim(0,20)
fig.tight_layout()
plt.show()
169/35:
fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=df.columns[sorted_idx])
ax.set_title("Permutation Importances (test set)")
fig.tight_layout()
plt.show()
169/36:
fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=df.columns[sorted_idx])
ax.set_title("Permutation Importances (test set)")
ax.set_ylim(1,20)
fig.tight_layout()
plt.show()
169/37:
fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=df.columns[sorted_idx])
ax.set_title("Permutation Importances (test set)")
ax.set_ylim(0,20)
fig.tight_layout()
plt.show()
169/38:
fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=df.columns[sorted_idx])
ax.set_title("Permutation Importances (test set)")
fig.tight_layout()
plt.show()
171/30:
from sklearn.inspection import permutation_importance
from sklearn.compose import ColumnTransformer

result = permutation_importance(RF, X_test, y_test, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()
fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=df.columns[sorted_idx])
ax.set_title("Permutation Importances (test set)")
fig.tight_layout()
plt.show()
172/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
172/2:
path = r'Fingerprint/'

Fp1  = pd.read_csv(path + 'Fingerprinter.csv'               , header = 0)
Fp2  = pd.read_csv(path + 'ExtendedFingerprinter.csv'       , header = 0)
Fp3  = pd.read_csv(path + 'EStateFingerprinter.csv'         , header = 0)
Fp4  = pd.read_csv(path + 'GraphOnlyFingerprinter.csv'      , header = 0)
Fp5  = pd.read_csv(path + 'MACCSFingerprinter.csv'          , header = 0)
Fp6  = pd.read_csv(path + 'PubchemFingerprinter.csv'        , header = 0)
Fp7  = pd.read_csv(path + 'SubstructureFingerprinter.csv'   , header = 0)
Fp8  = pd.read_csv(path + 'SubstructureFingerprintCount.csv', header = 0)
Fp9  = pd.read_csv(path + 'KlekotaRothFingerprinter.csv'    , header = 0)
Fp10 = pd.read_csv(path + 'KlekotaRothFingerprintCount.csv' , header = 0)
Fp11 = pd.read_csv(path + 'AtomPairs2DFingerprinter.csv'    , header = 0)
Fp12 = pd.read_csv(path + 'AtomPairs2DFingerprintCount.csv' , header = 0)
172/3: Fp1.shape
172/4:
df = pd.read_csv('df6_lipinski.csv')
df
172/5:
df = pd.read_csv('df6_lipinski.csv')
df
172/6: name = df['compound']
172/7: name.shape
172/8:
df2 = df.rename({'compound': 'Name'}, axis=1)
df2 = df2.drop('Unnamed: 0', axis=1)
df3 = df2.set_index('Name')
df3
174/1:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
174/2: pwd
174/3: df = pd.read_csv('rawdataqsar_sm.csv')
174/4: df
175/1:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
175/2: pwd
175/3: df = pd.read_csv('rawdataqsar_sm.csv')
175/4: df
175/5: df2 =df[df.kp.notna()]
175/6: df2
175/7: df3 = df2.drop(['logkp', 'kpu'], axis=1)
175/8: df3 = df2.drop(['logkp', 'kp'], axis=1)
175/9: df3
175/10:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
175/11: pwd
175/12: df = pd.read_csv('rawdataqsar_sm.csv')
175/13: df
175/14: df2 =df[df.kp.notna()]
175/15: df2
175/16: df3 = df2.drop(['logkp', 'kp'], axis=1)
175/17: df3
175/18:
def LogKp(input):
    LogKp = []

    for i in input['kp']:
        LogKp.append(np.log10(i))

    input['LogKp'] = LogKp
    x = input.drop('kp', 1)   
    return x
175/19: df4 = LogKp(df3)
175/20: df4
175/21: df3
175/22:
def LogKp(input):
    LogKp = []

    for i in input['kp']:
        LogKp.append(np.log10(i))

    input['LogKp'] = LogKp
    x = input.drop('kp', 1)   
    return x
175/23: df4 = LogKp(df3)
175/24:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
175/25: pwd
175/26: df = pd.read_csv('rawdataqsar_sm.csv')
175/27: df
175/28: df2 =df[df.kp.notna()]
175/29: df2
175/30: df3 = df2.drop(['logkp', 'kp'], axis=1)
175/31: df3
175/32:
def LogKp(input):
    LogKp = []

    for i in input['kp']:
        LogKp.append(np.log10(i))

    input['LogKp'] = LogKp
    x = input.drop('kp', 1)   
    return x
175/33: df4 = LogKp(df3)
175/34:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
175/35: pwd
175/36: df = pd.read_csv('rawdataqsar_sm.csv')
175/37: df
175/38: df2 =df[df.kp.notna()]
175/39: df2
175/40: df3 = df2.drop(['logkp', 'kp'], axis=1)
175/41: df3
175/42:
def LogKpu(input):
    LogKpu = []

    for i in input['kpu']:
        LogKpu.append(np.log10(i))

    input['LogKpu'] = LogKp
    x = input.drop('kpu', 1)   
    return x
175/43: df4 = LogKpu(df3)
175/44: df4
175/45:
df5 = df4.drop_duplicates(subset=['smiles'])
df5
175/46:
df6 = df5.reset_index(drop=True)
df6
175/47:
# Calculate Molecular Descriptors
from rdkit import Chem
from rdkit.Chem import Descriptors, Lipinski

def lipinski(smiles, verbose=False):

    moldata= []
    for elem in smiles:
        mol=Chem.MolFromSmiles(elem) 
        moldata.append(mol)
       
    baseData= np.arange(1,1)
    i=0  
    for mol in moldata:        
       
        desc_MolWt = Descriptors.MolWt(mol)
        desc_TPSA = Descriptors.TPSA(mol)
        desc_MolLogP = Descriptors.MolLogP(mol)
        desc_NumHDonors = Lipinski.NumHDonors(mol)
        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)
        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)
        desc_FpDensityMorgan1 = Descriptors.FpDensityMorgan1(mol)
        desc_FpDensityMorgan2 = Descriptors.FpDensityMorgan2(mol)
        desc_FpDensityMorgan3 = Descriptors.FpDensityMorgan3(mol)
        desc_NumRadicalElectrons = Descriptors.NumRadicalElectrons(mol)
        desc_NumValenceElectrons = Descriptors.NumValenceElectrons(mol)
           
        row = np.array([desc_MolWt,
                        desc_TPSA,
                        desc_MolLogP,
                        desc_NumHDonors,
                        desc_NumHAcceptors,
                        desc_NumRotatableBonds,
                        desc_FpDensityMorgan1,
                        desc_FpDensityMorgan2,
                        desc_FpDensityMorgan3,
                        desc_NumRadicalElectrons,
                        desc_NumValenceElectrons])   
    
        if(i==0):
            baseData=row
        else:
            baseData=np.vstack([baseData, row])
        i=i+1      
    
    columnNames=["MW","TPSA", "LogP","NumHDonors","NumHAcceptors", "NumRotatableBonds", 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'NumRadicalElectrons', 'NumValenceElectrons']   
    descriptors = pd.DataFrame(data=baseData,columns=columnNames)
    
    return descriptors
175/48:
df6_lipinski = lipinski(df6.smiles)
df6_lipinski
175/49:
data = pd.concat([df6, df6_lipinski], axis=1)
data.to_csv('df6_lipinski_Kpu.csv')
data
175/50:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)
plt2.set_xlabel("LogP", fontsize=16, fontweight='bold')
plt2.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)
plt2.set_ylim(0, 50)
plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)


#Histogram for NumHDonors
plt3.hist(data['NumHDonors'], density=False, bins= 30, color='#FF9A00', edgecolor='black', linewidth=0.5)
plt3.set_xlabel("Number of Hydrogen Bond Donors", fontsize=16, fontweight='bold')
plt3.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)
plt3.set_ylim(0, 50)
plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for NumHAcceptors
plt4.hist(data['NumHAcceptors'], density=False, bins= 30, color='#FF165D', edgecolor='black', linewidth=0.5)
plt4.set_xlabel("Number of Hydrogen Bond Acceptors", fontsize=16, fontweight='bold')
plt4.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)
plt4.set_ylim(0, 50)
plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for NumRotatableBonds
plt5.hist(data['NumRotatableBonds'], density=False, bins= 30, color='#52006A', edgecolor='black', linewidth=0.5)
plt5.set_xlabel("Number of RotatableBonds", fontsize=16, fontweight='bold')
plt5.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)
plt5.set_ylim(0, 50)
plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for TPSA
plt6.hist(data['TPSA'], density=False, bins= 30, color='#01937C', edgecolor='black', linewidth=0.5)
plt6.set_xlabel("Topolocial Plar Surface Area", fontsize=16, fontweight='bold')
plt6.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt6.tick_params(axis='both', which='major', labelsize=14)
plt6.set_ylim(0, 50)
plt6.axvline(90, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#save file
plt.tight_layout()
plt.savefig('Lipinski_basic descriptors.pdf', dpi=300)
175/51:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt



figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

x1 = data['MW']
x2 = data['LogP']
x3 = data['NumHDonors']
x4 = data['NumHAcceptors']
x5 = data['NumRotatableBonds']
x6 = data['TPSA']
y = data['LogKpu']
colors = {'HEK293':'red', 'SK-MEL-2':'blue', 'HL60':'green', 'SH-5Y':'lightblue', 'PBMC':'yellow'}

#MW
plt1.scatter(x1, y, alpha=0.5, c=data['Cells'].map(colors))
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("Kpu", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)

#LogP
plt2.scatter(x2, y, alpha=0.5, c=data['Cells'].map(colors))
plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')
plt2.set_ylabel("Kpu", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)

#NumHbondDonors
plt3.scatter(x3, y, alpha=0.5, c=data['Cells'].map(colors))
plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')
plt3.set_ylabel("Kpu", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt4.scatter(x4, y,  alpha=0.5, c=data['Cells'].map(colors))
plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')
plt4.set_ylabel("log Kp", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt5.scatter(x5, y, alpha=0.5, c=data['Cells'].map(colors))
plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')
plt5.set_ylabel("Kpu", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)

#TPSA
plt6.scatter(x6, y, alpha=0.5, c=data['Cells'].map(colors))
plt6.set_xlabel('Topological Polar Surface Area', fontsize=16, fontweight='bold')
plt6.set_ylabel("Kpu", fontsize=16, fontweight='bold')
plt6.tick_params(axis='both', which='major', labelsize=14)

#save file
plt.tight_layout()
plt.savefig('RO5_and_Kpu.pdf', dpi=300)
175/52:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
175/53: pwd
175/54: df = pd.read_csv('rawdataqsar_sm.csv')
175/55: df
175/56: df2 =df[df.kp.notna()]
175/57: df2
175/58: df3 = df2.drop(['logkp', 'kp'], axis=1)
175/59: df3
175/60:
def LogKpu(input):
    LogKpu = []

    for i in input['kpu']:
        LogKpu.append(np.log10(i))

    input['LogKpu'] = LogKpu
    x = input.drop('kpu', 1)   
    return x
175/61: df4 = LogKpu(df3)
175/62: df4
175/63:
df5 = df4.drop_duplicates(subset=['smiles'])
df5
175/64:
df6 = df5.reset_index(drop=True)
df6
175/65:
# Calculate Molecular Descriptors
from rdkit import Chem
from rdkit.Chem import Descriptors, Lipinski

def lipinski(smiles, verbose=False):

    moldata= []
    for elem in smiles:
        mol=Chem.MolFromSmiles(elem) 
        moldata.append(mol)
       
    baseData= np.arange(1,1)
    i=0  
    for mol in moldata:        
       
        desc_MolWt = Descriptors.MolWt(mol)
        desc_TPSA = Descriptors.TPSA(mol)
        desc_MolLogP = Descriptors.MolLogP(mol)
        desc_NumHDonors = Lipinski.NumHDonors(mol)
        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)
        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)
        desc_FpDensityMorgan1 = Descriptors.FpDensityMorgan1(mol)
        desc_FpDensityMorgan2 = Descriptors.FpDensityMorgan2(mol)
        desc_FpDensityMorgan3 = Descriptors.FpDensityMorgan3(mol)
        desc_NumRadicalElectrons = Descriptors.NumRadicalElectrons(mol)
        desc_NumValenceElectrons = Descriptors.NumValenceElectrons(mol)
           
        row = np.array([desc_MolWt,
                        desc_TPSA,
                        desc_MolLogP,
                        desc_NumHDonors,
                        desc_NumHAcceptors,
                        desc_NumRotatableBonds,
                        desc_FpDensityMorgan1,
                        desc_FpDensityMorgan2,
                        desc_FpDensityMorgan3,
                        desc_NumRadicalElectrons,
                        desc_NumValenceElectrons])   
    
        if(i==0):
            baseData=row
        else:
            baseData=np.vstack([baseData, row])
        i=i+1      
    
    columnNames=["MW","TPSA", "LogP","NumHDonors","NumHAcceptors", "NumRotatableBonds", 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'NumRadicalElectrons', 'NumValenceElectrons']   
    descriptors = pd.DataFrame(data=baseData,columns=columnNames)
    
    return descriptors
175/66:
df6_lipinski = lipinski(df6.smiles)
df6_lipinski
175/67:
data = pd.concat([df6, df6_lipinski], axis=1)
data.to_csv('df6_lipinski_Kpu.csv')
data
175/68:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

# Histogram for MW
plt1.hist(data['MW'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Log P
plt2.hist(data['LogP'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)
plt2.set_xlabel("LogP", fontsize=16, fontweight='bold')
plt2.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)
plt2.set_ylim(0, 50)
plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)


#Histogram for NumHDonors
plt3.hist(data['NumHDonors'], density=False, bins= 30, color='#FF9A00', edgecolor='black', linewidth=0.5)
plt3.set_xlabel("Number of Hydrogen Bond Donors", fontsize=16, fontweight='bold')
plt3.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)
plt3.set_ylim(0, 50)
plt3.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for NumHAcceptors
plt4.hist(data['NumHAcceptors'], density=False, bins= 30, color='#FF165D', edgecolor='black', linewidth=0.5)
plt4.set_xlabel("Number of Hydrogen Bond Acceptors", fontsize=16, fontweight='bold')
plt4.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)
plt4.set_ylim(0, 50)
plt4.axvline(10, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for NumRotatableBonds
plt5.hist(data['NumRotatableBonds'], density=False, bins= 30, color='#52006A', edgecolor='black', linewidth=0.5)
plt5.set_xlabel("Number of RotatableBonds", fontsize=16, fontweight='bold')
plt5.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)
plt5.set_ylim(0, 50)
plt5.axvline(15, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for TPSA
plt6.hist(data['TPSA'], density=False, bins= 30, color='#01937C', edgecolor='black', linewidth=0.5)
plt6.set_xlabel("Topolocial Plar Surface Area", fontsize=16, fontweight='bold')
plt6.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt6.tick_params(axis='both', which='major', labelsize=14)
plt6.set_ylim(0, 50)
plt6.axvline(90, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#save file
plt.tight_layout()
plt.savefig('Lipinski_basic descriptors.pdf', dpi=300)
175/69:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt



figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

x1 = data['MW']
x2 = data['LogP']
x3 = data['NumHDonors']
x4 = data['NumHAcceptors']
x5 = data['NumRotatableBonds']
x6 = data['TPSA']
y = data['LogKpu']
colors = {'HEK293':'red', 'SK-MEL-2':'blue', 'HL60':'green', 'SH-5Y':'lightblue', 'PBMC':'yellow'}

#MW
plt1.scatter(x1, y, alpha=0.5, c=data['Cells'].map(colors))
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("Kpu", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)

#LogP
plt2.scatter(x2, y, alpha=0.5, c=data['Cells'].map(colors))
plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')
plt2.set_ylabel("Kpu", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)

#NumHbondDonors
plt3.scatter(x3, y, alpha=0.5, c=data['Cells'].map(colors))
plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')
plt3.set_ylabel("Kpu", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt4.scatter(x4, y,  alpha=0.5, c=data['Cells'].map(colors))
plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')
plt4.set_ylabel("log Kp", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt5.scatter(x5, y, alpha=0.5, c=data['Cells'].map(colors))
plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')
plt5.set_ylabel("Kpu", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)

#TPSA
plt6.scatter(x6, y, alpha=0.5, c=data['Cells'].map(colors))
plt6.set_xlabel('Topological Polar Surface Area', fontsize=16, fontweight='bold')
plt6.set_ylabel("Kpu", fontsize=16, fontweight='bold')
plt6.tick_params(axis='both', which='major', labelsize=14)

#save file
plt.tight_layout()
plt.savefig('RO5_and_Kpu.pdf', dpi=300)
175/70:
selection = ['smiles']
smiles = data[selection]
smiles.to_csv(r'smiles/new_smiles.smi', sep='\t', index=False, header=False)
175/71:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt



figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

x1 = data['MW']
x2 = data['LogP']
x3 = data['NumHDonors']
x4 = data['NumHAcceptors']
x5 = data['NumRotatableBonds']
x6 = data['TPSA']
y = data['LogKpu']
colors = {'HEK293':'red', 'SK-MEL-2':'blue', 'HL60':'green', 'SH-5Y':'lightblue', 'PBMC':'yellow'}

#MW
plt1.scatter(x1, y, alpha=0.5, c=data['Cells'].map(colors))
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("Log Kpu", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)

#LogP
plt2.scatter(x2, y, alpha=0.5, c=data['Cells'].map(colors))
plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')
plt2.set_ylabel("Log Kpu", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)

#NumHbondDonors
plt3.scatter(x3, y, alpha=0.5, c=data['Cells'].map(colors))
plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')
plt3.set_ylabel("Kpu", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt4.scatter(x4, y,  alpha=0.5, c=data['Cells'].map(colors))
plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')
plt4.set_ylabel("Log Kpu", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt5.scatter(x5, y, alpha=0.5, c=data['Cells'].map(colors))
plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')
plt5.set_ylabel("Log Kpu", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)

#TPSA
plt6.scatter(x6, y, alpha=0.5, c=data['Cells'].map(colors))
plt6.set_xlabel('Topological Polar Surface Area', fontsize=16, fontweight='bold')
plt6.set_ylabel("Log Kpu", fontsize=16, fontweight='bold')
plt6.tick_params(axis='both', which='major', labelsize=14)

#save file
plt.tight_layout()
plt.savefig('RO5_and_Kpu.pdf', dpi=300)
175/72:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt



figure, ((plt1,plt2), (plt3,plt4), (plt5,plt6)) = plt.subplots(3, 2)
figure.set_size_inches(15,15)

x1 = data['MW']
x2 = data['LogP']
x3 = data['NumHDonors']
x4 = data['NumHAcceptors']
x5 = data['NumRotatableBonds']
x6 = data['TPSA']
y = data['LogKpu']
colors = {'HEK293':'red', 'SK-MEL-2':'blue', 'HL60':'green', 'SH-5Y':'lightblue', 'PBMC':'yellow'}

#MW
plt1.scatter(x1, y, alpha=0.5, c=data['Cells'].map(colors))
plt1.set_xlabel('Molecular weight (Da)', fontsize=16, fontweight='bold')
plt1.set_ylabel("Log Kpu", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)

#LogP
plt2.scatter(x2, y, alpha=0.5, c=data['Cells'].map(colors))
plt2.set_xlabel('LogP', fontsize=16, fontweight='bold')
plt2.set_ylabel("Log Kpu", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)

#NumHbondDonors
plt3.scatter(x3, y, alpha=0.5, c=data['Cells'].map(colors))
plt3.set_xlabel('Number of Hydrogen Bond Donors', fontsize=16, fontweight='bold')
plt3.set_ylabel("Log Kpu", fontsize=16, fontweight='bold')
plt3.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt4.scatter(x4, y,  alpha=0.5, c=data['Cells'].map(colors))
plt4.set_xlabel('Number of Hydrogen Bond Acceptors', fontsize=16, fontweight='bold')
plt4.set_ylabel("Log Kpu", fontsize=16, fontweight='bold')
plt4.tick_params(axis='both', which='major', labelsize=14)

#NumHbondAcceptors
plt5.scatter(x5, y, alpha=0.5, c=data['Cells'].map(colors))
plt5.set_xlabel('Number of RotatableBonds', fontsize=16, fontweight='bold')
plt5.set_ylabel("Log Kpu", fontsize=16, fontweight='bold')
plt5.tick_params(axis='both', which='major', labelsize=14)

#TPSA
plt6.scatter(x6, y, alpha=0.5, c=data['Cells'].map(colors))
plt6.set_xlabel('Topological Polar Surface Area', fontsize=16, fontweight='bold')
plt6.set_ylabel("Log Kpu", fontsize=16, fontweight='bold')
plt6.tick_params(axis='both', which='major', labelsize=14)

#save file
plt.tight_layout()
plt.savefig('RO5_and_Kpu.pdf', dpi=300)
172/9:
df = pd.read_csv('df6_lipinski_kpu.csv')
df
172/10: name = df['compound']
172/11: name.shape
172/12:
df2 = df.rename({'compound': 'Name'}, axis=1)
df2 = df2.drop('Unnamed: 0', axis=1)
df3 = df2.set_index('Name')
df3
172/13:
LogKpu = df3['LogKpu']
LogKpu = LogKpu.to_frame()
LogKpu
172/14:
Fp1.Name = name
Fp2.Name = name
Fp3.Name = name
Fp4.Name = name
Fp5.Name = name
Fp6.Name = name
Fp7.Name = name
Fp8.Name = name
Fp9.Name = name
Fp10.Name = name
Fp11.Name = name
Fp12.Name = name
172/15:
Fpp1 = Fp1.drop('Name', axis=1)
Fpp2 = Fp2.drop('Name', axis=1)
Fpp3 = Fp3.drop('Name', axis=1)
Fpp4 = Fp4.drop('Name', axis=1)
Fpp5 = Fp5.drop('Name', axis=1)
Fpp6 = Fp6.drop('Name', axis=1)
Fpp7 = Fp7.drop('Name', axis=1)
Fpp8 = Fp8.drop('Name', axis=1)
Fpp9 = Fp9.drop('Name', axis=1)
Fpp10 = Fp10.drop('Name', axis=1)
Fpp11 = Fp11.drop('Name', axis=1)
Fpp12 = Fp12.drop('Name', axis=1)
172/16:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
172/17:
Fp1_n  = normalized (Fpp1 )
Fp2_n  = normalized (Fpp2 )
Fp3_n  = normalized (Fpp3 )
Fp4_n  = normalized (Fpp4 )
Fp5_n  = normalized (Fpp5 )
Fp6_n  = normalized (Fpp6 )
Fp7_n  = normalized (Fpp7 )
Fp8_n  = normalized (Fpp8 )
Fp9_n  = normalized (Fpp9 )
Fp10_n = normalized (Fpp10)
Fp11_n = normalized (Fpp11)
Fp12_n = normalized (Fpp12)
172/18: Fp1_n
172/19:
Fp1_n["Name"] = Fp1.Name
Fp2_n["Name"] = Fp2.Name
Fp3_n["Name"] = Fp3.Name
Fp4_n["Name"] = Fp4.Name
Fp5_n["Name"] = Fp5.Name
Fp6_n["Name"] = Fp6.Name
Fp7_n["Name"] = Fp7.Name
Fp8_n["Name"] = Fp8.Name
Fp9_n["Name"] = Fp9.Name
Fp10_n["Name"] = Fp10.Name
Fp11_n["Name"] = Fp11.Name
Fp12_n["Name"] = Fp12.Name
172/20: Fp1_n
172/21:
Fp1_n = Fp1_n.set_index('Name')
Fp2_n = Fp2_n.set_index('Name')
Fp3_n = Fp3_n.set_index('Name')
Fp4_n = Fp4_n.set_index('Name')
Fp5_n = Fp5_n.set_index('Name')
Fp6_n = Fp6_n.set_index('Name')
Fp7_n = Fp7_n.set_index('Name')
Fp8_n = Fp8_n.set_index('Name')
Fp9_n = Fp9_n.set_index('Name')
Fp10_n = Fp10_n.set_index('Name')
Fp11_n = Fp11_n.set_index('Name')
Fp12_n = Fp12_n.set_index('Name')
172/22: Fp1_n
172/23:
raw1  = LogKpu.merge(Fp1_n, on='Name', how='outer')
raw2  = LogKpu.merge(Fp2_n, on='Name', how='outer')
raw3  = LogKpu.merge(Fp3_n, on='Name', how='outer')
raw4  = LogKpu.merge(Fp4_n, on='Name', how='outer')
raw5  = LogKpu.merge(Fp5_n, on='Name', how='outer')
raw6  = LogKpu.merge(Fp6_n, on='Name', how='outer')
raw7  = LogKpu.merge(Fp7_n, on='Name', how='outer')
raw8  = LogKpu.merge(Fp8_n, on='Name', how='outer')
raw9  = LogKpu.merge(Fp9_n, on='Name', how='outer')
raw10  = LogKpu.merge(Fp10_n, on='Name', how='outer')
raw11  = LogKpu.merge(Fp11_n, on='Name', how='outer')
raw12  = LogKpu.merge(Fp12_n, on='Name', how='outer')
172/24: raw1
172/25:
raw1 .to_csv('QSAR_Kpu/Kp_Fingerprinter.csv'               , sep=',' ,index=False)
raw2 .to_csv('QSAR_Kpu/Kp_ExtendedFingerprinter.csv'       , sep=',' ,index=False)
raw3 .to_csv('QSAR_Kpu/Kp_EStateFingerprinter.csv'         , sep=',' ,index=False)
raw4 .to_csv('QSAR_Kpu/Kp_GraphOnlyFingerprinter.csv'      , sep=',' ,index=False)
raw5 .to_csv('QSAR_Kpu/Kp_MACCSFingerprinter.csv'          , sep=',' ,index=False)
raw6 .to_csv('QSAR_Kpu/Kp_PubchemFingerprinter.csv'        , sep=',' ,index=False)
raw7 .to_csv('QSAR_Kpu/Kp_SubstructureFingerprinter.csv'   , sep=',' ,index=False)
raw8 .to_csv('QSAR_Kpu/Kp_SubstructureFingerprintCount.csv', sep=',' ,index=False)
raw9 .to_csv('QSAR_Kpu/Kp_KlekotaRothFingerprinter.csv'    , sep=',' ,index=False)
raw10.to_csv('QSAR_Kpu/Kp_KlekotaRothFingerprintCount.csv' , sep=',' ,index=False)
raw11.to_csv('QSAR_Kpu/Kp_AtomPairs2DFingerprinter.csv'    , sep=',' ,index=False)
raw12.to_csv('QSAR_Kpu/Kp_AtomPairs2DFingerprintCount.csv' , sep=',' ,index=False)
172/26:
print (len(raw1 ),len(raw2 ),len(raw3 ),len(raw4 ),len(raw5 )
      ,len(raw6 ),len(raw7 ),len(raw8 ),len(raw9 ),len(raw10)
      ,len(raw11),len(raw12))
172/27:
print (len(raw1 .columns),len(raw2 .columns),len(raw3 .columns),len(raw4 .columns),len(raw5 )
      ,len(raw6 .columns),len(raw7 .columns),len(raw8 .columns),len(raw9 .columns),len(raw10)
      ,len(raw11.columns),len(raw12.columns))
176/1: pwd
176/2:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
176/3:
df = pd.read_csv("df6_lipinski_kpu.csv")
df
176/4: df1 = df.drop(['Unnamed: 0', 'smiles', 'Cells', 'compound'], axis=True)
176/5:
df = pd.read_csv("/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/df6_lipinski_kpu.csv")
df
176/6: df1 = df.drop(['Unnamed: 0', 'smiles', 'Cells', 'compound'], axis=True)
178/1: !python -m mordred --help
178/2:
!python -m mordred -t smiles/new_smiles.smi -o smiles/mordard.csv -d ABCIndex AcidBase AdjacencyMatrix Aromatic AtomCount
Autocorrelation BCUT BalabanJ BaryszMatrix BertzCT BondCount CarbonTypes
Chi Constitutional DetourMatrix DistanceMatrix EState
EccentricConnectivityIndex ExtendedTopochemicalAtom FragmentComplexity
Framework GeometricalIndex GravitationalIndex HydrogenBond InformationContent
KappaShapeIndex Lipinski LogS McGowanVolume MoeType
MolecularDistanceEdge MolecularId PathCount Polarizability
RingCount RotatableBond SLogP TopoPSA TopologicalCharge TopologicalIndex
VdwVolumeABC VertexAdjacencyInformation WalkCount Weight WienerIndex
ZagrebIndex
178/3:
!python -m mordred -t smiles/new_smiles.smi -o smiles/mordard.csv -d ABCIndex AcidBase AdjacencyMatrix Aromatic AtomCount
Autocorrelation BalabanJ BaryszMatrix BertzCT BondCount CarbonTypes
Chi Constitutional DetourMatrix DistanceMatrix EState
EccentricConnectivityIndex ExtendedTopochemicalAtom FragmentComplexity
Framework GeometricalIndex GravitationalIndex HydrogenBond InformationContent
KappaShapeIndex Lipinski LogS McGowanVolume MoeType
MolecularDistanceEdge MolecularId PathCount Polarizability
RingCount RotatableBond SLogP TopoPSA TopologicalCharge TopologicalIndex
VdwVolumeABC VertexAdjacencyInformation WalkCount Weight WienerIndex
ZagrebIndex
178/4: !python -m mordred -t smiles/new_smiles.smi -o smiles/mordard.csv -d ABCIndex
178/5: !python -m mordred -t smi smiles/new_smiles.smi -o smiles/mordard.csv -d ABCIndex
178/6:
!python -m mordred -t smi smiles/new_smiles.smi -o smiles/mordard.csv -d ABCIndex AcidBase AdjacencyMatrix Aromatic AtomCount
Autocorrelation BCUT BalabanJ BaryszMatrix BertzCT BondCount CarbonTypes
Chi Constitutional DetourMatrix DistanceMatrix EState
EccentricConnectivityIndex ExtendedTopochemicalAtom FragmentComplexity
Framework GeometricalIndex GravitationalIndex HydrogenBond InformationContent
KappaShapeIndex Lipinski LogS McGowanVolume MoeType
MolecularDistanceEdge MolecularId PathCount Polarizability
RingCount RotatableBond SLogP TopoPSA TopologicalCharge TopologicalIndex
VdwVolumeABC VertexAdjacencyInformation WalkCount Weight WienerIndex
ZagrebIndex
178/7: !python -m mordred -t smi smiles/new_smiles.smi -o smiles/mordard.csv -d ABCIndex, AcidBase
178/8:
!python -m mordred -t smi smiles/new_smiles.smi -o smiles/mordard.csv -d 
'ABCIndex,' (choose from 'ABCIndex', 'AcidBase', 'AdjacencyMatrix', 'Aromatic', 'AtomCount', 'Autocorrelation', 'BCUT', 'BalabanJ', 'BaryszMatrix', 'BertzCT', 'BondCount', 'CPSA', 'CarbonTypes', 'Chi', 'Constitutional', 'DetourMatrix', 'DistanceMatrix', 'EState', 'EccentricConnectivityIndex', 'ExtendedTopochemicalAtom', 'FragmentComplexity', 'Framework', 'GeometricalIndex', 'GravitationalIndex', 'HydrogenBond', 'InformationContent', 'KappaShapeIndex', 'Lipinski', 'LogS', 'McGowanVolume', 'MoRSE', 'MoeType', 'MolecularDistanceEdge', 'MolecularId', 'MomentOfInertia', 'PBF', 'PathCount', 'Polarizability', 'RingCount', 'RotatableBond', 'SLogP', 'TopoPSA', 'TopologicalCharge', 'TopologicalIndex', 'VdwVolumeABC', 'VertexAdjacencyInformation', 'WalkCount', 'Weight', 'WienerIndex', 'ZagrebIndex'
178/9: !python -m mordred -t smi smiles/new_smiles.smi -o smiles/mordard.csv -d 'ABCIndex', 'AcidBase'
178/10: !python -m mordred -t smi smiles/new_smiles.smi -o smiles/mordard.csv -d ('ABCIndex', 'AcidBase')
178/11: !python -m mordred -t smi smiles/new_smiles.smi -o smiles/mordard.csv -d ABCIndex, AcidBase
178/12: !python -m mordred -t smi smiles/new_smiles.smi -o smiles/mordard.csv -d ABCIndex AcidBase
178/13: !python -m mordred -t smi smiles/new_smiles.smi -o smiles/mordard.csv -d ABCIndex
178/14: !python -m mordred -t smi smiles/new_smiles.smi -o smiles/mordard.csv -d AcidBase
178/15:
!python -m mordred -t smi smiles/new_smiles.smi -o smiles/mordard.csv -d ABCIndex, -d AcidBase -d AdjacencyMatrix -d Aromatic -d AtomCount
-d Autocorrelation -d BCUT -d BalabanJ -d BaryszMatrix -d BertzCT -d BondCount -d CarbonTypes
-d Chi -d Constitutional -d DetourMatrix -d DistanceMatrix -d EState
-d EccentricConnectivityIndex -d ExtendedTopochemicalAtom -d FragmentComplexity
-d Framework -d GeometricalIndex -d GravitationalIndex -d HydrogenBond -d InformationContent
-d KappaShapeIndex -d Lipinski -d LogS -d McGowanVolume -d MoeType
-d MolecularDistanceEdge -d MolecularId -d PathCount -d Polarizability
-d RingCount -d RotatableBond -d SLogP -d TopoPSA -d TopologicalCharge -d TopologicalIndex
-d VdwVolumeABC -d VertexAdjacencyInformation -d WalkCount -d Weight -d WienerIndex
-d ZagrebIndex
178/16:
!python -m mordred -t smi smiles/new_smiles.smi -o smiles/mordard.csv -d ABCIndex, -d AcidBase -d AdjacencyMatrix -d Aromatic -d AtomCount -d Autocorrelation -d BCUT -d BalabanJ -d BaryszMatrix -d BertzCT -d BondCount -d CarbonTypes
-d Chi -d Constitutional -d DetourMatrix -d DistanceMatrix -d EState
-d EccentricConnectivityIndex -d ExtendedTopochemicalAtom -d FragmentComplexity
-d Framework -d GeometricalIndex -d GravitationalIndex -d HydrogenBond -d InformationContent
-d KappaShapeIndex -d Lipinski -d LogS -d McGowanVolume -d MoeType
-d MolecularDistanceEdge -d MolecularId -d PathCount -d Polarizability
-d RingCount -d RotatableBond -d SLogP -d TopoPSA -d TopologicalCharge -d TopologicalIndex
-d VdwVolumeABC -d VertexAdjacencyInformation -d WalkCount -d Weight -d WienerIndex
-d ZagrebIndex
178/17: !python -m mordred -t smi smiles/new_smiles.smi -o smiles/mordard.csv -d ABCIndex, -d AcidBase -d AdjacencyMatrix -d Aromatic -d AtomCount -d Autocorrelation -d BCUT -d BalabanJ -d BaryszMatrix -d BertzCT -d BondCount -d CarbonTypes
178/18:
!python -m mordred -t smi smiles/new_smiles.smi -o smiles/mordard.csv -d ABCIndex -d AcidBase -d AdjacencyMatrix -d Aromatic -d AtomCount -d Autocorrelation -d BCUT -d BalabanJ -d BaryszMatrix -d BertzCT -d BondCount -d CarbonTypes
-d Chi -d Constitutional -d DetourMatrix -d DistanceMatrix -d EState
-d EccentricConnectivityIndex -d ExtendedTopochemicalAtom -d FragmentComplexity
-d Framework -d GeometricalIndex -d GravitationalIndex -d HydrogenBond -d InformationContent
-d KappaShapeIndex -d Lipinski -d LogS -d McGowanVolume -d MoeType
-d MolecularDistanceEdge -d MolecularId -d PathCount -d Polarizability
-d RingCount -d RotatableBond -d SLogP -d TopoPSA -d TopologicalCharge -d TopologicalIndex
-d VdwVolumeABC -d VertexAdjacencyInformation -d WalkCount -d Weight -d WienerIndex
-d ZagrebIndex
178/19:
!python -m mordred -t smi smiles/new_smiles.smi -o smiles/mordard.csv -d ABCIndex -d AcidBase -d AdjacencyMatrix -d Aromatic -d AtomCount -d Autocorrelation -d BCUT -d BalabanJ -d BaryszMatrix -d BertzCT -d BondCount -d CarbonTypes
-d Chi -d Constitutional -d DetourMatrix -d DistanceMatrix -d EState
-d EccentricConnectivityIndex -d ExtendedTopochemicalAtom -d FragmentComplexity
-d Framework -d GeometricalIndex -d GravitationalIndex -d HydrogenBond -d InformationContent
-d KappaShapeIndex -d Lipinski -d LogS -d McGowanVolume -d MoeType
-d MolecularDistanceEdge -d MolecularId -d PathCount -d Polarizability
-d RingCount -d RotatableBond -d SLogP -d TopoPSA -d TopologicalCharge -d TopologicalIndex
-d VdwVolumeABC -d VertexAdjacencyInformation -d WalkCount -d Weight -d WienerIndex
-d ZagrebIndex
178/20:
-d Chi -d Constitutional -d DetourMatrix -d DistanceMatrix -d EState
-d EccentricConnectivityIndex -d ExtendedTopochemicalAtom -d FragmentComplexity
-d Framework -d GeometricalIndex -d GravitationalIndex -d HydrogenBond -d InformationContent
-d KappaShapeIndex -d Lipinski -d LogS -d McGowanVolume -d MoeType
-d MolecularDistanceEdge -d MolecularId -d PathCount -d Polarizability
-d RingCount -d RotatableBond -d SLogP -d TopoPSA -d TopologicalCharge -d TopologicalIndex
-d VdwVolumeABC -d VertexAdjacencyInformation -d WalkCount -d Weight -d WienerIndex
-d ZagrebIndex
178/21:
!python -m mordred -t smi smiles/new_smiles.smi -o smiles/mordard.csv -d ABCIndex -d AcidBase -d AdjacencyMatrix -d Aromatic -d AtomCount -d Autocorrelation -d BCUT -d BalabanJ -d BaryszMatrix -d BertzCT -d BondCount -d CarbonTypes
    -d Chi -d Constitutional -d DetourMatrix -d DistanceMatrix -d EState
    -d EccentricConnectivityIndex -d ExtendedTopochemicalAtom -d FragmentComplexity
    -d Framework -d GeometricalIndex -d GravitationalIndex -d HydrogenBond -d InformationContent
    -d KappaShapeIndex -d Lipinski -d LogS -d McGowanVolume -d MoeType
    -d MolecularDistanceEdge -d MolecularId -d PathCount -d Polarizability
    -d RingCount -d RotatableBond -d SLogP -d TopoPSA -d TopologicalCharge -d TopologicalIndex
    -d VdwVolumeABC -d VertexAdjacencyInformation -d WalkCount -d Weight -d WienerIndex
    -d ZagrebIndex
178/22: !python -m mordred -t smi smiles/new_smiles.smi -o smiles/mordard.csv -d ABCIndex -d AcidBase -d AdjacencyMatrix -d Aromatic -d AtomCount -d Autocorrelation -d BCUT -d BalabanJ -d BaryszMatrix -d BertzCT -d BondCount -d CarbonTypes
178/23: !python -m mordred -t smi smiles/new_smiles.smi -o smiles/mordard.csv -d ABCIndex -d AcidBase -d AdjacencyMatrix -d Aromatic -d AtomCount -d Autocorrelation -d BCUT -d BalabanJ -d BaryszMatrix -d BertzCT -d BondCount -d CarbonTypes -d Chi -d Constitutional -d DetourMatrix -d DistanceMatrix -d EState -d EccentricConnectivityIndex -d ExtendedTopochemicalAtom -d FragmentComplexity -d Framework -d GeometricalIndex -d GravitationalIndex -d HydrogenBond -d InformationContent -d KappaShapeIndex -d Lipinski -d LogS -d McGowanVolume -d MoeType -d MolecularDistanceEdge -d MolecularId -d PathCount -d Polarizability -d RingCount -d RotatableBond -d SLogP -d TopoPSA -d TopologicalCharge -d TopologicalIndex -d VdwVolumeABC -d VertexAdjacencyInformation -d WalkCount -d Weight -d WienerIndex -d ZagrebIndex
178/24: !python -m mordred -t smi smiles/new_smiles.smi -o smiles/mordard.csv -d ABCIndex -d AcidBase -d AdjacencyMatrix -d Aromatic -d AtomCount -d Autocorrelation -d BCUT -d BalabanJ -d BertzCT -d BondCount -d CarbonTypes -d Chi -d Constitutional -d DetourMatrix -d DistanceMatrix -d EState -d EccentricConnectivityIndex -d ExtendedTopochemicalAtom -d FragmentComplexity -d Framework -d GeometricalIndex -d GravitationalIndex -d HydrogenBond -d InformationContent -d KappaShapeIndex -d Lipinski -d LogS -d McGowanVolume -d MoeType -d MolecularDistanceEdge -d MolecularId -d PathCount -d Polarizability -d RingCount -d RotatableBond -d SLogP -d TopoPSA -d TopologicalCharge -d TopologicalIndex -d VdwVolumeABC -d VertexAdjacencyInformation -d WalkCount -d Weight -d WienerIndex -d ZagrebIndex
178/25: !python -m mordred -t smi smiles/new_smiles.smi -o smiles/mordard.csv -d ABCIndex -d AcidBase -d AdjacencyMatrix -d Aromatic -d AtomCount -d Autocorrelation -d BCUT -d BalabanJ -d BertzCT -d BondCount -d CarbonTypes -d Chi -d Constitutional -d DistanceMatrix -d EState -d EccentricConnectivityIndex -d ExtendedTopochemicalAtom -d FragmentComplexity -d Framework -d GeometricalIndex -d GravitationalIndex -d HydrogenBond -d InformationContent -d KappaShapeIndex -d Lipinski -d LogS -d McGowanVolume -d MoeType -d MolecularDistanceEdge -d MolecularId -d PathCount -d Polarizability -d RingCount -d RotatableBond -d SLogP -d TopoPSA -d TopologicalCharge -d TopologicalIndex -d VdwVolumeABC -d VertexAdjacencyInformation -d WalkCount -d Weight -d WienerIndex -d ZagrebIndex
179/1:
path = r'Fingerprint/'

Fp1  = pd.read_csv(path + 'Fingerprinter.csv'               , header = 0)
Fp2  = pd.read_csv(path + 'ExtendedFingerprinter.csv'       , header = 0)
Fp3  = pd.read_csv(path + 'EStateFingerprinter.csv'         , header = 0)
Fp4  = pd.read_csv(path + 'GraphOnlyFingerprinter.csv'      , header = 0)
Fp5  = pd.read_csv(path + 'MACCSFingerprinter.csv'          , header = 0)
Fp6  = pd.read_csv(path + 'PubchemFingerprinter.csv'        , header = 0)
Fp7  = pd.read_csv(path + 'SubstructureFingerprinter.csv'   , header = 0)
Fp8  = pd.read_csv(path + 'SubstructureFingerprintCount.csv', header = 0)
Fp9  = pd.read_csv(path + 'KlekotaRothFingerprinter.csv'    , header = 0)
Fp10 = pd.read_csv(path + 'KlekotaRothFingerprintCount.csv' , header = 0)
Fp11 = pd.read_csv(path + 'AtomPairs2DFingerprinter.csv'    , header = 0)
Fp12 = pd.read_csv(path + 'AtomPairs2DFingerprintCount.csv' , header = 0)
Fp13  = pd.read_csv(path + 'mordard.csv'               , header = 0)
179/2:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
179/3:
path = r'Fingerprint/'

Fp1  = pd.read_csv(path + 'Fingerprinter.csv'               , header = 0)
Fp2  = pd.read_csv(path + 'ExtendedFingerprinter.csv'       , header = 0)
Fp3  = pd.read_csv(path + 'EStateFingerprinter.csv'         , header = 0)
Fp4  = pd.read_csv(path + 'GraphOnlyFingerprinter.csv'      , header = 0)
Fp5  = pd.read_csv(path + 'MACCSFingerprinter.csv'          , header = 0)
Fp6  = pd.read_csv(path + 'PubchemFingerprinter.csv'        , header = 0)
Fp7  = pd.read_csv(path + 'SubstructureFingerprinter.csv'   , header = 0)
Fp8  = pd.read_csv(path + 'SubstructureFingerprintCount.csv', header = 0)
Fp9  = pd.read_csv(path + 'KlekotaRothFingerprinter.csv'    , header = 0)
Fp10 = pd.read_csv(path + 'KlekotaRothFingerprintCount.csv' , header = 0)
Fp11 = pd.read_csv(path + 'AtomPairs2DFingerprinter.csv'    , header = 0)
Fp12 = pd.read_csv(path + 'AtomPairs2DFingerprintCount.csv' , header = 0)
Fp13  = pd.read_csv(path + 'mordard.csv'               , header = 0)
179/4: Fp1.shape
179/5:
df = pd.read_csv('df6_lipinski_kpu.csv')
df
179/6: name = df['compound']
179/7: name.shape
179/8:
df2 = df.rename({'compound': 'Name'}, axis=1)
df2 = df2.drop('Unnamed: 0', axis=1)
df3 = df2.set_index('Name')
df3
179/9:
LogKpu = df3['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']
LogKpu = LogKpu.to_frame()
LogKpu
179/10:
LogKpu = df3[['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]
LogKpu = LogKpu.to_frame()
LogKpu
179/11:
LogKpu = df3[['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]
LogKpu
179/12:
Fp1.Name = name
Fp2.Name = name
Fp3.Name = name
Fp4.Name = name
Fp5.Name = name
Fp6.Name = name
Fp7.Name = name
Fp8.Name = name
Fp9.Name = name
Fp10.Name = name
Fp11.Name = name
Fp12.Name = name
Fp13.Name = name
179/13: Fp13
179/14: name = df['compound']
179/15: name.shape
179/16:
df2 = df.rename({'compound': 'Name'}, axis=1)
df2 = df2.drop('Unnamed: 0', axis=1)
df3 = df2.set_index('Name')
df3
179/17:
LogKpu = df3[['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]
LogKpu
179/18:
Fp1.Name = name
Fp2.Name = name
Fp3.Name = name
Fp4.Name = name
Fp5.Name = name
Fp6.Name = name
Fp7.Name = name
Fp8.Name = name
Fp9.Name = name
Fp10.Name = name
Fp11.Name = name
Fp12.Name = name
Fp13.Name = name
179/19:
Fpp1 = Fp1.drop('Name', axis=1)
Fpp2 = Fp2.drop('Name', axis=1)
Fpp3 = Fp3.drop('Name', axis=1)
Fpp4 = Fp4.drop('Name', axis=1)
Fpp5 = Fp5.drop('Name', axis=1)
Fpp6 = Fp6.drop('Name', axis=1)
Fpp7 = Fp7.drop('Name', axis=1)
Fpp8 = Fp8.drop('Name', axis=1)
Fpp9 = Fp9.drop('Name', axis=1)
Fpp10 = Fp10.drop('Name', axis=1)
Fpp11 = Fp11.drop('Name', axis=1)
Fpp12 = Fp12.drop('Name', axis=1)
Fpp13 = Fp13.drop('Name', axis=1)
179/20: Fp13
179/21: Fp13.rename {name:Name}
179/22: Fp13.rename({name:Name})
179/23: Fp13.rename(name:Name)
179/24: Fp13.rename(name: Name)
179/25: Fp13.rename(name = Name)
179/26: Fp13.rename({'name': 'Name'})
179/27:
Fp13 = Fp13.rename({'name': 'Name'})
Fp13
179/28:
Fp13 = Fp13.rename({'Name': 'name'})
Fp13
179/29:
Fp13 = Fp13.rename({'name': 'Mame'})
Fp13
179/30:
Fp13 = Fp13.rename(columns={'name': 'Name'})
Fp13
179/31:
Fp1.Name = name
Fp2.Name = name
Fp3.Name = name
Fp4.Name = name
Fp5.Name = name
Fp6.Name = name
Fp7.Name = name
Fp8.Name = name
Fp9.Name = name
Fp10.Name = name
Fp11.Name = name
Fp12.Name = name
Fp13.Name = name
179/32:
Fpp1 = Fp1.drop('Name', axis=1)
Fpp2 = Fp2.drop('Name', axis=1)
Fpp3 = Fp3.drop('Name', axis=1)
Fpp4 = Fp4.drop('Name', axis=1)
Fpp5 = Fp5.drop('Name', axis=1)
Fpp6 = Fp6.drop('Name', axis=1)
Fpp7 = Fp7.drop('Name', axis=1)
Fpp8 = Fp8.drop('Name', axis=1)
Fpp9 = Fp9.drop('Name', axis=1)
Fpp10 = Fp10.drop('Name', axis=1)
Fpp11 = Fp11.drop('Name', axis=1)
Fpp12 = Fp12.drop('Name', axis=1)
Fpp13 = Fp13.drop('Name', axis=1)
179/33:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
179/34:
Fp1_n  = normalized (Fpp1 )
Fp2_n  = normalized (Fpp2 )
Fp3_n  = normalized (Fpp3 )
Fp4_n  = normalized (Fpp4 )
Fp5_n  = normalized (Fpp5 )
Fp6_n  = normalized (Fpp6 )
Fp7_n  = normalized (Fpp7 )
Fp8_n  = normalized (Fpp8 )
Fp9_n  = normalized (Fpp9 )
Fp10_n = normalized (Fpp10)
Fp11_n = normalized (Fpp11)
Fp12_n = normalized (Fpp12)
Fp13_n = normalized (Fpp13)
179/35: Fp13_n
179/36:
Fp1_n["Name"] = Fp1.Name
Fp2_n["Name"] = Fp2.Name
Fp3_n["Name"] = Fp3.Name
Fp4_n["Name"] = Fp4.Name
Fp5_n["Name"] = Fp5.Name
Fp6_n["Name"] = Fp6.Name
Fp7_n["Name"] = Fp7.Name
Fp8_n["Name"] = Fp8.Name
Fp9_n["Name"] = Fp9.Name
Fp10_n["Name"] = Fp10.Name
Fp11_n["Name"] = Fp11.Name
Fp12_n["Name"] = Fp12.Name
Fp13_n["Name"] = Fp13.Name
179/37: Fp1_n
179/38:
Fp1_n = Fp1_n.set_index('Name')
Fp2_n = Fp2_n.set_index('Name')
Fp3_n = Fp3_n.set_index('Name')
Fp4_n = Fp4_n.set_index('Name')
Fp5_n = Fp5_n.set_index('Name')
Fp6_n = Fp6_n.set_index('Name')
Fp7_n = Fp7_n.set_index('Name')
Fp8_n = Fp8_n.set_index('Name')
Fp9_n = Fp9_n.set_index('Name')
Fp10_n = Fp10_n.set_index('Name')
Fp11_n = Fp11_n.set_index('Name')
Fp12_n = Fp12_n.set_index('Name')
Fp13_n = Fp13_n.set_index('Name')
179/39: Fp1_n
179/40:
Fp1_n .to_csv('Fp_normalized/Fingerprinter.csv'               , sep=',' ,index=False)
Fp2_n .to_csv('Fp_normalized/ExtendedFingerprinter.csv'       , sep=',' ,index=False)
Fp3_n .to_csv('Fp_normalized/EStateFingerprinter.csv'         , sep=',' ,index=False)
Fp4_n .to_csv('Fp_normalized/GraphOnlyFingerprinter.csv'      , sep=',' ,index=False)
Fp5_n .to_csv('Fp_normalized/MACCSFingerprinter.csv'          , sep=',' ,index=False)
Fp6_n .to_csv('Fp_normalized/PubchemFingerprinter.csv'        , sep=',' ,index=False)
Fp7_n .to_csv('Fp_normalized/SubstructureFingerprinter.csv'   , sep=',' ,index=False)
Fp8_n .to_csv('Fp_normalized/SubstructureFingerprintCount.csv', sep=',' ,index=False)
Fp9_n .to_csv('Fp_normalized/KlekotaRothFingerprinter.csv'    , sep=',' ,index=False)
Fp10_n.to_csv('Fp_normalized/KlekotaRothFingerprintCount.csv' , sep=',' ,index=False)
Fp11_n.to_csv('Fp_normalized/AtomPairs2DFingerprinter.csv'    , sep=',' ,index=False)
Fp12_n.to_csv('Fp_normalized/AtomPairs2DFingerprintCount.csv' , sep=',' ,index=False)
Fp13_n.to_csv('Fp_normalized/AtomPairs2DFingerprintCount.csv' , sep=',' ,index=False)
179/41: raw13  = LogKpu.merge(Fp13_n, on='Name', how='outer')
179/42: raw13
179/43:

raw13.to_csv('QSAR_Kpu/Mordard.csv' , sep=',' ,index=False)
179/44:
print (len(raw1 ),len(raw2 ),len(raw3 ),len(raw4 ),len(raw5 )
      ,len(raw6 ),len(raw7 ),len(raw8 ),len(raw9 ),len(raw10)
      ,len(raw11),len(raw12))
179/45: print (len(raw13)
179/46: print (len(raw13))
179/47: print (len(raw13 .columns))
177/1:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
177/2:
df = pd.read_csv("QSAR_Kpu/Mordard.csv")
df
177/3:
X = df.drop(['LogKpu'], axis=1)
X.shape
177/4:
y = df1['LogKp']
y
177/5:
y = df['LogKp']
y
177/6:
y = df['LogKpu']
y
177/7: y.shape
177/8:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
X = sel.fit_transform(X)
X.shape
177/9:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.10))
X = sel.fit_transform(X)
X.shape
177/10:
from sklearn.model_selection import KFold, cross_val_score, cross_val_predict
kf =KFold(n_splits=10, shuffle=False)
177/11:
cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1
177/12:
from sklearn.ensemble import RandomForestRegressor
RF = RandomForestRegressor(n_estimators=400)
y_pred = cross_val_predict(RF, X, y, cv=kf)
177/13:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.10))
X = sel.fit_transform(X)
X.shape
179/48: raw13.isnull()
179/49: raw13.isnull(sum)
179/50: raw13.isna()
177/14: df.isnull()
177/15: df.isnull().sum
177/16: df.isna().sum
177/17: df.dropna()
177/18: df.dropna(column)
177/19: df.fillna(0)
177/20:
X = df.drop(['LogKpu'], axis=1)
X.shape
177/21:
y = df['LogKpu']
y
177/22: y.shape
177/23:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.10))
X = sel.fit_transform(X)
X.shape
177/24:
from sklearn.model_selection import KFold, cross_val_score, cross_val_predict
kf =KFold(n_splits=10, shuffle=False)
177/25:
cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1
177/26:
from sklearn.ensemble import RandomForestRegressor
RF = RandomForestRegressor(n_estimators=400)
y_pred = cross_val_predict(RF, X, y, cv=kf)
177/27:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
177/28: normalized(df)
177/29: df2 = normalized(df)
177/30:
X = df2.drop(['LogKpu'], axis=1)
X.shape
177/31:
y = df2['LogKpu']
y
177/32:
X = df.drop(['LogKpu'], axis=1)
X.shape
177/33:
y = df['LogKpu']
y
177/34: df2 = normalized(X)
177/35: X = normalized(X)
177/36:
X = normalized(X)
X.shape
177/37: X.isna()
177/38:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.10))
X = sel.fit_transform(X)
X.shape
177/39:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.10))
X = sel.fit_transform(X)
X
177/40:
from sklearn.model_selection import KFold, cross_val_score, cross_val_predict
kf =KFold(n_splits=10, shuffle=False)
177/41:
cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1
177/42:
from sklearn.ensemble import RandomForestRegressor
RF = RandomForestRegressor(n_estimators=400)
y_pred = cross_val_predict(RF, X, y, cv=kf)
177/43:
X = normalized(X)
X = np.nan_to_num(X)
177/44: X = normalized(X)
177/45:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
177/46: X = normalized(X)
177/47: normalized(X)
177/48:
X = df.drop(['LogKpu'], axis=1)
X.shape
177/49:
y = df['LogKpu']
y
177/50:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
177/51: pwd
177/52:
df = pd.read_csv("QSAR_Kpu/Mordard.csv")
df
177/53: df.fillna(0)
177/54:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
177/55:
X = df.drop(['LogKpu'], axis=1)
X.shape
177/56:
y = df['LogKpu']
y
177/57: y.shape
177/58: normalized(X)
177/59: X = normalized(X)
177/60: X = np.nan_to_num(X)
177/61:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.10))
X = sel.fit_transform(X)
X
177/62:
from sklearn.model_selection import KFold, cross_val_score, cross_val_predict
kf =KFold(n_splits=10, shuffle=False)
177/63:
cnt = 1
# split()  method generate indices to split data into training and test set.
for train_index, test_index in kf.split(X, y):
    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')
    cnt += 1
177/64:
from sklearn.ensemble import RandomForestRegressor
RF = RandomForestRegressor(n_estimators=400)
y_pred = cross_val_predict(RF, X, y, cv=kf)
181/1: print('hello world')
183/1:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
185/1:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
185/2: pwd
185/3:
df = pd.read_csv("QSAR_Kpu/Mordard.csv")
df
185/4:
df = pd.read_csv("QSAR_Kpu/Mordard.csv")
df
189/1:
#import important library
import numpy as pd
import pandas as pd
189/2: df = pd.read_csv('df6_lipinski_Kpu.csv')
189/3: df
189/4: df.head(6)
189/5: df.shape
189/6: df = df.drop(df['Unnamed: 0'])
189/7: df = df.drop(df['Unnamed: 0'])
189/8: df
189/9: df = df.drop(['Unnamed: 0'])
189/10: df = df.drop(['Unnamed: 0'], axis = 1)
189/11: df
189/12: df.shape
189/13: df = pd.read_csv('df6_lipinski_Kpu.csv')
189/14: df.head(6)
189/15: df.head(6)
189/16: df.shape
189/17: df1 = df.drop(['Unnamed: 0'], axis = 1)
189/18: df
189/19: df1
189/20:
#import important library
import numpy as pd
import pandas as pd
from sklearn import svm, metrics, clone
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import KFold, train_test_split
from sklearn.metrics import auc, accuracy_score, recall_score
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt
from rdkit import Chem
from rdkit.Chem import MACCSkeys
from rdkit.Chem.AllChem import GetMorganFingerprintAsBitVect
189/21: df1 = df[['compound', 'smiles'], 'LogKpu']
189/22: df1 = df[['compound', 'smiles', 'LogKpu']]
189/23: df1
189/24:
def smilestofp (smiles, method="maccs", n_bits=2048):
     # convert smiles to RDKit mol object
    mol = Chem.MolFromSmiles(smiles)

    if method == "maccs":
        return np.array(MACCSkeys.GenMACCSKeys(mol))
    if method == "morgan2":
        return np.array(GetMorganFingerprintAsBitVect(mol, 2, nBits=n_bits))
    if method == "morgan3":
        return np.array(GetMorganFingerprintAsBitVect(mol, 3, nBits=n_bits))
    else:
        # NBVAL_CHECK_OUTPUT
        print(f"Warning: Wrong method specified: {method}. Default will be used instead.")
        return np.array(MACCSkeys.GenMACCSKeys(mol))
189/25:
# Add column for fingerprint
df1["fp"] = df1["smiles"].apply(smiles_to_fp)
compound_df.head(3)
# NBVAL_CHECK_OUTPUT
189/26:
def smiles_to_fp (smiles, method="maccs", n_bits=2048):
     # convert smiles to RDKit mol object
    mol = Chem.MolFromSmiles(smiles)

    if method == "maccs":
        return np.array(MACCSkeys.GenMACCSKeys(mol))
    if method == "morgan2":
        return np.array(GetMorganFingerprintAsBitVect(mol, 2, nBits=n_bits))
    if method == "morgan3":
        return np.array(GetMorganFingerprintAsBitVect(mol, 3, nBits=n_bits))
    else:
        # NBVAL_CHECK_OUTPUT
        print(f"Warning: Wrong method specified: {method}. Default will be used instead.")
        return np.array(MACCSkeys.GenMACCSKeys(mol))
189/27:
# Add column for fingerprint
df1["fp"] = df1["smiles"].apply(smiles_to_fp)
compound_df.head(3)
# NBVAL_CHECK_OUTPUT
189/28:
#import important library
import numpy as np
import pandas as pd
from sklearn import svm, metrics, clone
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import KFold, train_test_split
from sklearn.metrics import auc, accuracy_score, recall_score
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt
from rdkit import Chem
from rdkit.Chem import MACCSkeys
from rdkit.Chem.AllChem import GetMorganFingerprintAsBitVect
189/29:
# Add column for fingerprint
df1["fp"] = df1["smiles"].apply(smiles_to_fp)
compound_df.head(3)
# NBVAL_CHECK_OUTPUT
189/30:
# Add column for fingerprint
df1["fp"] = df1["smiles"].apply(smiles_to_fp)
df1.head(3)
# NBVAL_CHECK_OUTPUT
189/31:
compound_df = df1.copy()
# Add column for fingerprint
compound_df["fp"] = compound_df["smiles"].apply(smiles_to_fp)
compound_df.head(3)
# NBVAL_CHECK_OUTPUT
189/32:
# Fix seed for reproducible results
SEED = 22
seed_everything(SEED)
189/33:
# Fix seed for reproducible results
import random

random.seed(10)
189/34:
# Fix seed for reproducible results
import random

seed = random.seed(10)
189/35:
# Fix seed for reproducible results
import random

seed = random.seed(10)
seed
189/36:
# Fix seed for reproducible results
import random

seed = random.seed(10)
seed
189/37: seed
189/38:
fingerprint_to_model = compound_df.fp.tolist()
label_to_model = compound_df.LohKpu.tolist()
train_internal, train_external, test_internal, train_external = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
189/39:
fingerprint_to_model = compound_df.fp.tolist()
label_to_model = compound_df.LogKpu.tolist()
train_internal, train_external, test_internal, train_external = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
189/40: trian_internal.shape
189/41: train_internal.shape
189/42:
fingerprint_to_model = compound_df.fp.tolist()
label_to_model = compound_df.LogKpu.tolist()
train_internal_x, train_external_y, test_internal_x, test_external-y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
print("Training data size:", len(train))
print("Test data size:", len(test))
189/43:
fingerprint_to_model = compound_df.fp.tolist()
label_to_model = compound_df.LogKpu.tolist()
train_internal_x, train_external_y, test_internal_x, test_external_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
print("Training data size:", len(train))
print("Test data size:", len(test))
189/44:
fingerprint_to_model = compound_df.fp.tolist()
label_to_model = compound_df.LogKpu.tolist()
train_internal_x, train_external_y, test_internal_x, test_external_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
print("Training data size:", len(train_internal))
print("Test data size:", len(train_external))
189/45:
# Shuffle the indices for the k-fold cross-validation
    kf = KFold(n_splits=n_folds, shuffle=True, random_state=SEED)
189/46:
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=n_folds, shuffle=True, random_state=SEED)
189/47:
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
189/48:
from sklearn.linear_model import LinearRegression
reg = LinearRegression()
189/49:
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
cross_validation_score(reg, train_internal_x, train_external_y, cv=kf)
189/50:
from sklearn.model_selection import cross_validation_score
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
cross_validation_score(reg, train_internal_x, train_external_y, cv=kf)
189/51:
from sklearn.model_selection import cross_val_score
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
cross_validation_score(reg, train_internal_x, train_external_y, cv=kf)
189/52:
from sklearn.model_selection import cross_val_score
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
cross_val_score(reg, train_internal_x, train_external_y, cv=kf)
189/53:
from sklearn.model_selection import cross_val_score
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
cross_val_score(reg, train_internal_x, train_internal_y, cv=kf)
189/54:
fingerprint_to_model = compound_df.fp.tolist()
label_to_model = compound_df.LogKpu.tolist()
train_internal_x, train_internal_y, test_internal_x, test_external_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
print("Training data size:", len(train_internal))
print("Test data size:", len(train_external))
189/55:
from sklearn.linear_model import LinearRegression
reg = LinearRegression()
189/56:
from sklearn.model_selection import cross_val_score
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
cross_val_score(reg, train_internal_x, train_internal_y, cv=kf)
189/57: train_internal_y
189/58: train_internal_y.shape
189/59: len(train_internal_y)
189/60: len(train_internal_x)
189/61: len(train_internal_y)
189/62: train_internal_x
189/63:
fingerprint_to_model = compound_df.fp
label_to_model = compound_df.LogKpu
train_internal_x, train_internal_y, test_internal_x, test_external_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
print("Training data size:", len(train_internal))
print("Test data size:", len(train_external))
189/64:
from sklearn.linear_model import LinearRegression
reg = LinearRegression()
189/65:
from sklearn.model_selection import cross_val_score
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
cross_val_score(reg, train_internal_x, train_internal_y, cv=kf)
189/66:
fingerprint_to_model = compound_df.fp.tolist()
label_to_model = compound_df.LogKpu.tolist()
train_internal_x, train_internal_y, test_internal_x, test_external_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
print("Training data size:", len(train_internal))
print("Test data size:", len(train_external))
189/67:
from sklearn.linear_model import LinearRegression
reg = LinearRegression()
189/68:
from sklearn.model_selection import cross_val_score
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
cross_val_score(reg, train_internal_x, train_internal_y, cv=kf)
189/69:
fingerprint_to_model = compound_df.fp.tolist()
label_to_model = compound_df.LogKpu.tolist()
train_internal_x, train_external_y, test_internal_x, test_external_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
print("Training data size:", len(train_internal))
print("Test data size:", len(train_external))
189/70:
from sklearn.linear_model import LinearRegression
reg = LinearRegression()
189/71:
from sklearn.model_selection import cross_val_score
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
cross_val_score(reg, train_internal_x, train_internal_x, cv=kf)
189/72:
from sklearn.model_selection import cross_val_score
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
cross_val_score(reg, train_internal_x, train_internal_x, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/73:
from sklearn.model_selection import cross_val_score
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
scores=cross_val_score(reg, train_internal_x, train_internal_x, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/74:
def model_performance(ml_model, test_x, test_y, verbose=True):
    """
    Helper function to calculate model performance

    Parameters
    ----------
    ml_model: sklearn model object
        The machine learning model to train.
    test_x: list
        Molecular fingerprints for test set.
    test_y: list
        Associated activity labels for test set.
    verbose: bool
        Print performance measure (default = True)

    Returns
    -------
    tuple:
        Accuracy, sensitivity, specificity, auc on test set.
    """

    # Prediction probability on test set
    test_prob = ml_model.predict_proba(test_x)[:, 1]

    # Prediction class on test set
    test_pred = ml_model.predict(test_x)

    # Performance of model on test set
    accuracy = accuracy_score(test_y, test_pred)
    sens = recall_score(test_y, test_pred)
    spec = recall_score(test_y, test_pred, pos_label=0)
    auc = roc_auc_score(test_y, test_prob)

    if verbose:
        # Print performance results
        # NBVAL_CHECK_OUTPUT        print(f"Accuracy: {accuracy:.2}")
        print(f"Sensitivity: {sens:.2f}")
        print(f"Specificity: {spec:.2f}")
        print(f"AUC: {auc:.2f}")

    return accuracy, sens, spec, auc
189/75:
from sklearn.model_selection import cross_val_score
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
scores=cross_val_score(reg, train_internal_x, train_internal_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/76:
from sklearn.model_selection import cross_val_score
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
scores=cross_val_score(reg, train_internal_x, train_internal_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/77:
from sklearn.model_selection import cross_val_score
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
scores=cross_val_score(reg, train_internal_x, train_external_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/78:
fingerprint_to_model = compound_df.fp.tolist()
label_to_model = compound_df.LogKpu.tolist()
train_internal_x, train_external_x, test_internal_y, test_external_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
print("Training data size:", len(train_internal))
print("Test data size:", len(train_external))
189/79:
from sklearn.model_selection import cross_val_score
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
scores=cross_val_score(reg, train_internal_x, train_internal_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/80:
fingerprint_to_model = compound_df.fp.tolist()
label_to_model = compound_df.LogKpu.tolist()
train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
print("Training data size:", len(train_internal))
print("Test data size:", len(train_external))
189/81:
from sklearn.linear_model import LinearRegression
reg = LinearRegression()
189/82:
from sklearn.model_selection import cross_val_score
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
scores=cross_val_score(reg, train_internal_x, train_internal_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/83:
from sklearn.model_selection import cross_val_score
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
scores=cross_val_score(reg, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/84:
from sklearn.model_selection import cross_val_score
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
scores=cross_val_score(reg, train_x, train_y, cv=kf)
scores
189/85:
from sklearn.model_selection import cross_val_score
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
scores=cross_val_score(reg, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/86:
from sklearn.model_selection import cross_val_score
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
scores=cross_val_score(reg, train_x, train_x, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/87:
from sklearn.model_selection import cross_val_score
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
scores=cross_val_score(reg, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/88:
fingerprint_to_model = compound_df.fp.tolist()
label_to_model = compound_df.LogKpu.tolist()
train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
print("Training data size:", len(train_x))
print("Test data size:", len(train_x))
189/89:
fingerprint_to_model = compound_df.fp.tolist()
label_to_model = compound_df.LogKpu.tolist()
train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
189/90:
from sklearn.model_selection import cross_val_score
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
scores=cross_val_score(reg, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/91: np.any(np.isnan(fingerprint_to_model))
189/92: np.any(np.isnan(label_to_model))
189/93: label_to_model
189/94:
df1 = df[['compound', 'smiles', 'LogKpu']]
df2 =df1[df1.kp.notna()]
189/95:
df1 = df[['compound', 'smiles', 'LogKpu']]
df2 =df1[df1.LogKpu.notna()]
189/96: df2
189/97:
def smiles_to_fp (smiles, method="maccs", n_bits=2048):
     # convert smiles to RDKit mol object
    mol = Chem.MolFromSmiles(smiles)

    if method == "maccs":
        return np.array(MACCSkeys.GenMACCSKeys(mol))
    if method == "morgan2":
        return np.array(GetMorganFingerprintAsBitVect(mol, 2, nBits=n_bits))
    if method == "morgan3":
        return np.array(GetMorganFingerprintAsBitVect(mol, 3, nBits=n_bits))
    else:
        # NBVAL_CHECK_OUTPUT
        print(f"Warning: Wrong method specified: {method}. Default will be used instead.")
        return np.array(MACCSkeys.GenMACCSKeys(mol))
189/98:
compound_df = df1.copy()
# Add column for fingerprint
compound_df["fp"] = compound_df["smiles"].apply(smiles_to_fp)
compound_df.head(3)
# NBVAL_CHECK_OUTPUT
189/99:
fingerprint_to_model = compound_df.fp.tolist()
label_to_model = compound_df.LogKpu.tolist()
train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
189/100: df2
189/101:
compound_df = df2.copy()
# Add column for fingerprint
compound_df["fp"] = compound_df["smiles"].apply(smiles_to_fp)
compound_df.head(3)
# NBVAL_CHECK_OUTPUT
189/102:
fingerprint_to_model = compound_df.fp.tolist()
label_to_model = compound_df.LogKpu.tolist()
train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
189/103:
from sklearn.linear_model import LinearRegression
reg = LinearRegression()
189/104: np.any(np.isnan(fingerprint_to_model))
189/105: np.any(np.isnan(label_to_model))
189/106:
from sklearn.model_selection import cross_val_score. cross_val_predict
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
scores=cross_val_score(reg, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/107:
from sklearn.model_selection import cross_val_score, cross_val_predict
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
scores=cross_val_score(reg, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/108:
def model_performance(ml_model, test_x, test_y, verbose=True):
    """
    Helper function to calculate model performance

    Parameters
    ----------
    ml_model: sklearn model object
        The machine learning model to train.
    test_x: list
        Molecular fingerprints for test set.
    test_y: list
        Associated activity labels for test set.
    verbose: bool
        Print performance measure (default = True)

    Returns
    -------
    tuple:
        Accuracy, sensitivity, specificity, auc on test set.
    """

    # Prediction probability on test set
    test_prob = ml_model.predict_proba(test_x)[:, 1]

    # Prediction class on test set
    test_pred = ml_model.predict(test_x)

    # Performance of model on test set
    accuracy = accuracy_score(test_y, test_pred)
    sens = recall_score(test_y, test_pred)
    spec = recall_score(test_y, test_pred, pos_label=0)
    auc = roc_auc_score(test_y, test_prob)

    if verbose:
        # Print performance results
        # NBVAL_CHECK_OUTPUT        print(f"Accuracy: {accuracy:.2}")
        print(f"Sensitivity: {sens:.2f}")
        print(f"Specificity: {spec:.2f}")
        print(f"AUC: {auc:.2f}")

    return accuracy, sens, spec, auc
189/109: model_performance(reg, test_x, test_y)
189/110:
def model_training_and_validation(ml_model, name, splits, verbose=True):
    """
    Fit a machine learning model on a random train-test split of the data
    and return the performance measures.

    Parameters
    ----------
    ml_model: sklearn model object
        The machine learning model to train.
    name: str
        Name of machine learning algorithm: RF, SVM, ANN
    splits: list
        List of desciptor and label data: train_x, test_x, train_y, test_y.
    verbose: bool
        Print performance info (default = True)

    Returns
    -------
    tuple:
        Accuracy, sensitivity, specificity, auc on test set.

    """
    train_x, test_x, train_y, test_y = splits

    # Fit the model
    ml_model.fit(train_x, train_y)

    # Calculate model performance results
    accuracy, sens, spec, auc = model_performance(ml_model, test_x, test_y, verbose)

    return accuracy, sens, spec, auc
189/111:
def model_training_and_validation(ml_model, name, splits, verbose=True):
    """
    Fit a machine learning model on a random train-test split of the data
    and return the performance measures.

    Parameters
    ----------
    ml_model: sklearn model object
        The machine learning model to train.
    name: str
        Name of machine learning algorithm: RF, SVM, ANN
    splits: list
        List of desciptor and label data: train_x, test_x, train_y, test_y.
    verbose: bool
        Print performance info (default = True)

    Returns
    -------
    tuple:
        Accuracy, sensitivity, specificity, auc on test set.

    """
    train_x, test_x, train_y, test_y = splits

    # Fit the model
    ml_model.fit(train_x, train_y)

    # Calculate model performance results
    accuracy, sens, spec, auc = model_performance(ml_model, test_x, test_y, verbose)

    return accuracy, sens, spec, auc
189/112:
def crossvalidation(ml_model, df, n_folds=5, verbose=False):
    """
    Machine learning model training and validation in a cross-validation loop.

    Parameters
    ----------
    ml_model: sklearn model object
        The machine learning model to train.
    df: pd.DataFrame
        Data set with SMILES and their associated activity labels.
    n_folds: int, optional
        Number of folds for cross-validation.
    verbose: bool, optional
        Performance measures are printed.

    Returns
    -------
    None

    """
    t0 = time.time()
    # Shuffle the indices for the k-fold cross-validation
    kf = KFold(n_splits=n_folds, shuffle=True, random_state=SEED)

    # Results for each of the cross-validation folds
    acc_per_fold = []
    sens_per_fold = []
    spec_per_fold = []
    auc_per_fold = []

    # Loop over the folds
    for train_index, test_index in kf.split(df):
        # clone model -- we want a fresh copy per fold!
        fold_model = clone(ml_model)
        # Training

        # Convert the fingerprint and the label to a list
        train_x = df.iloc[train_index].fp.tolist()
        train_y = df.iloc[train_index].active.tolist()

        # Fit the model
        fold_model.fit(train_x, train_y)

        # Testing

        # Convert the fingerprint and the label to a list
        test_x = df.iloc[test_index].fp.tolist()
        test_y = df.iloc[test_index].active.tolist()

        # Performance for each fold
        accuracy, sens, spec, auc = model_performance(fold_model, test_x, test_y, verbose)

        # Save results
        acc_per_fold.append(accuracy)
        sens_per_fold.append(sens)
        spec_per_fold.append(spec)
        auc_per_fold.append(auc)

    # Print statistics of results
    print(
        f"Mean accuracy: {np.mean(acc_per_fold):.2f} \t"
        f"and std : {np.std(acc_per_fold):.2f} \n"
        f"Mean sensitivity: {np.mean(sens_per_fold):.2f} \t"
        f"and std : {np.std(sens_per_fold):.2f} \n"
        f"Mean specificity: {np.mean(spec_per_fold):.2f} \t"
        f"and std : {np.std(spec_per_fold):.2f} \n"
        f"Mean AUC: {np.mean(auc_per_fold):.2f} \t"
        f"and std : {np.std(auc_per_fold):.2f} \n"
        f"Time taken : {time.time() - t0:.2f}s\n"
    )

    return acc_per_fold, sens_per_fold, spec_per_fold, auc_per_fold
189/113:
# Set model parameter for random forest
param = {
    "n_estimators": 100,  # number of trees to grows
    "criterion": "entropy",  # cost function to be optimized for a split
}
model_RF = RandomForestClassifier(**param)
189/114:
# Fit model on single split
performance_measures = model_training_and_validation(model_RF, "RF", splits)
189/115:
fingerprint_to_model = compound_df.fp.tolist()
label_to_model = compound_df.LogKpu.tolist()
(
    static_train_x,
    static_test_x,
    static_train_y,
    static_test_y,
) = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
splits = [static_train_x, static_test_x, static_train_y, static_test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(static_train_x))
print("Test data size:", len(static_test_x))
189/116:
from sklearn.linear_model import LinearRegression
reg = LinearRegression()
189/117: np.any(np.isnan(fingerprint_to_model))
189/118: np.any(np.isnan(label_to_model))
189/119:
from sklearn.model_selection import cross_val_score, cross_val_predict
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
scores=cross_val_score(reg, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/120:
def model_performance(ml_model, test_x, test_y, verbose=True):
    """
    Helper function to calculate model performance

    Parameters
    ----------
    ml_model: sklearn model object
        The machine learning model to train.
    test_x: list
        Molecular fingerprints for test set.
    test_y: list
        Associated activity labels for test set.
    verbose: bool
        Print performance measure (default = True)

    Returns
    -------
    tuple:
        Accuracy, sensitivity, specificity, auc on test set.
    """

    # Prediction probability on test set
    test_prob = ml_model.predict_proba(test_x)[:, 1]

    # Prediction class on test set
    test_pred = ml_model.predict(test_x)

    # Performance of model on test set
    accuracy = accuracy_score(test_y, test_pred)
    sens = recall_score(test_y, test_pred)
    spec = recall_score(test_y, test_pred, pos_label=0)
    auc = roc_auc_score(test_y, test_prob)

    if verbose:
        # Print performance results
        # NBVAL_CHECK_OUTPUT        print(f"Accuracy: {accuracy:.2}")
        print(f"Sensitivity: {sens:.2f}")
        print(f"Specificity: {spec:.2f}")
        print(f"AUC: {auc:.2f}")

    return accuracy, sens, spec, auc
189/121:
def model_training_and_validation(ml_model, name, splits, verbose=True):
    """
    Fit a machine learning model on a random train-test split of the data
    and return the performance measures.

    Parameters
    ----------
    ml_model: sklearn model object
        The machine learning model to train.
    name: str
        Name of machine learning algorithm: RF, SVM, ANN
    splits: list
        List of desciptor and label data: train_x, test_x, train_y, test_y.
    verbose: bool
        Print performance info (default = True)

    Returns
    -------
    tuple:
        Accuracy, sensitivity, specificity, auc on test set.

    """
    train_x, test_x, train_y, test_y = splits

    # Fit the model
    ml_model.fit(train_x, train_y)

    # Calculate model performance results
    accuracy, sens, spec, auc = model_performance(ml_model, test_x, test_y, verbose)

    return accuracy, sens, spec, auc
189/122:
# Set model parameter for random forest
param = {
    "n_estimators": 100,  # number of trees to grows
    "criterion": "entropy",  # cost function to be optimized for a split
}
model_RF = RandomForestClassifier(**param)
189/123:
# Fit model on single split
performance_measures = model_training_and_validation(model_RF, "RF", splits)
189/124:
def model_performance(ml_model, test_x, test_y, verbose=True):
     # Prediction probability on test set
    test_prob = ml_model.predict_proba(test_x)[:, 1]

    # Prediction class on test set
    test_pred = ml_model.predict(test_x)

    # Performance of model on test set
    accuracy = accuracy_score(test_y, test_pred)
    sens = recall_score(test_y, test_pred)
    spec = recall_score(test_y, test_pred, pos_label=0)
    auc = roc_auc_score(test_y, test_prob)

    if verbose:
        # Print performance results
        # NBVAL_CHECK_OUTPUT        print(f"Accuracy: {accuracy:.2}")
        print(f"Sensitivity: {sens:.2f}")
        print(f"Specificity: {spec:.2f}")
        print(f"AUC: {auc:.2f}")

    return accuracy, sens, spec, auc
189/125:
def model_training_and_validation(ml_model, name, splits, verbose=True):
    train_x, test_x, train_y, test_y = splits

    # Fit the model
    ml_model.fit(train_x, train_y)

    # Calculate model performance results
    accuracy, sens, spec, auc = model_performance(ml_model, test_x, test_y, verbose)

    return accuracy, sens, spec, auc
189/126:
def crossvalidation(ml_model, df, n_folds=5, verbose=False):
    t0 = time.time()
    # Shuffle the indices for the k-fold cross-validation
    kf = KFold(n_splits=n_folds, shuffle=True, random_state=SEED)

    # Results for each of the cross-validation folds
    acc_per_fold = []
    sens_per_fold = []
    spec_per_fold = []
    auc_per_fold = []

    # Loop over the folds
    for train_index, test_index in kf.split(df):
        # clone model -- we want a fresh copy per fold!
        fold_model = clone(ml_model)
        # Training

        # Convert the fingerprint and the label to a list
        train_x = df.iloc[train_index].fp.tolist()
        train_y = df.iloc[train_index].active.tolist()

        # Fit the model
        fold_model.fit(train_x, train_y)

        # Testing

        # Convert the fingerprint and the label to a list
        test_x = df.iloc[test_index].fp.tolist()
        test_y = df.iloc[test_index].active.tolist()

        # Performance for each fold
        accuracy, sens, spec, auc = model_performance(fold_model, test_x, test_y, verbose)

        # Save results
        acc_per_fold.append(accuracy)
        sens_per_fold.append(sens)
        spec_per_fold.append(spec)
        auc_per_fold.append(auc)

    # Print statistics of results
    print(
        f"Mean accuracy: {np.mean(acc_per_fold):.2f} \t"
        f"and std : {np.std(acc_per_fold):.2f} \n"
        f"Mean sensitivity: {np.mean(sens_per_fold):.2f} \t"
        f"and std : {np.std(sens_per_fold):.2f} \n"
        f"Mean specificity: {np.mean(spec_per_fold):.2f} \t"
        f"and std : {np.std(spec_per_fold):.2f} \n"
        f"Mean AUC: {np.mean(auc_per_fold):.2f} \t"
        f"and std : {np.std(auc_per_fold):.2f} \n"
        f"Time taken : {time.time() - t0:.2f}s\n"
    )

    return acc_per_fold, sens_per_fold, spec_per_fold, auc_per_fold
189/127:
# Set model parameter for random forest
param = {
    "n_estimators": 100,  # number of trees to grows
    "criterion": "entropy",  # cost function to be optimized for a split
}
model_RF = RandomForestClassifier(**param)
189/128:
# Fit model on single split
performance_measures = model_training_and_validation(model_RF, "RF", splits)
189/129:
# Fit model on single split
performance_measures = model_training_and_validation(model_RF, splits)
189/130:
# Fit model on single split
performance_measures = model_training_and_validation(model_RF, 'rf', splits, verbose=True)
189/131:
#import important library
import numpy as np
import pandas as pd
from sklearn import svm, metrics, clone
from sklearn.ensemble import RandomForestRegressor
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import KFold, train_test_split
from sklearn.metrics import auc, accuracy_score, recall_score
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt
from rdkit import Chem
from rdkit.Chem import MACCSkeys
from rdkit.Chem.AllChem import GetMorganFingerprintAsBitVect
189/132:
# Set model parameter for random forest regression
param = {
    "n_estimators": 100,  # number of trees to grows
    "criterion": "entropy",  # cost function to be optimized for a split
}
model_RF = RandomForestRegressorR(**param)
189/133:
# Set model parameter for random forest regression
param = {
    "n_estimators": 100,  # number of trees to grows
    "criterion": "entropy",  # cost function to be optimized for a split
}
model_RF = RandomForestRegressor(**param)
189/134:
# Fit model on single split
performance_measures = model_training_and_validation(model_RF, 'rf', splits, verbose=True)
189/135:
# Set model parameter for random forest regression
param = {
    "n_estimators": 100,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
189/136:
# Fit model on single split
performance_measures = model_training_and_validation(model_RF, 'rf', splits, verbose=True)
189/137: N_FOLDS = 5
189/138:
for model in models:
    print("\n======= ")
    print(f"{model['label']}")
    crossvalidation(model["model"], compound_df, n_folds=N_FOLDS)
189/139:
# Initialize the list that stores all models. First one is RF.
models = [{"label": "Model_RF", "model": model_RF}]
189/140:
for model in models:
    print("\n======= ")
    print(f"{model['label']}")
    crossvalidation(model["model"], compound_df, n_folds=N_FOLDS)
189/141:
def crossvalidation(ml_model, df, n_folds=5, verbose=False):
    # Shuffle the indices for the k-fold cross-validation
    kf = KFold(n_splits=n_folds, shuffle=True, random_state=SEED)

    # Results for each of the cross-validation folds
    acc_per_fold = []
    sens_per_fold = []
    spec_per_fold = []
    auc_per_fold = []

    # Loop over the folds
    for train_index, test_index in kf.split(df):
        # clone model -- we want a fresh copy per fold!
        fold_model = clone(ml_model)
        # Training

        # Convert the fingerprint and the label to a list
        train_x = df.iloc[train_index].fp.tolist()
        train_y = df.iloc[train_index].active.tolist()

        # Fit the model
        fold_model.fit(train_x, train_y)

        # Testing

        # Convert the fingerprint and the label to a list
        test_x = df.iloc[test_index].fp.tolist()
        test_y = df.iloc[test_index].active.tolist()

        # Performance for each fold
        accuracy, sens, spec, auc = model_performance(fold_model, test_x, test_y, verbose)

        # Save results
        acc_per_fold.append(accuracy)
        sens_per_fold.append(sens)
        spec_per_fold.append(spec)
        auc_per_fold.append(auc)

    # Print statistics of results
    print(
        f"Mean accuracy: {np.mean(acc_per_fold):.2f} \t"
        f"and std : {np.std(acc_per_fold):.2f} \n"
        f"Mean sensitivity: {np.mean(sens_per_fold):.2f} \t"
        f"and std : {np.std(sens_per_fold):.2f} \n"
        f"Mean specificity: {np.mean(spec_per_fold):.2f} \t"
        f"and std : {np.std(spec_per_fold):.2f} \n"
        f"Mean AUC: {np.mean(auc_per_fold):.2f} \t"
        f"and std : {np.std(auc_per_fold):.2f} \n"
        f"Time taken : {time.time() - t0:.2f}s\n"
    )

    return acc_per_fold, sens_per_fold, spec_per_fold, auc_per_fold
189/142:
# Set model parameter for random forest regression
param = {
    "n_estimators": 100,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
189/143: N_FOLDS = 5
189/144:
# Initialize the list that stores all models. First one is RF.
models = [{"label": "Model_RF", "model": model_RF}]
189/145:
for model in models:
    print("\n======= ")
    print(f"{model['label']}")
    crossvalidation(model["model"], compound_df, n_folds=N_FOLDS)
189/146:
def crossvalidation(ml_model, df, n_folds=5, verbose=False):
    # Shuffle the indices for the k-fold cross-validation
    kf = KFold(n_splits=n_folds, shuffle=True, random_state=SEED)

    # Results for each of the cross-validation folds
    acc_per_fold = []
    sens_per_fold = []
    spec_per_fold = []
    auc_per_fold = []

    # Loop over the folds
    for train_index, test_index in kf.split(df):
        # clone model -- we want a fresh copy per fold!
        fold_model = clone(ml_model)
        # Training

        # Convert the fingerprint and the label to a list
        train_x = df.iloc[train_index].fp.tolist()
        train_y = df.iloc[train_index].active.tolist()

        # Fit the model
        fold_model.fit(train_x, train_y)

        # Testing

        # Convert the fingerprint and the label to a list
        test_x = df.iloc[test_index].fp.tolist()
        test_y = df.iloc[test_index].LogKpu.tolist()

        # Performance for each fold
        accuracy, sens, spec, auc = model_performance(fold_model, test_x, test_y, verbose)

        # Save results
        acc_per_fold.append(accuracy)
        sens_per_fold.append(sens)
        spec_per_fold.append(spec)
        auc_per_fold.append(auc)

    # Print statistics of results
    print(
        f"Mean accuracy: {np.mean(acc_per_fold):.2f} \t"
        f"and std : {np.std(acc_per_fold):.2f} \n"
        f"Mean sensitivity: {np.mean(sens_per_fold):.2f} \t"
        f"and std : {np.std(sens_per_fold):.2f} \n"
        f"Mean specificity: {np.mean(spec_per_fold):.2f} \t"
        f"and std : {np.std(spec_per_fold):.2f} \n"
        f"Mean AUC: {np.mean(auc_per_fold):.2f} \t"
        f"and std : {np.std(auc_per_fold):.2f} \n"
        f"Time taken : {time.time() - t0:.2f}s\n"
    )

    return acc_per_fold, sens_per_fold, spec_per_fold, auc_per_fold
189/147:
# Set model parameter for random forest regression
param = {
    "n_estimators": 100,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
189/148: N_FOLDS = 5
189/149:
# Initialize the list that stores all models. First one is RF.
models = [{"label": "Model_RF", "model": model_RF}]
189/150:
for model in models:
    print("\n======= ")
    print(f"{model['label']}")
    crossvalidation(model["model"], compound_df, n_folds=N_FOLDS)
189/151:
def crossvalidation(ml_model, df, n_folds=5, verbose=False):
    # Shuffle the indices for the k-fold cross-validation
    kf = KFold(n_splits=n_folds, shuffle=True, random_state=SEED)

    # Results for each of the cross-validation folds
    acc_per_fold = []
    sens_per_fold = []
    spec_per_fold = []
    auc_per_fold = []

    # Loop over the folds
    for train_index, test_index in kf.split(df):
        # clone model -- we want a fresh copy per fold!
        fold_model = clone(ml_model)
        # Training

        # Convert the fingerprint and the label to a list
        train_x = df.iloc[train_index].fp.tolist()
        train_y = df.iloc[train_index].LogKpu.tolist()

        # Fit the model
        fold_model.fit(train_x, train_y)

        # Testing

        # Convert the fingerprint and the label to a list
        test_x = df.iloc[test_index].fp.tolist()
        test_y = df.iloc[test_index].LogKpu.tolist()

        # Performance for each fold
        accuracy, sens, spec, auc = model_performance(fold_model, test_x, test_y, verbose)

        # Save results
        acc_per_fold.append(accuracy)
        sens_per_fold.append(sens)
        spec_per_fold.append(spec)
        auc_per_fold.append(auc)

    # Print statistics of results
    print(
        f"Mean accuracy: {np.mean(acc_per_fold):.2f} \t"
        f"and std : {np.std(acc_per_fold):.2f} \n"
        f"Mean sensitivity: {np.mean(sens_per_fold):.2f} \t"
        f"and std : {np.std(sens_per_fold):.2f} \n"
        f"Mean specificity: {np.mean(spec_per_fold):.2f} \t"
        f"and std : {np.std(spec_per_fold):.2f} \n"
        f"Mean AUC: {np.mean(auc_per_fold):.2f} \t"
        f"and std : {np.std(auc_per_fold):.2f} \n"
        f"Time taken : {time.time() - t0:.2f}s\n"
    )

    return acc_per_fold, sens_per_fold, spec_per_fold, auc_per_fold
189/152:
# Set model parameter for random forest regression
param = {
    "n_estimators": 100,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
189/153: N_FOLDS = 5
189/154:
# Initialize the list that stores all models. First one is RF.
models = [{"label": "Model_RF", "model": model_RF}]
189/155:
for model in models:
    print("\n======= ")
    print(f"{model['label']}")
    crossvalidation(model["model"], compound_df, n_folds=N_FOLDS)
189/156:
# Set model parameter for random forest regression
param = {
    "n_estimators": 100,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/157:
scores=cross_val_score(reg, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/158:
def crossvalidation(ml_model, df, n_folds=5, verbose=False):
    # Shuffle the indices for the k-fold cross-validation
    kf = KFold(n_splits=n_folds, shuffle=True, random_state=SEED)

    # Results for each of the cross-validation folds
    acc_per_fold = []
    sens_per_fold = []
    spec_per_fold = []
    auc_per_fold = []

    # Loop over the folds
    for train_index, test_index in kf.split(df):
        # clone model -- we want a fresh copy per fold!
        fold_model = clone(ml_model)
        # Training

        # Convert the fingerprint and the label to a list
        train_x = df.iloc[train_index].fp.tolist()
        train_y = df.iloc[train_index].LogKpu.tolist()

        # Fit the model
        fold_model.fit(train_x, train_y)

        # Testing

        # Convert the fingerprint and the label to a list
        test_x = df.iloc[test_index].fp.tolist()
        test_y = df.iloc[test_index].LogKpu.tolist()

        # Performance for each fold
        accuracy, sens, spec, auc = model_performance(fold_model, test_x, test_y, verbose)

        # Save results
        acc_per_fold.append(accuracy)
        sens_per_fold.append(sens)
        spec_per_fold.append(spec)
        auc_per_fold.append(auc)

    # Print statistics of results
    print(
        f"Mean accuracy: {np.mean(acc_per_fold):.2f} \t"
        f"and std : {np.std(acc_per_fold):.2f} \n"
        f"Mean sensitivity: {np.mean(sens_per_fold):.2f} \t"
        f"and std : {np.std(sens_per_fold):.2f} \n"
        f"Mean specificity: {np.mean(spec_per_fold):.2f} \t"
        f"and std : {np.std(spec_per_fold):.2f} \n"
        f"Mean AUC: {np.mean(auc_per_fold):.2f} \t"
        f"and std : {np.std(auc_per_fold):.2f} \n"
        f"Time taken : {time.time() - t0:.2f}s\n"
    )

    return acc_per_fold, sens_per_fold, spec_per_fold, auc_per_fold
189/159:
# Set model parameter for random forest regression
param = {
    "n_estimators": 100,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/160: N_FOLDS = 5
189/161:
# Initialize the list that stores all models. First one is RF.
models = [{"label": "Model_RF", "model": model_RF}]
189/162: from sklearn.cross_decomposition import PLSRegression
189/163: model_PLS = PLSRegression(n_components=10)
189/164:
scores=cross_val_score(model_PLS, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/165:
# Set model parameter for random forest regression
param = {
    "n_estimators": 200,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/166:
# Set model parameter for random forest regression
param = {
    "n_estimators": 300,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/167:
# Set model parameter for random forest regression
param = {
    "n_estimators": 1000,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/168:
# Set model parameter for random forest regression
param = {
    "n_estimators": 100,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/169:
# Set model parameter for random forest regression
param = {
    "n_estimators": 200,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/170:
from sklearn.linear_model import Ridge
Ridge = Ridge(alpha=1.0)
189/171:
scores=cross_val_score(Ridge, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/172:
scores=cross_val_score(Ridge, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/173:
scores=cross_val_score(Ridge, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/174:
scores=cross_val_score(Ridge, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/175:
from sklearn.neural_network import MLPRegressor
regr = MLPRegressor(random_state=1, max_iter=500, solver='lbfgs')
scores=cross_val_score(model_PLS, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/176:
y_pred = cross_val_predict(regr, train_x, train_y cv=kf)
r2_score(y, y_pred)
189/177:
y_pred = cross_val_predict(regr, train_x, train_y, cv=kf)
r2_score(y, y_pred)
189/178:
from sklearn.metrics import r2_score
y_pred = cross_val_predict(regr, train_x, train_y, cv=kf)
r2_score(y, y_pred)
189/179:
from sklearn.metrics import r2_score
y_pred = cross_val_predict(regr, train_x, train_y, cv=kf)
r2_score(train_y, y_pred)
189/180:
from sklearn.svm import SVR
SVR = SVR(kernel='linear')
189/181:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/182:
from sklearn.svm import SVR
SVR = SVR(kernel='poly')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/183:
from sklearn.svm import SVR
SVR = SVR(kernel='sigmoid')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/184:
from sklearn.svm import SVR
SVR = SVR(kernel='linear')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/185:
from sklearn.svm import SVR
SVR = SVR(kernel='precomputed')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/186:
# Set model parameter for random forest regression
param = {
    "n_estimators": 3000,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/187:
compound_df = df2.copy()
# Add column for fingerprint
compound_df["fp"] = compound_df["smiles"].apply(smiles_to_fp, args=("morgan3",))
compound_df.head(3)
# NBVAL_CHECK_OUTPUT
189/188: compound_df.shape
189/189:
fingerprint_to_model = compound_df.fp.tolist()
label_to_model = compound_df.LogKpu.tolist()
(
    static_train_x,
    static_test_x,
    static_train_y,
    static_test_y,
) = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
splits = [static_train_x, static_test_x, static_train_y, static_test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(static_train_x))
print("Test data size:", len(static_test_x))
189/190:
# Set model parameter for random forest regression
param = {
    "n_estimators": 3000,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/191:
fingerprint_to_model = compound_df.fp.tolist()
label_to_model = compound_df.LogKpu.tolist()
(
    train_x,
    test_x,
    train_y,
    test_y,
) = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(static_train_x))
print("Test data size:", len(static_test_x))
189/192:
# Set model parameter for random forest regression
param = {
    "n_estimators": 3000,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/193:
# Set model parameter for random forest regression
param = {
    "n_estimators": 1000,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/194:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/195: #Or we should move to mordard fingerprint and calculate with prediction
187/1:

raw13.to_csv('QSAR_Kpu/Mordard.csv' , sep=',' ,index=True)
187/2:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
187/3:
path = r'Fingerprint/'

Fp1  = pd.read_csv(path + 'Fingerprinter.csv'               , header = 0)
Fp2  = pd.read_csv(path + 'ExtendedFingerprinter.csv'       , header = 0)
Fp3  = pd.read_csv(path + 'EStateFingerprinter.csv'         , header = 0)
Fp4  = pd.read_csv(path + 'GraphOnlyFingerprinter.csv'      , header = 0)
Fp5  = pd.read_csv(path + 'MACCSFingerprinter.csv'          , header = 0)
Fp6  = pd.read_csv(path + 'PubchemFingerprinter.csv'        , header = 0)
Fp7  = pd.read_csv(path + 'SubstructureFingerprinter.csv'   , header = 0)
Fp8  = pd.read_csv(path + 'SubstructureFingerprintCount.csv', header = 0)
Fp9  = pd.read_csv(path + 'KlekotaRothFingerprinter.csv'    , header = 0)
Fp10 = pd.read_csv(path + 'KlekotaRothFingerprintCount.csv' , header = 0)
Fp11 = pd.read_csv(path + 'AtomPairs2DFingerprinter.csv'    , header = 0)
Fp12 = pd.read_csv(path + 'AtomPairs2DFingerprintCount.csv' , header = 0)
Fp13  = pd.read_csv(path + 'mordard.csv'               , header = 0)
187/4: Fp1.shape
187/5:
df = pd.read_csv('df6_lipinski_kpu.csv')
df
187/6: name = df['compound']
187/7: name.shape
187/8: name = df['compound']
187/9: name.shape
187/10:
df2 = df.rename({'compound': 'Name'}, axis=1)
df2 = df2.drop('Unnamed: 0', axis=1)
df3 = df2.set_index('Name')
df3
187/11:
LogKpu = df3[['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]
LogKpu
187/12:
Fp13 = Fp13.rename(columns={'name': 'Name'})
Fp13
187/13:
Fp1.Name = name
Fp2.Name = name
Fp3.Name = name
Fp4.Name = name
Fp5.Name = name
Fp6.Name = name
Fp7.Name = name
Fp8.Name = name
Fp9.Name = name
Fp10.Name = name
Fp11.Name = name
Fp12.Name = name
Fp13.Name = name
187/14:
Fpp1 = Fp1.drop('Name', axis=1)
Fpp2 = Fp2.drop('Name', axis=1)
Fpp3 = Fp3.drop('Name', axis=1)
Fpp4 = Fp4.drop('Name', axis=1)
Fpp5 = Fp5.drop('Name', axis=1)
Fpp6 = Fp6.drop('Name', axis=1)
Fpp7 = Fp7.drop('Name', axis=1)
Fpp8 = Fp8.drop('Name', axis=1)
Fpp9 = Fp9.drop('Name', axis=1)
Fpp10 = Fp10.drop('Name', axis=1)
Fpp11 = Fp11.drop('Name', axis=1)
Fpp12 = Fp12.drop('Name', axis=1)
Fpp13 = Fp13.drop('Name', axis=1)
187/15:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
187/16:
Fp1_n  = normalized (Fpp1 )
Fp2_n  = normalized (Fpp2 )
Fp3_n  = normalized (Fpp3 )
Fp4_n  = normalized (Fpp4 )
Fp5_n  = normalized (Fpp5 )
Fp6_n  = normalized (Fpp6 )
Fp7_n  = normalized (Fpp7 )
Fp8_n  = normalized (Fpp8 )
Fp9_n  = normalized (Fpp9 )
Fp10_n = normalized (Fpp10)
Fp11_n = normalized (Fpp11)
Fp12_n = normalized (Fpp12)
Fp13_n = normalized (Fpp13)
187/17:
path = r'Fingerprint/'

Fp13  = pd.read_csv(path + 'mordard.csv'               , header = 0)
187/18: Fp1.shape
187/19:
df = pd.read_csv('df6_lipinski_kpu.csv')
df
187/20: name = df['compound']
187/21: name.shape
187/22:
df2 = df.rename({'compound': 'Name'}, axis=1)
df2 = df2.drop('Unnamed: 0', axis=1)
df3 = df2.set_index('Name')
df3
187/23:
LogKpu = df3[['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]
LogKpu
187/24:
Fp13 = Fp13.rename(columns={'name': 'Name'})
Fp13
187/25:

Fp13.Name = name
187/26:

Fpp13 = Fp13.drop('Name', axis=1)
187/27:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
187/28:

Fp13_n = normalized (Fpp13)
187/29:

Fpp13 = Fp13.drop('Name', axis=1)
Fpp13
187/30: Fpp13.fillna(value='0', method='ffill')
187/31: Fpp13.fillna(value=0, method='ffill')
187/32: Fpp13 = Fpp13.fillna(0)
187/33:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
187/34:

Fp13_n = normalized (Fpp13)
187/35: Fp13_n
187/36: np.any(np.isnan(Fp13_n))
187/37: Fp13_n["Name"] = Fp13.Name
187/38: Fp1_n
187/39: Fp13_n = Fp13_n.set_index('Name')
187/40: Fp1_n
187/41: Fp13_n
187/42: Fp13_n.to_csv('Fp_normalized/Morderd.csv' , sep=',' ,index=True)
187/43: raw13  = LogKpu.merge(Fp13_n, on='Name', how='outer')
187/44: raw13
187/45: raw13.isna()
187/46:

raw13.to_csv('QSAR_Kpu/Mordard.csv' , sep=',' ,index=True)
187/47: print (len(raw13))
187/48: print (len(raw13 .columns)
187/49: print (len(raw13 .columns))
187/50:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
X = sel.fit_transform(X)
X.shape
187/51: compound_df = raw13.copy()
187/52:
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['LogKp'], axis=1)
label_to_model = compound_df.LogKpu.tolist()
187/53:
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['LogKpu'], axis=1)
label_to_model = compound_df.LogKpu.tolist()
187/54: compound_df.head(3)
187/55:
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.LogKpu.tolist()
187/56: compound_df.head(3)
187/57: fingerprint_to_model.head(3)
187/58:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
fingerprint_to_model = sel.fit_transform(X)
label_to_model.shape
187/59:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model.shape
187/60:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model.shape
187/61:
from sklearn.model_selection import KFold, cross_val_score, cross_val_predict
kf =KFold(n_splits=10, shuffle=False)
187/62:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
187/63:
(
    train_x,
    test_x,
    train_y,
    test_y,
) = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(static_train_x))
print("Test data size:", len(static_test_x))
187/64:
    train_x,
    test_x,
    train_y,
    test_y,
= train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(static_train_x))
print("Test data size:", len(static_test_x))
187/65:
    train_x,
    test_x,
    train_y,
    test_y,
    = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(static_train_x))
print("Test data size:", len(static_test_x))
187/66:
train_x,test_x,train_y,test_y= train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(static_train_x))
print("Test data size:", len(static_test_x))
187/67:
seed = random.seed(10)
seed
187/68:
import random
seed = random.seed(10)
seed
187/69:
train_x,test_x,train_y,test_y= train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(static_train_x))
print("Test data size:", len(static_test_x))
187/70:
from sklearn.model_selection import cross_val_score, cross_val_predict
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
187/71:
import random
SEED = random.seed(10)
seed
187/72:
train_x,test_x,train_y,test_y= train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(static_train_x))
print("Test data size:", len(static_test_x))
187/73:
from sklearn.model_selection import cross_val_score, cross_val_predict
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
187/74:
train_x,test_x,train_y,test_y= train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
187/75:
from sklearn.model_selection import cross_val_score, cross_val_predict
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
187/76:
# Set model parameter for random forest regression
param = {
    "n_estimators": 3000,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
187/77:
train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
187/78:
from sklearn.model_selection import cross_val_score, cross_val_predict
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
187/79:
# Set model parameter for random forest regression
param = {
    "n_estimators": 3000,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
187/80: np.any(np.isnan(raw13))
187/81: raw13 =raw13[raw13.LogKpu.notna()]
187/82:

raw13.to_csv('QSAR_Kpu/Mordard.csv' , sep=',' ,index=True)
187/83: print (len(raw13))
187/84: print (len(raw13 .columns))
187/85:
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.LogKpu.tolist()
187/86: fingerprint_to_model.head(3)
187/87:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model.shape
187/88:
import random
SEED = random.seed(10)
seed
187/89:
train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
187/90:
from sklearn.model_selection import cross_val_score, cross_val_predict
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
187/91:
# Set model parameter for random forest regression
param = {
    "n_estimators": 3000,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
187/92:
from sklearn.cross_decomposition import PLSRegression
model_PLS = PLSRegression(n_components=10)
scores=cross_val_score(model_PLS, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
187/93: fingerprint_to_model.head(3)
187/94:
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.LogKpu.tolist()
187/95: fingerprint_to_model.head(3)
187/96:
import random
SEED = random.seed(10)
seed
187/97:
#not reduce
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.LogKpu.tolist()
187/98:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model.shape
187/99:
import random
SEED = random.seed(10)
seed
187/100:
train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
187/101:
from sklearn.model_selection import cross_val_score, cross_val_predict
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
187/102:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
187/103:
#not reduce
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.LogKpu.tolist()
187/104: fingerprint_to_model.shape
187/105:
train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
187/106: train_x.shape
187/107:
# Set model parameter for random forest regression
param = {
    "n_estimators": 1000,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/196:
from sklearn.neural_network import MLPRegressor
regr = MLPRegressor(random_state=1, max_iter=500, solver='lbfgs')
scores=cross_val_score(model_regr, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
189/197:
from sklearn.neural_network import MLPRegressor
regr = MLPRegressor(random_state=1, max_iter=500, solver='lbfgs')
scores=cross_val_score(regr, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
197/2:
path = r'Fingerprint/'

Fp13  = pd.read_csv(path + 'mordard.csv'               , header = 0)
197/3: Fp1.shape
197/4:
df = pd.read_csv('df6_lipinski_kpu.csv')
df
197/5: Fp13.shape
197/6:
df = pd.read_csv('df6_lipinski_kpu.csv')
df
197/7: name = df['compound']
197/8: name.shape
197/9:
df2 = df.rename({'compound': 'Name'}, axis=1)
df2 = df2.drop('Unnamed: 0', axis=1)
df3 = df2.set_index('Name')
df3
197/10:
LogKpu = df3[['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]
LogKpu
197/11:
Fp13 = Fp13.rename(columns={'name': 'Name'})
Fp13
197/12:

Fp13.Name = name
197/13:

Fpp13 = Fp13.drop('Name', axis=1)
Fpp13
197/14: Fpp13 = Fpp13.fillna(0)
197/15:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
197/16:

Fp13_n = normalized (Fpp13)
197/17: Fp13_n
197/18: np.any(np.isnan(Fp13_n))
197/19: Fp13_n["Name"] = Fp13.Name
197/20: Fp1_n
197/21: Fp13_n
197/22: Fp13_n = Fp13_n.set_index('Name')
197/23: Fp13_n
197/24: raw13  = LogKpu.merge(Fp13_n, on='Name', how='outer')
197/25: raw13
197/26: raw13 =raw13[raw13.LogKpu.notna()]
197/27:
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.LogKpu.tolist()
197/28: fingerprint_to_model.head(3)
197/29:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model.shape
197/30:
import random
SEED = random.seed(10)
seed
197/31:
train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
197/32:
import random
SEED = random.seed(10)
SEED
197/33:
train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
197/34:
from sklearn.model_selection import cross_val_score, cross_val_predict
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
197/35:
from sklearn.model_selection import cross_val_score, cross_val_predict
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
197/36:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
197/37:
# Set model parameter for random forest regression
param = {
    "n_estimators": 1000,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/38:
from sklearn.cross_decomposition import PLSRegression
model_PLS = PLSRegression(n_components=10)
scores=cross_val_score(model_PLS, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/39:
from sklearn.metrics import r2_score
#Try cross_val_predict and calculate R2
y_pred = cross_val_predict(RF, X, y, cv=kf)
#Rsqure
r2_score(y, y_pred)
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(y, y_pred)

print('R2 between prediction is', '{:4}'.r2_score)
print('RMSE between prediction is', '{:4}'.RMSE_pred)
197/40:
from sklearn.metrics import r2_score
#Try cross_val_predict and calculate R2
y_pred = cross_val_predict(model_RF, train_x, train_y, cv=kf)
#Rsqure
r2_score(train_y, y_pred)
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(train_y, y_pred)

print('R2 between prediction is', '{:4}'.r2_score)
print('RMSE between prediction is', '{:4}'.RMSE_pred)
197/41:
from sklearn.metrics import r2_score
#Try cross_val_predict and calculate R2
y_pred = cross_val_predict(model_RF, train_x, train_y, cv=kf)
#Rsqure
r2_score(train_y, y_pred)
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(train_y, y_pred)

print(f'R2 between prediction is {r2_score}')
print('RMSE between prediction is', '{:4}'.RMSE_pred)
197/42:
from sklearn.metrics import r2_score
#Try cross_val_predict and calculate R2
y_pred = cross_val_predict(model_RF, train_x, train_y, cv=kf)
#Rsqure
r2_score(train_y, y_pred)
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(train_y, y_pred)

print(f'R2 between prediction is {r2_score}')
print(f'RMSE between prediction is {RMSE_pred}')
197/43:
from sklearn.metrics import r2_score
#Try cross_val_predict and calculate R2
y_pred = cross_val_predict(model_RF, train_x, train_y, cv=kf)
#Rsqure
r2_score = r2_score(train_y, y_pred)
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(train_y, y_pred)

print(f'R2 between prediction is {r2_score}')
print(f'RMSE between prediction is {RMSE_pred}')
197/44:
from sklearn.cross_decomposition import PLSRegression
model_PLS = PLSRegression(n_components=10)
scores=cross_val_score(model_PLS, train_x, train_y, cv=kf)
#Try cross_val_predict and calculate R2
y_pred = cross_val_predict(model_PLS, train_x, train_y, cv=kf)
#Rsqure
r2_score = r2_score(train_y, y_pred)
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(train_y, y_pred)

print(f'R2 between prediction is {r2_score}')
print(f'RMSE between prediction is {RMSE_pred}')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
198/1:
#flowchart
#download data
#create Morgan Fingerprint
#normalized fingerprint
#create test and train set
#create CV of train set
#train CV of train set model
#train using linear regressor
#train using pls
#train using randomforestregressor
#select most most descripted model
#select top 20 features
#predict test set from trained model
198/2:
#import important library
import numpy as np
import pandas as pd
from sklearn import svm, metrics, clone
from sklearn.ensemble import RandomForestRegressor
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import KFold, train_test_split
from sklearn.metrics import auc, accuracy_score, recall_score
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt
from rdkit import Chem
from rdkit.Chem import MACCSkeys
from rdkit.Chem.AllChem import GetMorganFingerprintAsBitVect
198/3:
# Fix seed for reproducible results
import random

seed = random.seed(10)
seed
198/4: seed
198/5: df = pd.read_csv('df6_lipinski_Kpu.csv')
198/6: df.head(6)
198/7: df.shape
198/8:
df1 = df[['compound', 'smiles', 'LogKpu']]
df2 =df1[df1.LogKpu.notna()]
198/9: df2
198/10:
def smiles_to_fp (smiles, method="maccs", n_bits=2048):
     # convert smiles to RDKit mol object
    mol = Chem.MolFromSmiles(smiles)

    if method == "maccs":
        return np.array(MACCSkeys.GenMACCSKeys(mol))
    if method == "morgan2":
        return np.array(GetMorganFingerprintAsBitVect(mol, 2, nBits=n_bits))
    if method == "morgan3":
        return np.array(GetMorganFingerprintAsBitVect(mol, 3, nBits=n_bits))
    else:
        # NBVAL_CHECK_OUTPUT
        print(f"Warning: Wrong method specified: {method}. Default will be used instead.")
        return np.array(MACCSkeys.GenMACCSKeys(mol))
198/11:
compound_df = df2.copy()
# Add column for fingerprint
compound_df["fp"] = compound_df["smiles"].apply(smiles_to_fp)
compound_df.head(3)
# NBVAL_CHECK_OUTPUT
198/12:
def plot_roc_curves_for_models(models, test_x, test_y, save_png=False):
    """
    Helper function to plot customized roc curve.

    Parameters
    ----------
    models: dict
        Dictionary of pretrained machine learning models.
    test_x: list
        Molecular fingerprints for test set.
    test_y: list
        Associated activity labels for test set.
    save_png: bool
        Save image to disk (default = False)

    Returns
    -------
    fig:
        Figure.
    """

    fig, ax = plt.subplots()

    # Below for loop iterates through your models list
    for model in models:
        # Select the model
        ml_model = model["model"]
        # Prediction probability on test set
        test_prob = ml_model.predict_proba(test_x)[:, 1]
        # Prediction class on test set
        test_pred = ml_model.predict(test_x)
        # Compute False postive rate and True positive rate
        fpr, tpr, thresholds = metrics.roc_curve(test_y, test_prob)
        # Calculate Area under the curve to display on the plot
        auc = roc_auc_score(test_y, test_prob)
        # Plot the computed values
        ax.plot(fpr, tpr, label=(f"{model['label']} AUC area = {auc:.2f}"))

    # Custom settings for the plot
    ax.plot([0, 1], [0, 1], "r--")
    ax.set_xlabel("False Positive Rate")
    ax.set_ylabel("True Positive Rate")
    ax.set_title("Receiver Operating Characteristic")
    ax.legend(loc="lower right")
    # Save plot
    if save_png:
        fig.savefig(f"{DATA}/roc_auc", dpi=300, bbox_inches="tight", transparent=True)
    return fig
198/13:
fingerprint_to_model = compound_df.fp.tolist()
label_to_model = compound_df.LogKpu.tolist()
(
    train_x,
    test_x,
    train_y,
    test_y,
) = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(static_train_x))
print("Test data size:", len(static_test_x))
198/14:
fingerprint_to_model = compound_df.fp.tolist()
label_to_model = compound_df.LogKpu.tolist()
    train_x, test_x, train_y, test_y, = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(static_train_x))
print("Test data size:", len(static_test_x))
198/15:
fingerprint_to_model = compound_df.fp.tolist()
label_to_model = compound_df.LogKpu.tolist()
train_x, test_x, train_y, test_y, = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(static_train_x))
print("Test data size:", len(static_test_x))
198/16:
# Fix seed for reproducible results
import random

SEED = random.seed(10)
SEED
198/17:
fingerprint_to_model = compound_df.fp.tolist()
label_to_model = compound_df.LogKpu.tolist()
train_x, test_x, train_y, test_y, = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(static_train_x))
print("Test data size:", len(static_test_x))
198/18:
fingerprint_to_model = compound_df.fp.tolist()
label_to_model = compound_df.LogKpu.tolist()
train_x, test_x, train_y, test_y, = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(static_train_x))
print("Test data size:", len(static_test_x))
198/19:
fingerprint_to_model = compound_df.fp.tolist()
label_to_model = compound_df.LogKpu.tolist()
train_x, test_x, train_y, test_y, = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
197/45:
from sklearn.cross_decomposition import PLSRegression
model_PLS = PLSRegression(n_components=10)
scores=cross_val_score(model_PLS, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/46:
from sklearn.metrics import r2_score
#Try cross_val_predict and calculate R2
y_pred = cross_val_predict(model_PLS, train_x, train_y, cv=kf)
#Rsqure
r2_score = r2_score(train_y, y_pred)
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(train_y, y_pred)

print(f'R2 between prediction is {r2_score}')
print(f'RMSE between prediction is {RMSE_pred}')
197/47:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/48:
from sklearn.metrics import r2_score
#Try cross_val_predict and calculate R2
y_pred = cross_val_predict(SVR, train_x, train_y, cv=kf)
#Rsqure
r2_score = r2_score(train_y, y_pred)
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(train_y, y_pred)

print(f'R2 between prediction is {r2_score}')
print(f'RMSE between prediction is {RMSE_pred}')
197/49:
from sklearn.linear_model import Ridge
Ridge = Ridge(alpha=1.0)
from sklearn.metrics import r2_score
#Try cross_val_predict and calculate R2
y_pred = cross_val_predict(Ridge, train_x, train_y, cv=kf)
#Rsqure
r2_score = r2_score(train_y, y_pred)
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(train_y, y_pred)

print(f'R2 between prediction is {r2_score}')
print(f'RMSE between prediction is {RMSE_pred}')
197/50:
from sklearn.neural_network import MLPRegressor
regr = MLPRegressor(random_state=1, max_iter=500, solver='lbfgs')
scores=cross_val_score(regr, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
from sklearn.metrics import r2_score
#Try cross_val_predict and calculate R2
y_pred = cross_val_predict(regr, train_x, train_y, cv=kf)
#Rsqure
r2_score = r2_score(train_y, y_pred)
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(train_y, y_pred)

print(f'R2 between prediction is {r2_score}')
print(f'RMSE between prediction is {RMSE_pred}')
197/51:
from sklearn.neural_network import MLPRegressor
regr = MLPRegressor(random_state=1, max_iter=500, solver='lbfgs', alpha=1.0)
scores=cross_val_score(regr, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
from sklearn.metrics import r2_score
#Try cross_val_predict and calculate R2
y_pred = cross_val_predict(regr, train_x, train_y, cv=kf)
#Rsqure
r2_score = r2_score(train_y, y_pred)
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(train_y, y_pred)

print(f'R2 between prediction is {r2_score}')
print(f'RMSE between prediction is {RMSE_pred}')
197/52:
from sklearn.neural_network import MLPRegressor
regr = MLPRegressor(random_state=1, max_iter=1000, solver='lbfgs', alpha=1.0)
scores=cross_val_score(regr, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
from sklearn.metrics import r2_score
#Try cross_val_predict and calculate R2
y_pred = cross_val_predict(regr, train_x, train_y, cv=kf)
#Rsqure
r2_score = r2_score(train_y, y_pred)
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(train_y, y_pred)

print(f'R2 between prediction is {r2_score}')
print(f'RMSE between prediction is {RMSE_pred}')
197/53:
from sklearn.neural_network import MLPRegressor
regr = MLPRegressor(random_state=1, max_iter=500, solver='lbfgs', alpha=0.1)
scores=cross_val_score(regr, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
from sklearn.metrics import r2_score
#Try cross_val_predict and calculate R2
y_pred = cross_val_predict(regr, train_x, train_y, cv=kf)
#Rsqure
r2_score = r2_score(train_y, y_pred)
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(train_y, y_pred)

print(f'R2 between prediction is {r2_score}')
print(f'RMSE between prediction is {RMSE_pred}')
197/54:
from sklearn.neural_network import MLPRegressor
regr = MLPRegressor(random_state=1, max_iter=500, solver='lbfgs')
scores=cross_val_score(regr, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
from sklearn.metrics import r2_score
#Try cross_val_predict and calculate R2
y_pred = cross_val_predict(regr, train_x, train_y, cv=kf)
#Rsqure
r2_score = r2_score(train_y, y_pred)
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(train_y, y_pred)

print(f'R2 between prediction is {r2_score}')
print(f'RMSE between prediction is {RMSE_pred}')
197/55:
from sklearn.neural_network import MLPRegressor
regr = MLPRegressor(random_state=1, max_iter=1000, solver='lbfgs', alpha = 0.1)
scores=cross_val_score(regr, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
from sklearn.metrics import r2_score
#Try cross_val_predict and calculate R2
y_pred = cross_val_predict(regr, train_x, train_y, cv=kf)
#Rsqure
r2_score = r2_score(train_y, y_pred)
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(train_y, y_pred)

print(f'R2 between prediction is {r2_score}')
print(f'RMSE between prediction is {RMSE_pred}')
197/56:
from sklearn.neural_network import MLPRegressor
regr = MLPRegressor(random_state=1, max_iter=1000, solver='lbfgs')
scores=cross_val_score(regr, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
from sklearn.metrics import r2_score
#Try cross_val_predict and calculate R2
y_pred = cross_val_predict(regr, train_x, train_y, cv=kf)
#Rsqure
r2_score = r2_score(train_y, y_pred)
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(train_y, y_pred)

print(f'R2 between prediction is {r2_score}')
print(f'RMSE between prediction is {RMSE_pred}')
197/57:
from sklearn.linear_model import LogisticRegression
clf = LogisticRegression(random_state=1, solver='liblinear')
197/58:
from sklearn.linear_model import LogisticRegression
LR = LogisticRegression(random_state=1, solver='liblinear')
scores=cross_val_score(LR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
from sklearn.metrics import r2_score
#Try cross_val_predict and calculate R2
y_pred = cross_val_predict(LR, train_x, train_y, cv=kf)
#Rsqure
r2_score = r2_score(train_y, y_pred)
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(train_y, y_pred)

print(f'R2 between prediction is {r2_score}')
print(f'RMSE between prediction is {RMSE_pred}')
197/59:
from sklearn.linear_model import linear_model
Lasso = linear_model.Lasso()
scores=cross_val_score(Lasso, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
from sklearn.metrics import r2_score
#Try cross_val_predict and calculate R2
y_pred = cross_val_predict(Lasso, train_x, train_y, cv=kf)
#Rsqure
r2_score = r2_score(train_y, y_pred)
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(train_y, y_pred)

print(f'R2 between prediction is {r2_score}')
print(f'RMSE between prediction is {RMSE_pred}')
197/60:
from sklearn import linear_model
Lasso = linear_model.Lasso()
scores=cross_val_score(Lasso, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
from sklearn.metrics import r2_score
#Try cross_val_predict and calculate R2
y_pred = cross_val_predict(Lasso, train_x, train_y, cv=kf)
#Rsqure
r2_score = r2_score(train_y, y_pred)
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(train_y, y_pred)

print(f'R2 between prediction is {r2_score}')
print(f'RMSE between prediction is {RMSE_pred}')
197/61:
from sklearn.linear_model import LassoCV
reg = LassoCV(cv=kf, random_state=0).fit(train_x, train_y)
scores=reg.score(train_x, train_y)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/62:
from sklearn.linear_model import LassoCV
reg = LassoCV(cv=kf, random_state=1).fit(train_x, train_y)
scores=reg.score(train_x, train_y)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/63:
from sklearn.linear_model import LassoCV
reg = LassoCV(cv=kf, random_state=1, tol=0.001).fit(train_x, train_y)
scores=reg.score(train_x, train_y)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/64:
from sklearn.linear_model import LassoCV
reg = LassoCV(cv=kf, random_state=1, tol=0.1).fit(train_x, train_y)
scores=reg.score(train_x, train_y)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/65:
from sklearn.linear_model import LassoCV
reg = LassoCV(cv=kf, random_state=1, tol=0.1).fit(train_x, train_y)
scores=reg.score(train_x, train_y)
print("%0.2f accuracy with a standard deviation of %0.5f" % (scores.mean(), scores.std()))
197/66:
from sklearn.linear_model import LassoCV
reg = LassoCV(cv=10, random_state=1, tol=0.1).fit(train_x, train_y)
scores=reg.score(train_x, train_y)
print("%0.2f accuracy with a standard deviation of %0.5f" % (scores.mean(), scores.std()))
y_pred = cross_val_predict(Lasso, train_x, train_y, cv=kf)
197/67:
from sklearn.linear_model import LassoCV
reg = LassoCV(cv=10, random_state=1, tol=0.1)
scores=cross_val_score(reg, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.5f" % (scores.mean(), scores.std()))
y_pred = cross_val_predict(Lasso, train_x, train_y, cv=kf)
197/68:
from sklearn.linear_model import LassoCV
reg = LassoCV(cv=10, random_state=1, tol=0.1)
scores=cross_val_score(reg, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.5f" % (scores.mean(), scores.std()))
y_pred = cross_val_predict(Lasso, train_x, train_y, cv=kf)
#Rsqure
r2_score = r2_score(train_y, y_pred)
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(train_y, y_pred)

print(f'R2 between prediction is {r2_score}')
print(f'RMSE between prediction is {RMSE_pred}')
197/69:
from sklearn.linear_model import LassoCV
reg = LassoCV(cv=10, random_state=1, tol=0.1)
scores=cross_val_score(reg, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.5f" % (scores.mean(), scores.std()))
y_pred = cross_val_predict(Lasso, train_x, train_y, cv=kf)

#Rsqure
r2_score = r2_score(train_y, y_pred)
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(train_y, y_pred)

print(f'R2 between prediction is {r2_score}')
print(f'RMSE between prediction is {RMSE_pred}')
197/70:
from sklearn.linear_model import LassoCV
reg = LassoCV(cv=10, random_state=1, tol=0.1)
scores=cross_val_score(reg, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.5f" % (scores.mean(), scores.std()))
y_pred = cross_val_predict(Lasso, train_x, train_y, cv=kf)
197/71:
from sklearn.linear_model import LassoCV
reg = LassoCV(cv=10, random_state=1, tol=0.1)
scores=cross_val_score(reg, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.5f" % (scores.mean(), scores.std()))
y_pred = cross_val_predict(Lasso, train_x, train_y, cv=kf)
197/72:
#not reduce
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.LogKpu.tolist()
197/73:
from sklearn.linear_model import LassoCV
reg = LassoCV(cv=10, random_state=1, tol=0.1)
scores=cross_val_score(reg, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.5f" % (scores.mean(), scores.std()))
y_pred = cross_val_predict(Lasso, train_x, train_y, cv=kf)
197/74:
#not reduce
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.LogKpu.tolist()
197/75:
from sklearn.linear_model import LassoCV
reg = LassoCV(cv=10, random_state=1, tol=0.1)
scores=cross_val_score(reg, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.5f" % (scores.mean(), scores.std()))
y_pred = cross_val_predict(Lasso, train_x, train_y, cv=kf)
197/76:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=20)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/77:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=10)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/78:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=5)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/79:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=2)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/80:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=100)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/81:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=20)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/82:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=30)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/83:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=10)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/84:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/85:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=20, shuffle=True, random_state=SEED)
197/86:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/87:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=5, shuffle=True, random_state=SEED)
197/88:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/89:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
197/90:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/91:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/92:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/93:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/94:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/95:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/96:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/97:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/98:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/99:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/100:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/101:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/102:
from sklearn.linear_model import Ridge
Ridge = Ridge(alpha=1.0)
from sklearn.metrics import r2_score
#Try cross_val_predict and calculate R2
y_pred = cross_val_predict(Ridge, train_x, train_y, cv=kf)
#Rsqure
r2_score = r2_score(train_y, y_pred)
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(train_y, y_pred)

print(f'R2 between prediction is {r2_score}')
print(f'RMSE between prediction is {RMSE_pred}')
197/103:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/104:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/105:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/106:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/107:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/108:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/109:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/110:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/111:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/112:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/113:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/114:
from sklearn.svm import SVR
SVR = SVR(kernel='linear')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/115:
from sklearn.svm import SVR
SVR = SVR(kernel='linear')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/116:
from sklearn.linear_model import Ridge
Ridge = Ridge(alpha=1.0)
from sklearn.metrics import r2_score
#Try cross_val_predict and calculate R2
y_pred = cross_val_predict(Ridge, train_x, train_y, cv=kf)
#Rsqure
r2_score = r2_score(train_y, y_pred)
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(train_y, y_pred)

print(f'R2 between prediction is {r2_score}')
print(f'RMSE between prediction is {RMSE_pred}')
197/117:
from sklearn.svm import SVR
SVR = SVR(kernel='linear')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/118:
from sklearn.svm import SVR
SVR = SVR(kernel='linear')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/119:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/120:
#Try cross_val_predict and calculate R2
y_pred = cross_val_predict(SVR, train_x, train_y, cv=kf)
#Rsqure
r2_score = r2_score(train_y, y_pred)
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(train_y, y_pred)

print(f'R2 between prediction is {r2_score}')
print(f'RMSE between prediction is {RMSE_pred}')
197/121:
#Try cross_val_predict and calculate R2
y_pred = cross_val_predict(SVR, train_x, train_y, cv=kf)
#Rsqure
r2_score(train_y, y_pred)
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(train_y, y_pred)

print(f'R2 between prediction is {r2_score}')
print(f'RMSE between prediction is {RMSE_pred}')
197/122:
#Try cross_val_predict and calculate R2
y_pred = cross_val_predict(SVR, train_x, train_y, cv=kf)
197/123:
#Try cross_val_predict and calculate R2
y_pred = cross_val_predict(SVR, train_x, train_y, cv=kf)
r2_score = r2_score(train_y, y_pred)
197/124:
from sklearn.metrics import r2_score
#Try cross_val_predict and calculate R2
y_pred = cross_val_predict(SVR, train_x, train_y, cv=kf)
#Rsqure
r2_score = r2_score(train_y, y_pred)
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(train_y, y_pred)

print(f'R2 between prediction is {r2_score}')
print(f'RMSE between prediction is {RMSE_pred}')
198/20:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model.shape
198/21:
train_x, test_x, train_y, test_y, = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
198/22:
# Set model parameter for random forest regression
param = {
    "n_estimators": 1000,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
198/23:
from sklearn.model_selection import cross_val_score, cross_val_predict
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
198/24:
# Set model parameter for random forest regression
param = {
    "n_estimators": 1000,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
198/25:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
198/26:
# Set model parameter for random forest regression
param = {
    "n_estimators": 1000,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/125: Fp13_n.mid
197/126: Fp13_n.mid()
197/127: Fp13_n.middle()
197/128: Fp13_n.head(20)
197/129:

raw13.to_csv('QSAR_Kpu/Mordard.csv' , sep=',' ,index=True)
197/130:
#not reduce
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.LogKpu.tolist()
197/131: fingerprint_to_model.shape
197/132:
# Set model parameter for random forest regression
param = {
    "n_estimators": 1000,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/133:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
from sklearn.metrics import r2_score
#Try cross_val_predict and calculate R2
y_pred = cross_val_predict(SVR, train_x, train_y, cv=kf)
#Rsqure
r2_score = r2_score(train_y, y_pred)
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(train_y, y_pred)

print(f'R2 between prediction is {r2_score}')
print(f'RMSE between prediction is {RMSE_pred}')
197/134: SVR = SVR.predict(test_y)
197/135: y_predic_external = SVR.predict(test_x)
197/136:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
from sklearn.metrics import r2_score
#Try cross_val_predict and calculate R2
y_pred = cross_val_predict(SVR, train_x, train_y, cv=kf)
#Rsqure
r2_score = r2_score(train_y, y_pred)
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(train_y, y_pred)

print(f'R2 between prediction is {r2_score}')
print(f'RMSE between prediction is {RMSE_pred}')
197/137:
from sklearn.svm import SVR
SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
from sklearn.metrics import r2_score
#Try cross_val_predict and calculate R2
y_pred = cross_val_predict(SVR, train_x, train_y, cv=kf)
#Rsqure
r2_score = r2_score(train_y, y_pred)
from sklearn.metrics import mean_absolute_error
RMSE_pred = mean_squared_error(train_y, y_pred)

print(f'R2 between prediction is {r2_score}')
print(f'RMSE between prediction is {RMSE_pred}')
197/138: y_predic_external = SVR.predict(test_x)
197/139: y_predic_external = SVR.fit(train_x, train_y)
197/140: SVR_fit = SVR.fit(train_x, train_y)
197/141: SVR_fit.score
197/142: SVR_fit.score()
197/143: SVR_fit.score(train_x, train_y)
197/144:
r2_score = SVR_fit.score(train_x, train_y)
print(f'R2 between train is {r2_score}')
197/145: SVR_predict_y = SVR.predict(test_x)
197/146: r2_score = r2_score(test_y, SVR_predict_y)
197/147:
from sklearn.metrics import r2_score
r2_score = r2_score(test_y, SVR_predict_y)
197/148: y_pred = SVR.predict(test_x)
197/149:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error
r2_score = r2_score(test_y, y_pred)
RMSE_pred = mean_squared_error(train_y, y_pred)

print(f'R2 between prediction is {r2_score}')
print(f'RMSE between prediction is {RMSE_pred}')
197/150:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error
r2_score = r2_score(test_y, y_pred)
RMSE_pred = mean_squared_error(test_y, y_pred)

print(f'R2 between prediction is {r2_score}')
print(f'RMSE between prediction is {RMSE_pred}')
197/151:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error
r2_score = r2_score(test_y, y_pred)
RMSE_pred = mean_squared_error(test_y, y_pred)

print(f'Q2 between prediction and test is {r2_score}')
print(f'RMSE between prediction is {RMSE_pred}')
197/152:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))


#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(train_y, y_ext)
RMSE_test = mean_squared_error(train_y, y_pred)
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/153:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))


#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/154:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))


#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)
print('Results')
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/155:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/156: from sklearn.inspection import permutation_importance
197/157:
from sklearn.inspection import permutation_importance

perm_importance = permutation_importance(SVC, train_x, train_y)
peature_names = ['feature1', 'feature2', 'feature3', ...... ]
features = np.array(feature_names)

sorted_idx = perm_importance.importances_mean.argsort()
plt.barh(features[sorted_idx], perm_importance.importances_mean[sorted_idx])
plt.xlabel("Permutation Importance")
197/158:
from sklearn.inspection import permutation_importance

perm_importance = permutation_importance(SVC, train_x, train_y)
peature_names = ['feature1', 'feature2', 'feature3', ......]
features = np.array(feature_names)

sorted_idx = perm_importance.importances_mean.argsort()
plt.barh(features[sorted_idx], perm_importance.importances_mean[sorted_idx])
plt.xlabel("Permutation Importance")
197/159:
from sklearn.inspection import permutation_importance

result = permutation_importance(SVC, train_x, train_y, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()

fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=train_x.columns[sorted_idx])
ax.set_title("Permutation Importances (train set)")
fig.tight_layout()
plt.show()
197/160:
from sklearn.inspection import permutation_importance

result = permutation_importance(SVR, train_x, train_y, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()

fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=train_x.columns[sorted_idx])
ax.set_title("Permutation Importances (train set)")
fig.tight_layout()
plt.show()
197/161:
from sklearn.inspection import permutation_importance

result = permutation_importance(SVR, train_x, train_y, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()

fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=train_x.head[sorted_idx])
ax.set_title("Permutation Importances (train set)")
fig.tight_layout()
plt.show()
197/162:
from sklearn.inspection import permutation_importance

result = permutation_importance(SVR, train_x, train_y, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()

fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=fingerprint_to_model.columns[sorted_idx])
ax.set_title("Permutation Importances (train set)")
fig.tight_layout()
plt.show()
197/163:
from sklearn.inspection import permutation_importance

result = permutation_importance(SVR, train_x, train_y, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()

fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=fingerprint_to_model.columns[sorted_idx])
ax.set_title("Permutation Importances (train set)")
fig.tight_layout()
fig.set_size_inches(18.5, 10.5)
plt.show()
197/164:
from sklearn.inspection import permutation_importance

result = permutation_importance(SVR, train_x, train_y, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()

fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=fingerprint_to_model.columns[sorted_idx])
ax.set_title("Permutation Importances (train set)")
fig.tight_layout()
fig.set_size_inches(18.5, 20)
plt.show()
197/165:
from sklearn.inspection import permutation_importance

result = permutation_importance(SVR, train_x, train_y, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()

fig, ax = plt.subplots()
ax.boxplot(result.importances[sorted_idx].T,
           vert=False, labels=fingerprint_to_model.columns[sorted_idx])
ax.set_title("Permutation Importances (train set)")
fig.tight_layout()
fig.set_size_inches(18.5, 25)
plt.show()
197/166: fingerprint_to_model.columns[sorted_idx]
197/167: result.importances[sorted_idx].T
197/168: result
197/169: result.to_frame()
197/170: pd.DataFrame(result.importances[sorted_idx], columns=fingerprint_to_model.columns[sorted_idx])
197/171: pd.DataFrame(sorted_idx, columns=fingerprint_to_model.columns[sorted_idx])
197/172: pd.DataFrame(result.importances_mean, columns=fingerprint_to_model.columns)
197/173: pd.DataFrame(result.importances_mean, columns=fingerprint_to_model.columns[sorted_idx])
197/174: pd.DataFrame(result.importances_mean.T, columns=fingerprint_to_model.columns[sorted_idx])
197/175: result.importances_mean
197/176: result
197/177: result.shape
197/178: len(result)
197/179: result.importances
197/180: pd.DataFrame(result, columns=fingerprint_to_model.columns[sorted_idx])
197/181: pd.DataFrame(result.importances_mean, columns=fingerprint_to_model.columns[sorted_idx])
197/182: result.importances_mean
197/183: result.importances_mean.transpose
197/184: result.importances_mean.transpose()
197/185: result.importances_mean.transpose(X)
197/186: result.importances_mean.transpose(1)
197/187: result.importances_mean.transpose(1,1)
197/188: result.importances_mean.transpose(0,1)
197/189: result.importances_mean
197/190: result.importances_mean.to_csv('importances_mean.csv')
197/191: pd.DataFrame(result.importances_mean)
197/192: importances_mean = pd.DataFrame(result.importances_mean)
197/193: importances_mean.to_csv('importances_mean.csv')
197/194: fingerprint_to_model.columns[sorted_idx]
197/195: sorted_idx
197/196: pd.DataFrame(result.importances_mean[sorted_idx], columns=fingerprint_to_model.columns[sorted_idx])
197/197: pd.DataFrame(result.importances_mean[sorted_idx].T, columns=fingerprint_to_model.columns[sorted_idx])
197/198: result.importances_mean[sorted_idx].T
197/199: pd.DataFrame(result.importances_mean[sorted_idx].T, columns=fingerprint_to_model.columns[sorted_idx].T)
197/200: result.importances_mean[sorted_idx]
197/201: df = pd.DataFrame(fingerprint_to_model.columns[sorted_idx])
197/202: df
197/203: result.importances[sorted_idx]
197/204: df2 = pd.DataFrame(result.importances_mean[sorted_idx])
197/205: df2
197/206: df3 = pd.concat([df, df2], axis=1)
197/207: df3
197/208: df3.to_csv('importances_mean.csv')
197/209: df3
197/210: df3.rename({0: Features}, {0: importnaces_mean})
197/211: df3.rename({'0': 'Features'}, {'0': 'importnaces_mean'})
197/212: df3.rename({'0':'Features'}, {'0':'importnaces_mean'})
197/213: df3.rename(columns={'0':'Features', '0':'importnaces_mean'})
197/214: df4 = df3.rename(columns={'0':'Features', '0':'importnaces_mean'})
197/215:
df4 = df3.rename(columns={'0':'Features', '0':'importnaces_mean'})
df4
197/216:
df4 = df3.rename(columns={'Features':'0', '0':'importnaces_mean'})
df4
197/217:
df4 = df3.rename(columns={'Features':'0', '0':'importnaces_mean'})
df4
197/218:
df3 = pd.concat([df, df2], axis=1)
df3
197/219:
df4 = df3.rename(columns={'0':'Features', '0':'importnaces_mean'})
df4
197/220: df5.read_csv('importances_mean.csv')
197/221: df5 = pd.read_csv('importances_mean.csv')
197/222: df5
197/223: df5 = df5.drop(['Unnamed: 0'], axis=1)
197/224:
df5 = df5.drop(['Unnamed: 0'], axis=1)
df5
197/225:
df6 = df5.drop(['Unnamed: 0'], axis=1)
df6
197/226:
df5 = pd.read_csv('importances_mean.csv')
df5
197/227:
df5 = pd.read_csv('importances_mean.csv')
df5.head(3)
197/228:
df6 = df5.drop(['Unnamed: 0'], axis=1)
df6
197/229:
df6 = df5.drop(['Unnamed: 0'], axis=1)
df6.head(20)
197/230:
df6 = df5.drop(['Unnamed: 0'], axis=1)
df6.tail(20)
197/231: [selection] = ['ABC', 'ATS4pe', 'ATS4Z', 'ATS6Rare', 'nl', 'ATS3p', 'ATS2Z', 'ATS6v', 'ATS0m', 'ATS2m', 'ATS4v', 'ATS2se', 'ATS5pe', 'ATS4se', 'ATS4s', 'ATS8s', 'AATS3s', 'AATS2s', 'ATS1v', 'nH']
197/232:
[selection] = ['ABC', 'ATS4pe', 'ATS4Z', 'ATS6Rare', 'nl', 'ATS3p', 'ATS2Z', 'ATS6v', 'ATS0m', 'ATS2m', 'ATS4v', 'ATS2se', 'ATS5pe', 'ATS4se', 'ATS4s', 'ATS8s', 'AATS3s', 'AATS2s', 'ATS1v', 'nH']
fingerprint_to_model = compound_df[selection]
197/233: fingerprint_to_model
197/234:
[selection] = ['ABC', 'ATS4pe', 'ATS4Z', 'ATS6Rare', 'nl', 'ATS3p', 'ATS2Z', 'ATS6v', 'ATS0m']
fingerprint_to_model = compound_df[selection]
197/235:

fingerprint_to_model = compound_df[['ABC', 'ATS4pe', 'ATS4Z', 'ATS6Rare', 'nl', 'ATS3p', 'ATS2Z', 'ATS6v', 'ATS0m',
               'ATS2m', 'ATS4v', 'ATS2se', 'ATS5pe', 'ATS4se', 'ATS4s', 'ATS8s', 'AATS3s', 'AATS2s', 'ATS1v', 'nH']]
197/236:
fingerprint_to_model = compound_df[['ABC', 'ATS4pe', 'ATS4Z', 'ATS6are', 'nI', 'ATS3p', 'ATS2Z', 'ATS6v', 'ATS0m',
               'ATS2m', 'ATS4v', 'ATS2se', 'ATS5pe', 'ATS4se', 'ATS4s', 'ATS8s', 'AATS3s', 'AATS2s', 'ATS1v', 'nH']]
197/237: fingerprint_to_model
197/238: fingerprint_to_model
197/239: label_to_model = compound_df.LogKpu.tolist()
197/240:
train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
197/241:
label_to_model = compound_df.LogKpu.tolist()
fingerprint_to_modl.shape
197/242:
label_to_model = compound_df.LogKpu.tolist()
fingerprint_to_model.shape
197/243:
train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
197/244:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/245:
#not reduce
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.LogKpu.tolist()
197/246: fingerprint_to_model.shape
197/247:
train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
197/248: train_x.shape
197/249:
# Set model parameter for random forest regression
param = {
    "n_estimators": 1000,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/250:
# higher Features reduced performance?
# yes
197/251:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/252: y_cv.to_csv('y_cv.csv')
197/253:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/254:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/255:
#not reduce
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.LogKpu.tolist()
197/256: fingerprint_to_model.shape
197/257:
train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
197/258: train_x.shape
197/259:
# Set model parameter for random forest regression
param = {
    "n_estimators": 1000,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/260:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/261:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/262:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/263:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/264:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/265:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/266:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/267:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
197/268:
path = r'Fingerprint/'

Fp13  = pd.read_csv(path + 'mordard.csv'               , header = 0)
197/269: Fp13.shape
197/270:
df = pd.read_csv('df6_lipinski_kpu.csv')
df
197/271: name = df['compound']
197/272: name.shape
197/273:
df2 = df.rename({'compound': 'Name'}, axis=1)
df2 = df2.drop('Unnamed: 0', axis=1)
df3 = df2.set_index('Name')
df3
197/274:
LogKpu = df3[['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]
LogKpu
197/275:
Fp13 = Fp13.rename(columns={'name': 'Name'})
Fp13
197/276:

Fp13.Name = name
197/277:

Fpp13 = Fp13.drop('Name', axis=1)
Fpp13
197/278: Fpp13 = Fpp13.fillna(0)
197/279:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
197/280:

Fp13_n = normalized (Fpp13)
197/281: Fp13_n
197/282: np.any(np.isnan(Fp13_n))
197/283: Fp13_n["Name"] = Fp13.Name
197/284: Fp13_n
197/285: Fp13_n = Fp13_n.set_index('Name')
197/286: Fp13_n.head(20)
197/287: raw13  = LogKpu.merge(Fp13_n, on='Name', how='outer')
197/288: raw13
197/289: np.any(np.isnan(raw13))
197/290: raw13 =raw13[raw13.LogKpu.notna()]
197/291:
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.LogKpu.tolist()
197/292:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model.shape
197/293:
import random
SEED = random.seed(10)
SEED
197/294:
train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
197/295:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
197/296: fingerprint_to_model.shape
197/297:
train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
197/298: train_x.shape
197/299:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/300:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/301:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/302:
train_x.to_csv('y_train.csv')
train_x.to_csv('x_train.csv')
train_y.to_csv('y_train.csv')
test_y.to_csv('y_test.csv')
y_pred.to_csv('y_pred.csv')

y_ext.to_csv('y_SVR_ext.csv')
197/303: importances_mean = pd.DataFrame(result.importances_mean)
197/304: fingerprint_to_model.columns[sorted_idx]
197/305: df2 = pd.DataFrame(result.importances_mean[sorted_idx])
197/306: df = pd.DataFrame(fingerprint_to_model.columns[sorted_idx])
197/307:
from sklearn.inspection import permutation_importance

result = permutation_importance(SVR, train_x, train_y, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()

fig, ax = plt.subplots()
ax.boxplot(อT,
           vert=False, labels=fingerprint_to_model.columns[sorted_idx])
ax.set_title("Permutation Importances (train set)")
fig.tight_layout()
fig.set_size_inches(18.5, 25)
plt.show()
197/308:
from sklearn.inspection import permutation_importance

result = permutation_importance(SVR, train_x, train_y, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()

fig, ax = plt.subplots()
ax.boxplot(result.importances_mean[sorted_idx].T,
           vert=False, labels=fingerprint_to_model.columns[sorted_idx])
ax.set_title("Permutation Importances (train set)")
fig.tight_layout()
fig.set_size_inches(18.5, 25)
plt.show()
197/309:
from sklearn.inspection import permutation_importance

result = permutation_importance(SVR, train_x, train_y, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()

fig, ax = plt.subplots()
ax.boxplot(result.importances_mean[sorted_idx].T,
           vert=False, labels=fingerprint_to_model.columns[sorted_idx])
ax.set_title("Permutation Importances (train set)")
fig.tight_layout()
fig.set_size_inches(18.5, 25)
plt.show()
197/310: importances_mean = pd.DataFrame(result.importances_mean)
197/311: fingerprint_to_model.columns[sorted_idx]
197/312: df2 = pd.DataFrame(result.importances_mean[sorted_idx])
197/313: df = pd.DataFrame(fingerprint_to_model.columns[sorted_idx])
197/314: df
197/315: df2
197/316:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/317:
train_x.to_csv('y_train.csv')
train_x.to_csv('x_train.csv')
train_y.to_csv('y_train.csv')
test_y.to_csv('y_test.csv')
y_pred.to_csv('y_pred.csv')

y_ext.to_csv('y_SVR_ext.csv')
197/318: plt.scatter(test_y, y_ext)
197/319:
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='black')
plt.scatter(train_y, train_y, c='blue')
197/320:
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='black')
197/321:
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='black')
m, b = np.polyfit(test_y, y_ext, 1)
197/322:
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='black')
m, b = np.polyfit(test_y, y_ext, 1)
plt.plot(x, m*x + b)
197/323:
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='black')
m, b = np.polyfit(test_y, y_ext, 1)
plt.plot(test_y, m*x + b)
197/324:
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='black')
m, b = np.polyfit(test_y, y_ext, 1)
plt.plot(test_y, m*test_y + b)
197/325:
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='black')
197/326:
pd.DataFrame(train_x).to_csv("Results/x_train.csv")
pd.DataFrame(test_x).to_csv("Results/x_test.csv")
pd.DataFrame(train_x).to_csv("Results/train_x.csv")
pd.DataFrame(train_y).to_csv("Results/train_y.csv")
pd.DataFrame(test_y).to_csv("Results/test_y.csv")
pd.DataFrame(y_pred).to_csv("Results/y_pred.csv")
pd.DataFrame(y_ext).to_csv("Results/y_SVR_ext.csv")
197/327:
pd.DataFrame(train_x).to_csv("Result/x_train.csv")
pd.DataFrame(test_x).to_csv("Result/x_test.csv")
pd.DataFrame(train_x).to_csv("Result/train_x.csv")
pd.DataFrame(train_y).to_csv("Result/train_y.csv")
pd.DataFrame(test_y).to_csv("Result/test_y.csv")
pd.DataFrame(y_pred).to_csv("Result/y_pred.csv")
pd.DataFrame(y_ext).to_csv("Result/y_SVR_ext.csv")
202/1:
#import important library
import numpy as np
import pandas as pd
from sklearn import svm, metrics, clone
from sklearn.ensemble import RandomForestRegressor
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import KFold, train_test_split
from sklearn.metrics import auc, accuracy_score, recall_score
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt
from rdkit import Chem
from rdkit.Chem import MACCSkeys
from rdkit.Chem.AllChem import GetMorganFingerprintAsBitVect
202/2:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
202/3:
df = pd.read_csv('df6_lipinski_kpu.csv')
df
202/4: name = df['compound']
202/5:
df2 = df.rename({'compound': 'Name'}, axis=1)
df2 = df2.drop('Unnamed: 0', axis=1)
df3 = df2.set_index('Name')
df3
202/6:
df = pd.read_csv('swissadme_kpu.csv')
df
202/7:
df2 = df.rename({'compound': 'Name'}, axis=1)
df2 = df2.drop('Canonical SMILES', axis=1)
df3 = df2.set_index('Name')
df3
202/8:
df_Fn = df3.drop(['smiles', 'Cells', 'LogKpu', 'Formula'], axis=1)
df_Fn
202/9:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
202/10:

df_Fn_n = normalized (df_Fn)
202/11:
df2 = df.rename({'compound': 'Name'}, axis=1)
df2 = df2.drop('Canonical SMILES', axis=1)
df3 = df2.set_index('Name')
df3
202/12:
df2 = df.rename({'compound': 'Name'}, axis=1)
df2 = df2.drop('Canonical SMILES', axis=1)
df3 = df2.set_index('Name')
df3.head(3)
202/13:
ESOL Class = []
for i in df_final.pIC50:
  if float(i) >= 6:
    bioactivity_class.append("active")
  elif float(i) <= 4:
    bioactivity_class.append("inactive")
  else:
    bioactivity_class.append("intermediate")
202/14:
ESOL_Class = []
for i in df3.ESOL_Class:

    if i = 'Pooly soluble':
        ESOL_Class.append("0")
    elif i = 'Moderately soluble':
        ESOL_Class.append("1")
    elif i = 'Soluble':
        ESOL_Class.append("2")
    else:
        ESOL_Class.append("3")
202/15:
ESOL_Class = []
for i in df3.ESOL_Class:

    if i == 'Pooly soluble':
        ESOL_Class.append("0")
    elif i = 'Moderately soluble':
        ESOL_Class.append("1")
    elif i = 'Soluble':
        ESOL_Class.append("2")
    else:
        ESOL_Class.append("3")
202/16:
ESOL_Class = []
for i in df3.ESOL_Class:

    if i == 'Pooly soluble':
        ESOL_Class.append("0")
    elif i == 'Moderately soluble':
        ESOL_Class.append("1")
    elif i == 'Soluble':
        ESOL_Class.append("2")
    else:
        ESOL_Class.append("3")
202/17:
ESOL_Class = []
for i in df3.ESOL_Class:

    if i == 'Pooly soluble':
        ESOL_Class.append("0")
    elif i == 'Moderately soluble':
        ESOL_Class.append("1")
    elif i == 'Soluble':
        ESOL_Class.append("2")
    else:
        ESOL_Class.append("3")
202/18:
df = pd.read_csv('swissadme_kpu.csv')
df
202/19:
df2 = df.rename({'compound': 'Name'}, axis=1)
df2 = df2.drop('Canonical SMILES', axis=1)
df3 = df2.set_index('Name')
df3.head(3)
202/20:
ESOL_Class = []
for i in df3.ESOL_Class:

    if i == 'Pooly soluble':
        ESOL_Class.append("0")
    elif i == 'Moderately soluble':
        ESOL_Class.append("1")
    elif i == 'Soluble':
        ESOL_Class.append("2")
    else:
        ESOL_Class.append("3")
202/21:
df2 = df.rename({'compound': 'Name'}, axis=1)
df2 = df2.drop('Canonical_SMILES', axis=1)
df3 = df2.set_index('Name')
df3.head(3)
202/22:
ESOL_Class = []
for i in df3.ESOL_Class:

    if i == 'Pooly soluble':
        ESOL_Class.append("0")
    elif i == 'Moderately soluble':
        ESOL_Class.append("1")
    elif i == 'Soluble':
        ESOL_Class.append("2")
    else:
        ESOL_Class.append("3")
202/23: df3.head(3)
202/24: df3.ESOL_Class(3)
202/25: df3.ESOL_Class
202/26: ESOL_Class
202/27:
Ali_Class = []
for i in df3.Ali_Class:

    if i == 'Pooly soluble':
        Ali_Class.append("0")
    elif i == 'Moderately soluble':
        Ali_Class.append("1")
    elif i == 'Soluble':
        Ali_Class.append("2")
    else:
        Ali_Class.append("3")
202/28:
df = pd.read_csv('swissadme_kpu.csv')
df
202/29:
ESOL_Class = []
for i in df3.ESOL_Class:

    if i == 'Poorly soluble':
        ESOL_Class.append("1")
    elif i == 'Moderately soluble':
        ESOL_Class.append("2")
    elif i == 'Soluble':
        ESOL_Class.append("3")
    else:
        ESOL_Class.append("4")
202/30:
df = pd.read_csv('swissadme_kpu.csv')
df
202/31:
df = pd.read_csv('swissadme_kpu.csv')
df
202/32:
df2 = df.rename({'compound': 'Name'}, axis=1)
df2 = df2.drop('Canonical_SMILES', axis=1)
df3 = df2.set_index('Name')
df3.head(3)
202/33:
ESOL_Class = []
for i in df3.ESOL_Class:

    if i == 'Poorly soluble':
        ESOL_Class.append("1")
    elif i == 'Moderately soluble':
        ESOL_Class.append("2")
    elif i == 'Soluble':
        ESOL_Class.append("3")
    else:
        ESOL_Class.append("4")
202/34:
Ali_Class = []
for i in df3.Ali_Class:

    if i == 'insoluble':
        Ali_Class.append("0")
    elif i == 'Poorly soluble':
        Ali_Class.append("1")
    elif i == 'Moderately soluble':
        Ali_Class.append("2")
    elif i == 'Soluble':
        Ali_Class.append("3")
    else:
        Ali_Class.append("4")
202/35:
Ali_Class = []
for i in df3.Silli:

    if i == 'insoluble':
        Ali_Class.append("0")
    elif i == 'Poorly soluble':
        Ali_Class.append("1")
    elif i == 'Moderately soluble':
        Ali_Class.append("2")
    elif i == 'Soluble':
        Ali_Class.append("3")
    else:
        Ali_Class.append("4")
202/36:
Ali_Class = []
for i in df3.Silicos-IT_class:

    if i == 'insoluble':
        Ali_Class.append("0")
    elif i == 'Poorly soluble':
        Ali_Class.append("1")
    elif i == 'Moderately soluble':
        Ali_Class.append("2")
    elif i == 'Soluble':
        Ali_Class.append("3")
    else:
        Ali_Class.append("4")
202/37:
df = pd.read_csv('swissadme_kpu.csv')
df
202/38:
df2 = df.rename({'compound': 'Name'}, axis=1)
df2 = df2.drop('Canonical_SMILES', axis=1)
df3 = df2.set_index('Name')
df3.head(3)
202/39:
ESOL_Class = []
for i in df3.ESOL_Class:

    if i == 'Poorly soluble':
        ESOL_Class.append("1")
    elif i == 'Moderately soluble':
        ESOL_Class.append("2")
    elif i == 'Soluble':
        ESOL_Class.append("3")
    else:
        ESOL_Class.append("4")
202/40:
Ali_Class = []
for i in df3.Ali_Class:

    if i == 'insoluble':
        Ali_Class.append("0")
    elif i == 'Poorly soluble':
        Ali_Class.append("1")
    elif i == 'Moderately soluble':
        Ali_Class.append("2")
    elif i == 'Soluble':
        Ali_Class.append("3")
    else:
        Ali_Class.append("4")
202/41:
Ali_Class = []
for i in df3.Silicos_IT_class:

    if i == 'insoluble':
        Ali_Class.append("0")
    elif i == 'Poorly soluble':
        Ali_Class.append("1")
    elif i == 'Moderately soluble':
        Ali_Class.append("2")
    elif i == 'Soluble':
        Ali_Class.append("3")
    else:
        Ali_Class.append("4")
202/42:
df = pd.read_csv('swissadme_kpu.csv')
df
202/43:
df2 = df.rename({'compound': 'Name'}, axis=1)
df2 = df2.drop('Canonical_SMILES', axis=1)
df3 = df2.set_index('Name')
df3.head(3)
202/44: df3.columns()
202/45: df3.columns
202/46:
Ali_Class = []
for i in df3.'Silicos-IT_class':

    if i == 'insoluble':
        Ali_Class.append("0")
    elif i == 'Poorly soluble':
        Ali_Class.append("1")
    elif i == 'Moderately soluble':
        Ali_Class.append("2")
    elif i == 'Soluble':
        Ali_Class.append("3")
    else:
        Ali_Class.append("4")
202/47:
df = pd.read_csv('swissadme_kpu.csv')
df
202/48:
df2 = df.rename({'compound': 'Name'}, axis=1)
df2 = df2.drop('Canonical_SMILES', axis=1)
df3 = df2.set_index('Name')
df3.head(3)
202/49: df3.columns
202/50:
df = pd.read_csv('swissadme_kpu.csv')
df
202/51:
df2 = df.rename({'compound': 'Name'}, axis=1)
df2 = df2.drop('Canonical_SMILES', axis=1)
df3 = df2.set_index('Name')
df3.head(3)
202/52:
ESOL_Class = []
for i in df3.ESOL_Class:

    if i == 'Poorly soluble':
        ESOL_Class.append("1")
    elif i == 'Moderately soluble':
        ESOL_Class.append("2")
    elif i == 'Soluble':
        ESOL_Class.append("3")
    else:
        ESOL_Class.append("4")
202/53: df3.columns
202/54:
Ali_Class = []
for i in df3.Ali_Class:

    if i == 'insoluble':
        Ali_Class.append("0")
    elif i == 'Poorly soluble':
        Ali_Class.append("1")
    elif i == 'Moderately soluble':
        Ali_Class.append("2")
    elif i == 'Soluble':
        Ali_Class.append("3")
    else:
        Ali_Class.append("4")
202/55:
Ali_Class = []
for i in df3.Silicos-IT_class:

    if i == 'insoluble':
        Ali_Class.append("0")
    elif i == 'Poorly soluble':
        Ali_Class.append("1")
    elif i == 'Moderately soluble':
        Ali_Class.append("2")
    elif i == 'Soluble':
        Ali_Class.append("3")
    else:
        Ali_Class.append("4")
202/56:
Ali_Class = []
for i in df3.Silicos_IT_class:

    if i == 'insoluble':
        Silicos_IT_class.append("0")
    elif i == 'Poorly soluble':
        Silicos_IT_class.append("1")
    elif i == 'Moderately soluble':
        Silicos_IT_class.append("2")
    elif i == 'Soluble':
        Silicos_IT_class.append("3")
    else:
        Silicos_IT_class.append("4")
202/57:
Silicos_IT_class = []
for i in df3.Silicos_IT_class:

    if i == 'insoluble':
        Silicos_IT_class.append("0")
    elif i == 'Poorly soluble':
        Silicos_IT_class.append("1")
    elif i == 'Moderately soluble':
        Silicos_IT_class.append("2")
    elif i == 'Soluble':
        Silicos_IT_class.append("3")
    else:
        Silicos_IT_class.append("4")
202/58:
ESOL_Class = []
for i in df3.ESOL_Class:

    if i == 'Poorly soluble':
        ESOL_Class.append("1")
    elif i == 'Moderately soluble':
        ESOL_Class.append("2")
    elif i == 'Soluble':
        ESOL_Class.append("3")
    else:
        ESOL_Class.append("4")
202/59:
Ali_Class = []
for i in df3.Ali_Class:

    if i == 'insoluble':
        Ali_Class.append("0")
    elif i == 'Poorly soluble':
        Ali_Class.append("1")
    elif i == 'Moderately soluble':
        Ali_Class.append("2")
    elif i == 'Soluble':
        Ali_Class.append("3")
    else:
        Ali_Class.append("4")
202/60:
Silicos_IT_class = []
for i in df3.Silicos_IT_class:

    if i == 'insoluble':
        Silicos_IT_class.append("0")
    elif i == 'Poorly soluble':
        Silicos_IT_class.append("1")
    elif i == 'Moderately soluble':
        Silicos_IT_class.append("2")
    elif i == 'Soluble':
        Silicos_IT_class.append("3")
    else:
        Silicos_IT_class.append("4")
202/61:
#drop 'GIabsorption','BBBpermeant', 'Pgpsubstrate', 'CYP1A2inhibitor', 'CYP2C19inhibitor','CYP2C9inhibitor', 'CYP2D6inhibitor', 'CYP3A4inhibitor'

df4 =df3.drop(['GIabsorption','BBBpermeant', 'Pgpsubstrate', 'CYP1A2inhibitor', 'CYP2C19inhibitor','CYP2C9inhibitor', 'CYP2D6inhibitor', 'CYP3A4inhibitor'], axis=1)
202/62: df4
202/63:
#drop 'smiles','Cells', 'GIabsorption','BBBpermeant', 'Pgpsubstrate', 'CYP1A2inhibitor', 'CYP2C19inhibitor','CYP2C9inhibitor', 'CYP2D6inhibitor', 'CYP3A4inhibitor'

df4 =df3.drop(['smiles','Cells','GIabsorption','BBBpermeant', 'Pgpsubstrate', 'CYP1A2inhibitor', 'CYP2C19inhibitor','CYP2C9inhibitor', 'CYP2D6inhibitor', 'CYP3A4inhibitor'], axis=1)
202/64: df4
202/65:
#drop 'smiles','Cells','Formula', 'GIabsorption','BBBpermeant', 'Pgpsubstrate', 'CYP1A2inhibitor', 'CYP2C19inhibitor','CYP2C9inhibitor', 'CYP2D6inhibitor', 'CYP3A4inhibitor'

df4 =df3.drop(['smiles','Cells','Formula', 'GIabsorption','BBBpermeant', 'Pgpsubstrate', 'CYP1A2inhibitor', 'CYP2C19inhibitor','CYP2C9inhibitor', 'CYP2D6inhibitor', 'CYP3A4inhibitor'], axis=1)
202/66: df4
202/67:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
202/68: df4_n = normalized(df4)
202/69:
ESOL_Class = pd.DataFrame(ESOL_Class)
Ali_Class = pd.DataFrame(Ali_Class)
Silicos_IT_class = pd.DataFrame(Silicos_IT_class)
202/70: df5 = pd.concat[[df4, ESOL_Class, Ali_Class, Silicos_IT_class]]
202/71: df5 = pd.concat([df4, ESOL_Class, Ali_Class, Silicos_IT_class])
202/72: df5
202/73: df5 = pd.concat([df4, ESOL_Class, Ali_Class, Silicos_IT_class], axis=1)
202/74: df5
202/75:
ESOL_Class = pd.DataFrame(ESOL_Class)
Ali_Class = pd.DataFrame(Ali_Class)
Silicos_IT_class = pd.DataFrame(Silicos_IT_class)
ESOL_Class.shape
202/76:
ESOL_Class.shape
Ali_Class.shape
202/77:

Ali_Class.shape
202/78:
ESOL_Class = pd.DataFrame(ESOL_Class)
Ali_Class = pd.DataFrame(Ali_Class)
Silicos_IT_class = pd.DataFrame(Silicos_IT_class)
ESOL_Class.shape
202/79: Silicos_IT_class.shape
202/80: df5 = pd.concat([df4, ESOL_Class, Ali_Class, Silicos_IT_class], axis=True)
202/81: df5 = pd.concat([df4, ESOL_Class, Ali_Class, Silicos_IT_class], index=True)
202/82: df5 = pd.concat([df4, ESOL_Class, Ali_Class, Silicos_IT_class], axis=1)
202/83: df5
202/84: df4
202/85: ESOL_Class
202/86:
ESOL_Class = pd.DataFrame(ESOL_Class, columns='ESOL_Class')
Ali_Class = pd.DataFrame(Ali_Class, columns='Ali_Class')
Silicos_IT_class = pd.DataFrame(Silicos_IT_class, columns='Silicos_IT_class')
ESOL_Class.shape
202/87:
ESOL_Class = pd.DataFrame(ESOL_Class, columns='ESOL_Class')
Ali_Class = pd.DataFrame(Ali_Class, columns='Ali_Class')
Silicos_IT_class = pd.DataFrame(Silicos_IT_class, columns='Silicos_IT_class')
ESOL_Class.shape
202/88:
ESOL_Class = pd.DataFrame(ESOL_Class, columns=['ESOL_Class'])
Ali_Class = pd.DataFrame(Ali_Class, columns=['Ali_Class'])
Silicos_IT_class = pd.DataFrame(Silicos_IT_class, columns=['Silicos_IT_class'])
ESOL_Class.shape
202/89: ESOL_Class
202/90: df5 = pd.concat([df4, ESOL_Class, Ali_Class, Silicos_IT_class], axis=1)
202/91: df5
202/92: df_all = pd.concat([df4, ESOL_Class], axis=1)
202/93: df_all
202/94: df_all = pd.concat([df4, ESOL_Class], axis=0)
202/95: df_all
202/96: df_all = pd.concat([df4, ESOL_Class])
202/97: df_all
202/98: ESOL_Class
202/99: ESOL_Class
202/100:
df2 = df.rename({'compound': 'Name'}, axis=1)
df2 = df2.drop('Canonical_SMILES', axis=1)
df3 = df2.set_index('Name')
df3.head(3)
202/101:
ESOL_Class = []
for i in df3.ESOL_Class:

    if i == 'Poorly soluble':
        ESOL_Class.append("1")
    elif i == 'Moderately soluble':
        ESOL_Class.append("2")
    elif i == 'Soluble':
        ESOL_Class.append("3")
    else:
        ESOL_Class.append("4")
202/102:
ESOL_Class = pd.DataFrame(ESOL_Class, columns=['ESOL_Class'])
Ali_Class = pd.DataFrame(Ali_Class, columns=['Ali_Class'])
Silicos_IT_class = pd.DataFrame(Silicos_IT_class, columns=['Silicos_IT_class'])
ESOL_Class.shape
202/103: ESOL_Class
202/104: Silicos_IT_class.shape
202/105:
Ali_Class = []
for i in df3.Ali_Class:

    if i == 'insoluble':
        Ali_Class.append("0")
    elif i == 'Poorly soluble':
        Ali_Class.append("1")
    elif i == 'Moderately soluble':
        Ali_Class.append("2")
    elif i == 'Soluble':
        Ali_Class.append("3")
    else:
        Ali_Class.append("4")
202/106:
Silicos_IT_class = []
for i in df3.Silicos_IT_class:

    if i == 'insoluble':
        Silicos_IT_class.append("0")
    elif i == 'Poorly soluble':
        Silicos_IT_class.append("1")
    elif i == 'Moderately soluble':
        Silicos_IT_class.append("2")
    elif i == 'Soluble':
        Silicos_IT_class.append("3")
    else:
        Silicos_IT_class.append("4")
202/107:
#drop 'smiles','Cells','Formula', 'GIabsorption','BBBpermeant', 'Pgpsubstrate', 'CYP1A2inhibitor', 'CYP2C19inhibitor','CYP2C9inhibitor', 'CYP2D6inhibitor', 'CYP3A4inhibitor'

df4 =df3.drop(['smiles','Cells','Formula', 'GIabsorption','BBBpermeant', 'Pgpsubstrate', 'CYP1A2inhibitor', 'CYP2C19inhibitor','CYP2C9inhibitor', 'CYP2D6inhibitor', 'CYP3A4inhibitor'], axis=1)
202/108: df4
202/109:
ESOL_Class = pd.DataFrame(ESOL_Class, columns=['ESOL_Class'])
Ali_Class = pd.DataFrame(Ali_Class, columns=['Ali_Class'])
Silicos_IT_class = pd.DataFrame(Silicos_IT_class, columns=['Silicos_IT_class'])
ESOL_Class.shape
202/110: ESOL_Class
202/111: Ali_Class
202/112: Silicos_IT_class.shape
202/113:

Ali_Class.shape
202/114: df4
202/115: df_all = pd.concat([df4, ESOL_Class, Ali_Class, Silicos_IT_class])
202/116: df_all
202/117: df_all = pd.concat([df4, ESOL_Class, Ali_Class, Silicos_IT_class], ignore_index=True)
202/118: df_all
202/119:
ESOL_Class = pd.DataFrame(ESOL_Class, columns=['ESOL_Class']).to_csv('ESOL_Class.csv')
Ali_Class = pd.DataFrame(Ali_Class, columns=['Ali_Class']).to_csv('Ali_Class.csv')
Silicos_IT_class = pd.DataFrame(Silicos_IT_class, columns=['Silicos_IT_class']).to_csv('Silicos_IT_class.csv')
ESOL_Class.shape
202/120:
ESOL_Class = pd.DataFrame(ESOL_Class, columns=['ESOL_Class']).to_csv('ESOL_Class.csv')
Ali_Class = pd.DataFrame(Ali_Class, columns=['Ali_Class']).to_csv('Ali_Class.csv')
Silicos_IT_class = pd.DataFrame(Silicos_IT_class, columns=['Silicos_IT_class']).to_csv('Silicos_IT_class.csv')
202/121:
ESOL_Class = pd.DataFrame(ESOL_Class, columns=['ESOL_Class'])
Ali_Class = pd.DataFrame(Ali_Class, columns=['Ali_Class'])
Silicos_IT_class = pd.DataFrame(Silicos_IT_class, columns=['Silicos_IT_class'])
202/122:
ESOL_Class.to_csv('ESOL_Class.csv')
Ali_Class.to_csv('Ali_Class.csv')
Silicos_IT_class.to_csv('Silicos_IT_class.csv')
202/123: ESOL_Class
202/124:
ESOL_Class = []
for i in df3.ESOL_Class:

    if i == 'Poorly soluble':
        ESOL_Class.append("1")
    elif i == 'Moderately soluble':
        ESOL_Class.append("2")
    elif i == 'Soluble':
        ESOL_Class.append("3")
    else:
        ESOL_Class.append("4")
202/125:
Ali_Class = []
for i in df3.Ali_Class:

    if i == 'insoluble':
        Ali_Class.append("0")
    elif i == 'Poorly soluble':
        Ali_Class.append("1")
    elif i == 'Moderately soluble':
        Ali_Class.append("2")
    elif i == 'Soluble':
        Ali_Class.append("3")
    else:
        Ali_Class.append("4")
202/126:
Silicos_IT_class = []
for i in df3.Silicos_IT_class:

    if i == 'insoluble':
        Silicos_IT_class.append("0")
    elif i == 'Poorly soluble':
        Silicos_IT_class.append("1")
    elif i == 'Moderately soluble':
        Silicos_IT_class.append("2")
    elif i == 'Soluble':
        Silicos_IT_class.append("3")
    else:
        Silicos_IT_class.append("4")
202/127:
#drop 'smiles','Cells','Formula', 'GIabsorption','BBBpermeant', 'Pgpsubstrate', 'CYP1A2inhibitor', 'CYP2C19inhibitor','CYP2C9inhibitor', 'CYP2D6inhibitor', 'CYP3A4inhibitor'

df4 =df3.drop(['smiles','Cells','Formula', 'GIabsorption','BBBpermeant', 'Pgpsubstrate', 'CYP1A2inhibitor', 'CYP2C19inhibitor','CYP2C9inhibitor', 'CYP2D6inhibitor', 'CYP3A4inhibitor'], axis=1)
202/128: df4
202/129:
ESOL_Class = pd.DataFrame(ESOL_Class, columns=['ESOL_Class'])
Ali_Class = pd.DataFrame(Ali_Class, columns=['Ali_Class'])
Silicos_IT_class = pd.DataFrame(Silicos_IT_class, columns=['Silicos_IT_class'])
202/130:
ESOL_Class.to_csv('ESOL_Class.csv')
Ali_Class.to_csv('Ali_Class.csv')
Silicos_IT_class.to_csv('Silicos_IT_class.csv')
202/131: ESOL_Class
202/132:
df = pd.read_csv('swissadme_kpu.csv')
df
202/133: df5 = pd.read_csv('swissadme_kpu.csv')
202/134: df5
202/135: df5[ESOL_Class]
202/136: df5.ESOL_Class
202/137: df5
202/138: df6 = df5.drop(['Canonical_SMILES', 'smiles','Cells','Formula', 'GIabsorption','BBBpermeant', 'Pgpsubstrate', 'CYP1A2inhibitor', 'CYP2C19inhibitor','CYP2C9inhibitor', 'CYP2D6inhibitor', 'CYP3A4inhibitor'], axis=1)
202/139: df6
202/140: df7 =df6[df6.LogKpu.notna()]
202/141: df7
202/142: df8 = df6.drop(['compound', 'LogKpu'], axis=1)
202/143: df8
202/144: df8 = df6.drop(['LogKpu'], axis=1)
202/145: df8
202/146: df8.set_index(compound)
202/147: df8.set_index('compound')
202/148: df9 = df8.set_index('compound')
202/149: df9
202/150: df9 = df6.set_index('compound')
202/151: df7 = df6.set_index('compound')
202/152: df7
202/153: label_to_model = df76.LogKpu.tolist()
202/154: label_to_model = df6.LogKpu.tolist()
202/155: label_to_model
202/156: label_to_model = df7.LogKpu.tolist()
202/157: df8_Fn = df7.drop(['LogKpu'], axis=1)
202/158:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
202/159:

df8_Fn_n = normalized (df8_Fn)
202/160: df8_Fn_n
202/161: fingerprint_to_model = df8_Fn_n
202/162:
import random
SEED = random.seed(10)
SEED
202/163:
train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
202/164:
df7 = df6.set_index('compound')
df7 = df6.LogKpu.notna
202/165: df7
202/166: df7 = df6.set_index('compound')
202/167: df7
202/168: df7
202/169: df7 =df6[df6.LogKpu.notna()]
202/170: df7
202/171: df8 = df7.set_index('compound')
202/172: df8
202/173: label_to_model = df8.LogKpu.tolist()
202/174: df8_Fn = df8.drop(['LogKpu'], axis=1)
202/175:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
202/176: df8_Fn_n = normalized (df8_Fn)
202/177: df8_Fn_n
202/178: fingerprint_to_model = df8_Fn_n
202/179:
import random
SEED = random.seed(10)
SEED
202/180:
train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
202/181:
# Set model parameter for random forest regression
param = {
    "n_estimators": 1000,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
202/182:
#import important library
import numpy as np
import pandas as pd
from sklearn import svm, metrics, clone
from sklearn.ensemble import RandomForestRegressor
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import KFold, train_test_split
from sklearn.metrics import auc, accuracy_score, recall_score
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt
from rdkit import Chem
from rdkit.Chem import MACCSkeys
from rdkit.Chem.AllChem import GetMorganFingerprintAsBitVect

# Set model parameter for random forest regression
param = {
    "n_estimators": 1000,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
202/183:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
# Set model parameter for random forest regression
param = {
    "n_estimators": 1000,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
202/184:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
202/185: fingerprint_to_model = df8_Fn
202/186:
import random
SEED = random.seed(10)
SEED
202/187:
train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
202/188:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
# Set model parameter for random forest regression
param = {
    "n_estimators": 1000,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
202/189:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
# Set model parameter for random forest regression
param = {
    "n_estimators": 100,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
202/190:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
202/191: fingerprint_to_model = df8_Fn_n
202/192:
import random
SEED = random.seed(10)
SEED
202/193:
train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
202/194:
train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
202/195:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
# Set model parameter for random forest regression
param = {
    "n_estimators": 100,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
202/196:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
202/197:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
202/198:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/328:
from sklearn.inspection import permutation_importance

result = permutation_importance(SVR, train_x, train_y, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()

fig, ax = plt.subplots()
ax.boxplot(result.importances_mean[sorted_idx].T,
           vert=False, labels=fingerprint_to_model.columns[sorted_idx])
ax.set_title("Permutation Importances (train set)")
fig.tight_layout()
fig.set_size_inches(18.5, 25)
plt.show()
197/329:
#not reduce
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.LogKpu.tolist()
197/330:
from sklearn.inspection import permutation_importance

result = permutation_importance(SVR, train_x, train_y, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()

fig, ax = plt.subplots()
ax.boxplot(result.importances_mean[sorted_idx].T,
           vert=False, labels=fingerprint_to_model.columns[sorted_idx])
ax.set_title("Permutation Importances (train set)")
fig.tight_layout()
fig.set_size_inches(18.5, 25)
plt.show()
197/331: fingerprint_to_model
197/332:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/333:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='linear')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/334:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='poly')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/335:
from sklearn.inspection import permutation_importance

result = permutation_importance(SVR, test_x, y_ext, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()

fig, ax = plt.subplots()
ax.boxplot(result.importances_mean[sorted_idx].T,
           vert=False, labels=fingerprint_to_model.columns[sorted_idx])
ax.set_title("Permutation Importances (train set)")
fig.tight_layout()
fig.set_size_inches(18.5, 25)
plt.show()
197/336:
from sklearn.inspection import permutation_importance

result = permutation_importance(SVR, test_x, y_ext, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()

fig, ax = plt.subplots()
ax.boxplot(result.importances_mean[sorted_idx].T,
           vert=False, labels=fingerprint_to_model.columns[sorted_idx])
ax.set_title("Permutation Importances (train set)")
fig.tight_layout()
fig.set_size_inches(18.5, 25)
plt.show()
197/337:
from sklearn.inspection import permutation_importance

result = permutation_importance(SVR, test_x, y_ext, n_repeats=10,
                                random_state=42, n_jobs=2)
197/338:
from sklearn.inspection import permutation_importance

result = permutation_importance(SVR, test_x, y_ext, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()

fig, ax = plt.subplots()
ax.boxplot(result.importances_mean[sorted_idx].T,
           vert=False, labels=fingerprint_to_model.columns[sorted_idx])
ax.set_title("Permutation Importances (train set)")
fig.tight_layout()
fig.set_size_inches(18.5, 25)
plt.show()
197/339:
from sklearn.inspection import permutation_importance

result = permutation_importance(SVR, test_x, y_ext, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()

fig, ax = plt.subplots()
ax.boxplot(result.importances_mean[sorted_idx].T,
           vert=False, labels= fingerprint_to_model.columns[sorted_idx])
ax.set_title("Permutation Importances (train set)")
fig.tight_layout()
fig.set_size_inches(18.5, 25)
plt.show()
197/340:
from sklearn.inspection import permutation_importance

result = permutation_importance(SVR, test_x, y_ext, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()

fig, ax = plt.subplots()
ax.boxplot(result.importances_mean[sorted_idx].T,
           vert=False, labels= test_x.columns[sorted_idx])
ax.set_title("Permutation Importances (train set)")
fig.tight_layout()
fig.set_size_inches(18.5, 25)
plt.show()
197/341:
from sklearn.inspection import permutation_importance

result = permutation_importance(SVR, test_x, y_ext, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()

fig, ax = plt.subplots()
ax.boxplot(result.importances_mean[sorted_idx].T,
           vert=False, labels= fingerprint_to_model.columns[sorted_idx])
ax.set_title("Permutation Importances (train set)")
fig.tight_layout()
fig.set_size_inches(18.5, 25)
plt.show()
197/342:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

param = {
    "n_estimators": 1000,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/343:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

param = {
    "n_estimators": 1000,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/344:
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='black')
197/345:
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='black')
plt.scatter(train_y, y_pred, c='black')
197/346:
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='black')
plt.scatter(train_y, y_pred, c='blue')
197/347:
plt.scatter(test_y, y_ext, c='red')

plt.scatter(train_y, y_pred, c='blue')
197/348:
pd.DataFrame(train_x, columns=['train_x']).to_csv("Result_RF/x_train.csv")
pd.DataFrame(test_x, columns=['test_x']).to_csv("Result_RF/x_test.csv")
pd.DataFrame(train_x, columns=['train_x']).to_csv("Result_RF/train_x.csv")
pd.DataFrame(train_y, columns=['train_y']).to_csv("Result_RF/train_y.csv")
pd.DataFrame(test_y, columns=['test_y']).to_csv("Result_RF/test_y.csv")
pd.DataFrame(y_pred, columns=['y_pred']).to_csv("Result_RF/y_pred.csv")
pd.DataFrame(y_ext, columns=['y_ext']).to_csv("Result_RF/y_SVR_ext.csv")
197/349: pd.DataFrame(train_x).to_csv("Result_RF/x_train.csv")
197/350:
pd.DataFrame(train_x, columns='train_x').to_csv("Result_RF/x_train.csv")
pd.DataFrame(test_x, columns='test_x').to_csv("Result_RF/x_test.csv")
pd.DataFrame(train_x, columns='train_x').to_csv("Result_RF/train_x.csv")
pd.DataFrame(train_y, columns='train_y').to_csv("Result_RF/train_y.csv")
pd.DataFrame(test_y, columns='test_y').to_csv("Result_RF/test_y.csv")
pd.DataFrame(y_pred, columns='y_pred').to_csv("Result_RF/y_pred.csv")
pd.DataFrame(y_ext, columns='y_ext').to_csv("Result_RF/y_SVR_ext.csv")
197/351:
pd.DataFrame(train_x, columns=['train_x']).to_csv("Result_RF/x_train.csv")
pd.DataFrame(test_x, columns='test_x').to_csv("Result_RF/x_test.csv")
pd.DataFrame(train_x, columns='train_x').to_csv("Result_RF/train_x.csv")
pd.DataFrame(train_y, columns='train_y').to_csv("Result_RF/train_y.csv")
pd.DataFrame(test_y, columns='test_y').to_csv("Result_RF/test_y.csv")
pd.DataFrame(y_pred, columns='y_pred').to_csv("Result_RF/y_pred.csv")
pd.DataFrame(y_ext, columns='y_ext').to_csv("Result_RF/y_SVR_ext.csv")
197/352:
pd.DataFrame(train_x).to_csv("Result_RF/x_train.csv")
pd.DataFrame(test_x).to_csv("Result_RF/x_test.csv")
pd.DataFrame(train_x).to_csv("Result_RF/train_x.csv")
pd.DataFrame(train_y).to_csv("Result_RF/train_y.csv")
pd.DataFrame(test_y).to_csv("Result_RF/test_y.csv")
pd.DataFrame(y_pred).to_csv("Result_RF/y_pred.csv")
pd.DataFrame(y_ext).to_csv("Result_RF/y_SVR_ext.csv")
197/353: pd.DataFrame(train_x, columns=['train_x']).to_csv("Result_RF/x_train.csv")
197/354: pd.DataFrame(train_x, columns=['x']).to_csv("Result_RF/x_train.csv")
197/355:
pd.DataFrame(train_x).to_csv("Result/x_train.csv")
pd.DataFrame(test_x).to_csv("Result/x_test.csv")
pd.DataFrame(y_cv).to_csv("Result/y_cv.csv")
pd.DataFrame(train_y).to_csv("Result/train_y.csv")
pd.DataFrame(test_y).to_csv("Result/test_y.csv")
pd.DataFrame(y_pred).to_csv("Result/y_pred.csv")
pd.DataFrame(y_ext).to_csv("Result/y_SVR_ext.csv")
197/356:
#not reduce
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.LogKpu.tolist()
197/357: fingerprint_to_model.shape
197/358:
train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
197/359: train_x.shape
197/360:
# Set model parameter for random forest regression
param = {
    "n_estimators": 1000,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/361:
# higher Features reduced performance?
# yes
197/362:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/363:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model.shape
197/364:
import random
SEED = random.seed(10)
SEED
197/365:
train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
197/366: train_x.shape
197/367:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/368:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/369:
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='black')
197/370:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/371:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/372:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/373:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/374:
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='black')
197/375:
import random
SEED = random.seed(10)
SEED
197/376:
train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
197/377:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
197/378:
train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
197/379:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/380:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/381:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/382:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/383:
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='black')
197/384:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/385:
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='black')
197/386:
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='black')
plt.scatter(train_y, y_pred, c='black')
197/387:
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='black')
plt.scatter(train_y, y_pred, c='blue')
197/388:
pd.DataFrame(train_x).to_csv("Result/x_train.csv")
pd.DataFrame(test_x).to_csv("Result/x_test.csv")
pd.DataFrame(y_cv).to_csv("Result/y_cv.csv")
pd.DataFrame(train_y).to_csv("Result/train_y.csv")
pd.DataFrame(test_y).to_csv("Result/test_y.csv")
pd.DataFrame(y_pred).to_csv("Result/y_pred.csv")
pd.DataFrame(y_ext).to_csv("Result/y_SVR_ext.csv")
197/389:
from sklearn.inspection import permutation_importance

result = permutation_importance(SVR, test_x, y_ext, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()

fig, ax = plt.subplots()
ax.boxplot(result.importances_mean[sorted_idx].T,
           vert=False, labels= fingerprint_to_model.columns[sorted_idx])
ax.set_title("Permutation Importances (train set)")
fig.tight_layout()
fig.set_size_inches(18.5, 25)
plt.show()
197/390:
from sklearn.inspection import permutation_importance

result = permutation_importance(SVR, test_x, y_ext, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()

fig, ax = plt.subplots()
ax.boxplot(result.importances_mean[sorted_idx].T,
           vert=False, labels=fingerprint_to_model.columns[sorted_idx])
ax.set_title("Permutation Importances (train set)")
fig.tight_layout()
fig.set_size_inches(18.5, 25)
plt.show()
197/391:
pd.DataFrame(train_x, columns='t').to_csv("Result/x_train.csv")
pd.DataFrame(test_x).to_csv("Result/x_test.csv")
pd.DataFrame(y_cv).to_csv("Result/y_cv.csv")
pd.DataFrame(train_y).to_csv("Result/train_y.csv")
pd.DataFrame(test_y).to_csv("Result/test_y.csv")
pd.DataFrame(y_pred).to_csv("Result/y_pred.csv")
pd.DataFrame(y_ext).to_csv("Result/y_SVR_ext.csv")
197/392:
pd.DataFrame(train_x, columns=[]'t').to_csv("Result/x_train.csv")
pd.DataFrame(test_x).to_csv("Result/x_test.csv")
pd.DataFrame(y_cv).to_csv("Result/y_cv.csv")
pd.DataFrame(train_y).to_csv("Result/train_y.csv")
pd.DataFrame(test_y).to_csv("Result/test_y.csv")
pd.DataFrame(y_pred).to_csv("Result/y_pred.csv")
pd.DataFrame(y_ext).to_csv("Result/y_SVR_ext.csv")
197/393:
pd.DataFrame(train_x, columns=['t']).to_csv("Result/x_train.csv")
pd.DataFrame(test_x).to_csv("Result/x_test.csv")
pd.DataFrame(y_cv).to_csv("Result/y_cv.csv")
pd.DataFrame(train_y).to_csv("Result/train_y.csv")
pd.DataFrame(test_y).to_csv("Result/test_y.csv")
pd.DataFrame(y_pred).to_csv("Result/y_pred.csv")
pd.DataFrame(y_ext).to_csv("Result/y_SVR_ext.csv")
197/394:
pd.DataFrame(train_x).to_csv("Result/x_train.csv")
pd.DataFrame(test_x).to_csv("Result/x_test.csv")
pd.DataFrame(y_cv).to_csv("Result/y_cv.csv")
pd.DataFrame(train_y).to_csv("Result/train_y.csv")
pd.DataFrame(test_y).to_csv("Result/test_y.csv")
pd.DataFrame(y_pred).to_csv("Result/y_pred.csv")
pd.DataFrame(y_ext).to_csv("Result/y_SVR_ext.csv")
197/395:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/396:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='linear')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/397:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

param = {
    "n_estimators": 1000,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/398:
plt.scatter(test_y, y_ext, c='red')

plt.scatter(train_y, y_pred, c='blue')
197/399:
path = r'Fingerprint/'

Fp13  = pd.read_csv(path + 'mordard.csv'               , header = 0)
197/400: Fp13.shape
197/401:
df = pd.read_csv('df6_lipinski_kpu.csv')
df
197/402: name = df['compound']
197/403: name.shape
197/404:
df2 = df.rename({'compound': 'Name'}, axis=1)
df2 = df2.drop('Unnamed: 0', axis=1)
df3 = df2.set_index('Name')
df3
197/405:
LogKpu = df3[['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]
LogKpu
197/406:
Fp13 = Fp13.rename(columns={'name': 'Name'})
Fp13
197/407:

Fp13.Name = name
197/408:

Fpp13 = Fp13.drop('Name', axis=1)
Fpp13
197/409: Fpp13 = Fpp13.fillna(0)
197/410:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
197/411:

Fp13_n = normalized (Fpp13)
197/412: Fp13_n
197/413: np.any(np.isnan(Fp13_n))
197/414: Fp13_n["Name"] = Fp13.Name
197/415: Fp13_n
197/416: Fp13_n = Fp13_n.set_index('Name')
197/417: Fp13_n.head(20)
197/418: raw13  = LogKpu.merge(Fp13_n, on='Name', how='outer')
197/419: raw13
197/420: np.any(np.isnan(raw13))
197/421: raw13 =raw13[raw13.LogKpu.notna()]
197/422:

raw13.to_csv('QSAR_Kpu/Mordard.csv' , sep=',' ,index=True)
197/423: print (len(raw13))
197/424: print (len(raw13 .columns))
197/425:
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.LogKpu.tolist()
197/426: fingerprint_to_model.head(3)
197/427:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model.shape
197/428:
import random
SEED = random.seed(10)
SEED
197/429:
train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
197/430:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
197/431:
# Set model parameter for random forest regression
param = {
    "n_estimators": 1000,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/432:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

param = {
    "n_estimators": 1000,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/433:
plt.scatter(test_y, y_ext, c='red')

plt.scatter(train_y, y_pred, c='blue')
197/434:
import random
SEED = random.seed(10)
SEED
197/435:
train_x, test_x, train_y, test_y = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
197/436:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
197/437:
# Set model parameter for random forest regression
param = {
    "n_estimators": 1000,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/438:
# Set model parameter for random forest regression
param = {
    "n_estimators": 100,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/439:
plt.scatter(test_y, y_ext, c='red')

plt.scatter(train_y, y_pred, c='blue')
197/440:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

param = {
    "n_estimators": 1000,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/441: # SVR_linear_model
197/442:
plt.scatter(test_y, y_ext, c='red')

plt.scatter(train_y, y_pred, c='blue')
197/443:
pd.DataFrame(train_x, columns=['train_x']).to_csv("Result_RF/x_train.csv")
pd.DataFrame(test_x, columns='test_x').to_csv("Result_RF/x_test.csv")
pd.DataFrame(train_x, columns='train_x').to_csv("Result_RF/train_x.csv")
pd.DataFrame(train_y, columns='train_y').to_csv("Result_RF/train_y.csv")
pd.DataFrame(test_y, columns='test_y').to_csv("Result_RF/test_y.csv")
pd.DataFrame(y_pred, columns='y_pred').to_csv("Result_RF/y_pred.csv")
pd.DataFrame(y_ext, columns='y_ext').to_csv("Result_RF/y_SVR_ext.csv")
197/444:
pd.DataFrame(train_x).to_csv("Result_RF/x_train.csv")
pd.DataFrame(test_x).to_csv("Result_RF/x_test.csv")
pd.DataFrame(y_cv).to_csv("Result_RF/y_cv.csv")
pd.DataFrame(train_y).to_csv("Result_RF/train_y.csv")
pd.DataFrame(test_y).to_csv("Result_RF/test_y.csv")
pd.DataFrame(y_pred).to_csv("Result_RF/y_pred.csv")
pd.DataFrame(y_ext).to_csv("Result_RF/y_SVR_ext.csv")
197/445: X = pd.DataFrame(train_x)
197/446: X
197/447: fingerprint_to_model.head(3)
197/448:
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.LogKpu.tolist()
197/449: fingerprint_to_model.head(3)
197/450:
from sklearn.inspection import permutation_importance

result = permutation_importance(SVR, test_x, y_ext, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()

fig, ax = plt.subplots()
ax.boxplot(result.importances_mean[sorted_idx].T,
           vert=False, labels=fingerprint_to_model.columns[sorted_idx])
ax.set_title("Permutation Importances (train set)")
fig.tight_layout()
fig.set_size_inches(18.5, 25)
plt.show()
197/451: train_x
197/452: X = pd.DataFrame(train_x, index = True)
197/453: X = pd.DataFrame(train_x)
197/454: train_x
197/455: fingerprint_to_model
197/456: X_train, X_test = X[train_index], X[test_index]
197/457: train_x.index
197/458: # RF_model
197/459:
train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
197/460: indices = fingerprint_to_model.index
197/461:
train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indicestest_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
197/462:
train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
197/463: indices_train, indices_test
197/464:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
197/465: fingerprint_to_model.shape
197/466:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model.shape
197/467:
import random
SEED = random.seed(10)
SEED
197/468: indices = fingerprint_to_model.index
197/469: indices = fingerprint_to_model.index
197/470: fingerprint_to_model.head(3)
197/471:
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.LogKpu.tolist()
197/472: fingerprint_to_model.head(3)
197/473: indices = fingerprint_to_model.index
197/474:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model.shape
197/475:
import random
SEED = random.seed(10)
SEED
197/476:
train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
197/477: indices_train, indices_test
197/478:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
197/479:
# Set model parameter for random forest regression
param = {
    "n_estimators": 100,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/480:
# Set model parameter for random forest regression
param = {
    "n_estimators": 100,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/481:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/482:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/483:
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='black')
plt.scatter(train_y, y_pred, c='blue')
197/484:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/485:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
197/486:
# Set model parameter for random forest regression
param = {
    "n_estimators": 100,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/487:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

param = {
    "n_estimators": 100,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/488:
plt.scatter(test_y, y_ext, c='red')

plt.scatter(train_y, y_pred, c='blue')
197/489:
plt.scatter(test_y, y_ext, c='red')

plt.scatter(train_y, y_cv, c='blue')
197/490:
plt.scatter(test_y, y_ext, c='red')

plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='blue')
197/491:
plt.scatter(test_y, y_ext, c='red')

plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
197/492:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/493:
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='black')
plt.scatter(train_y, y_pred, c='blue')
197/494:
from sklearn.inspection import permutation_importance

result = permutation_importance(SVR, test_x, y_ext, n_repeats=10,
                                random_state=42, n_jobs=2)
sorted_idx = result.importances_mean.argsort()

fig, ax = plt.subplots()
ax.boxplot(result.importances_mean[sorted_idx].T,
           vert=False, labels=fingerprint_to_model.columns[sorted_idx])
ax.set_title("Permutation Importances (train set)")
fig.tight_layout()
fig.set_size_inches(18.5, 25)
plt.show()
197/495:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='linear')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/496:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

param = {
    "n_estimators": 100,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/497:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

param = {
    "n_estimators": 100,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/498:
plt.scatter(test_y, y_ext, c='red')

plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
197/499:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

param = {
    "n_estimators": 100,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/500:
plt.scatter(test_y, y_ext, c='red')

plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
197/501: train_x.index
197/502: fingerprint_to_model
197/503: train_x.shape
197/504:
#not reduce
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.LogKpu.tolist()
197/505: fingerprint_to_model
197/506:
train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
197/507:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/508:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/509:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/510:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

param = {
    "n_estimators": 100,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/511: # non reduce RF
197/512:
plt.scatter(test_y, y_ext, c='red')

plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
197/513:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model.shape
197/514:
import random
SEED = random.seed(10)
SEED
197/515:
train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
197/516: indices_train, indices_test
197/517:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
197/518:
# Set model parameter for random forest regression
param = {
    "n_estimators": 100,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
197/519:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/520:
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='black')
plt.scatter(train_y, y_pred, c='blue')
197/521:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='linear')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/522:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

param = {
    "n_estimators": 100,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/523:
plt.scatter(test_y, y_ext, c='red')

plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
197/524:
pd.DataFrame(train_x).to_csv("Result_RF/x_train.csv")
pd.DataFrame(test_x).to_csv("Result_RF/x_test.csv")
pd.DataFrame(y_cv).to_csv("Result_RF/y_cv.csv")
pd.DataFrame(train_y).to_csv("Result_RF/train_y.csv")
pd.DataFrame(test_y).to_csv("Result_RF/test_y.csv")
pd.DataFrame(y_pred).to_csv("Result_RF/y_pred.csv")
pd.DataFrame(y_ext).to_csv("Result_RF/y_SVR_ext.csv")
197/525:
pd.DataFrame(train_x).to_csv("Result/x_train.csv")
pd.DataFrame(test_x).to_csv("Result/x_test.csv")
pd.DataFrame(y_cv).to_csv("Result/y_cv.csv")
pd.DataFrame(train_y).to_csv("Result/train_y.csv")
pd.DataFrame(test_y).to_csv("Result/test_y.csv")
pd.DataFrame(y_pred).to_csv("Result/y_pred.csv")
pd.DataFrame(y_ext).to_csv("Result/y_SVR_ext.csv")
197/526:
pd.DataFrame(train_x, indices_train).to_csv("Result/x_train.csv")
pd.DataFrame(test_x).to_csv("Result/x_test.csv")
pd.DataFrame(y_cv).to_csv("Result/y_cv.csv")
pd.DataFrame(train_y).to_csv("Result/train_y.csv")
pd.DataFrame(test_y).to_csv("Result/test_y.csv")
pd.DataFrame(y_pred).to_csv("Result/y_pred.csv")
pd.DataFrame(y_ext).to_csv("Result/y_SVR_ext.csv")
197/527:
pd.DataFrame(train_x, indices_train).to_csv("Result/x_train.csv")
pd.DataFrame(test_x, indices_test).to_csv("Result/x_test.csv")
pd.DataFrame(y_cv, indices_train).to_csv("Result/y_cv.csv")
pd.DataFrame(train_y, indices_train).to_csv("Result/train_y.csv")
pd.DataFrame(test_y).to_csv("Result/test_y.csv")
pd.DataFrame(y_pred, indices_train).to_csv("Result/y_pred.csv")
pd.DataFrame(y_ext, indices_test).to_csv("Result/y_SVR_ext.csv")
197/528:
pd.DataFrame(train_x, indices_train).to_csv("Result/x_train.csv")
pd.DataFrame(test_x, indices_test).to_csv("Result/x_test.csv")
pd.DataFrame(y_cv, indices_train).to_csv("Result/y_cv.csv")
pd.DataFrame(train_y, indices_train).to_csv("Result/train_y.csv")
pd.DataFrame(test_y, indices_test).to_csv("Result/test_y.csv")
pd.DataFrame(y_pred, indices_train).to_csv("Result/y_pred.csv")
pd.DataFrame(y_ext, indices_test).to_csv("Result/y_SVR_ext.csv")
197/529:
pd.DataFrame(train_x, indices_train).to_csv("Result_RF/x_train.csv")
pd.DataFrame(test_x, indices_test).to_csv("Result_RF/x_test.csv")
pd.DataFrame(y_cv, indices_train).to_csv("Result_RF/y_cv.csv")
pd.DataFrame(train_y, indices_train).to_csv("Result_RF/train_y.csv")
pd.DataFrame(test_y, indices_test).to_csv("Result_RF/test_y.csv")
pd.DataFrame(y_pred, indices_train).to_csv("Result_RF/y_pred.csv")
pd.DataFrame(y_ext, indices_test).to_csv("Result_RF/y_SVR_ext.csv")
197/530:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/531:
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='black')
plt.scatter(train_y, y_pred, c='blue')
197/532:
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='black')
plt.scatter(train_y, y_pred, c='blue')
197/533:
pd.DataFrame(train_x, indices_train).to_csv("Result/x_train.csv")
pd.DataFrame(test_x, indices_test).to_csv("Result/x_test.csv")
pd.DataFrame(y_cv, indices_train).to_csv("Result/y_cv.csv")
pd.DataFrame(train_y, indices_train).to_csv("Result/train_y.csv")
pd.DataFrame(test_y, indices_test).to_csv("Result/test_y.csv")
pd.DataFrame(y_pred, indices_train).to_csv("Result/y_pred.csv")
pd.DataFrame(y_ext, indices_test).to_csv("Result/y_SVR_ext.csv")
197/534:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

param = {
    "n_estimators": 100,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/535:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

param = {
    "n_estimators": 100,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/536:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

param = {
    "n_estimators": 100,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/537:
plt.scatter(test_y, y_ext, c='red')

plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
197/538:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

param = {
    "n_estimators": 100,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/539:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

param = {
    "n_estimators": 200,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
197/540:
plt.scatter(test_y, y_ext, c='red')

plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
197/541:
pd.DataFrame(train_x, indices_train).to_csv("Result_RF/x_train.csv")
pd.DataFrame(test_x, indices_test).to_csv("Result_RF/x_test.csv")
pd.DataFrame(y_cv, indices_train).to_csv("Result_RF/y_cv.csv")
pd.DataFrame(train_y, indices_train).to_csv("Result_RF/train_y.csv")
pd.DataFrame(test_y, indices_test).to_csv("Result_RF/test_y.csv")
pd.DataFrame(y_pred, indices_train).to_csv("Result_RF/y_pred.csv")
pd.DataFrame(y_ext, indices_test).to_csv("Result_RF/y_SVR_ext.csv")
207/1:
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
207/2:
path = r'Fingerprint/'

Fp13  = pd.read_csv(path + 'mordard.csv'               , header = 0)
207/3: Fp13.shape
207/4:
df = pd.read_csv('df6_lipinski_kpu.csv')
df
207/5: name = df['compound']
207/6: name.shape
207/7:
df2 = df.rename({'compound': 'Name'}, axis=1)
df2 = df2.drop('Unnamed: 0', axis=1)
df3 = df2.set_index('Name')
df3
207/8:
LogKpu = df3[['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]
LogKpu
207/9:
Fp13 = Fp13.rename(columns={'name': 'Name'})
Fp13
207/10: Fpp13 = Fpp13.fillna(0)
207/11:

Fpp13 = Fp13.drop('Name', axis=1)
Fpp13
207/12: Fpp13 = Fpp13.fillna(0)
207/13:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
207/14:

Fp13_n = normalized (Fpp13)
207/15: Fp13_n
207/16: np.any(np.isnan(Fp13_n))
207/17: Fp13_n["Name"] = Fp13.Name
207/18: Fp13_n
207/19: Fp13_n = Fp13_n.set_index('Name')
207/20: Fp13_n.head(20)
207/21: Fp13_n.head(20)
207/22:
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
207/23:
path = r'Fingerprint/'

Fp13  = pd.read_csv(path + 'mordard.csv'               , header = 0)
207/24: Fp13.shape
207/25:
df = pd.read_csv('df6_lipinski_kpu.csv')
df
207/26: name = df['compound']
207/27: name.shape
207/28:
df2 = df.rename({'compound': 'Name'}, axis=1)
df2 = df2.drop('Unnamed: 0', axis=1)
df3 = df2.set_index('Name')
df3
207/29:
LogKpu = df3[['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]
LogKpu
207/30:
Fp13 = Fp13.rename(columns={'name': 'Name'})
Fp13
207/31: Fp13
207/32:
df = pd.read_csv('df6_lipinski_kpu.csv')
df
207/33: name = df['compound']
207/34: name.shape
207/35:
df2 = df.rename({'compound': 'Name'}, axis=1)
df2 = df2.drop('Unnamed: 0', axis=1)
df3 = df2.set_index('Name')
df3
207/36:
Fp13 = Fp13.rename(columns={'name': 'Name'})
Fp13
207/37:

Fp13.Name = name
207/38:

Fpp13 = Fp13.drop('Name', axis=1)
Fpp13
207/39: Fpp13 = Fpp13.fillna(0)
207/40:

Fp13_n = normalized (Fpp13)
207/41: Fp13_n
207/42: np.any(np.isnan(Fp13_n))
207/43: Fp13_n["Name"] = Fp13.Name
207/44: Fp13_n
207/45: Fp13_n = Fp13_n.set_index('Name')
207/46: Fp13_n.head(20)
207/47: raw13  = LogKpu.merge(Fp13_n, on='Name', how='outer')
207/48: raw13
207/49: np.any(np.isnan(raw13))
207/50: raw13 =raw13[raw13.LogKpu.notna()]
207/51: print (len(raw13))
207/52: print (len(raw13 .columns))
207/53:
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.LogKpu.tolist()
207/54: fingerprint_to_model.head(3)
207/55: indices = fingerprint_to_model.index
207/56:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model.shape
207/57:
import random
SEED = random.seed(10)
SEED
207/58:
train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
207/59: indices_train, indices_test
207/60:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
207/61:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

param = {
    "n_estimators": 200,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
207/62:
plt.scatter(test_y, y_ext, c='red')

plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
207/63:
plt.scatter(test_y, y_ext, c='red')

plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
207/64:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

param = {
    "n_estimators": 200,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
207/65:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

param = {
    "n_estimators": 200,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
207/66:
plt.scatter(test_y, y_ext, c='red')

plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
207/67:
result = permutation_importance(
    model_RF, test_x, test_y, n_repeats=kf, random_state=SEED, n_jobs=2)

forest_importances = pd.Series(result.importances_mean, index=feature_names)
207/68:
from sklearn.inspection import permutation_importance
result = permutation_importance(
    model_RF, test_x, test_y, n_repeats=kf, random_state=SEED, n_jobs=2)
feature_names = [f'feature {i}' for i in range(fingerprint_to_model.shape[1])]

forest_importances = pd.Series(result.importances_mean, index=feature_names)
207/69:
from sklearn.inspection import permutation_importance
result = permutation_importance(model_RF, test_x, test_y, n_repeats=10, random_state=SEED, n_jobs=2)
feature_names = [f'feature {i}' for i in range(fingerprint_to_model.shape[1])]

forest_importances = pd.Series(result.importances_mean, index=feature_names)
207/70:
from sklearn.inspection import permutation_importance
result = permutation_importance(model_RF, test_x, test_y, n_repeats=10, random_state=SEED, n_jobs=2)
feature_names = [f'feature {i}' for i in range(fingerprint_to_model.shape[1])]

forest_importances = pd.Series(result.importances_mean, index=feature_names)
fig, ax = plt.subplots()
forest_importances.plot.bar(yerr=result.importances_std, ax=ax)
ax.set_title("Feature importances using permutation on full model")
ax.set_ylabel("Mean accuracy decrease")
fig.tight_layout()
plt.show()
211/1:
from sklearn.inspection import permutation_importance
result = permutation_importance(model_RF, test_x, test_y, n_repeats=10, random_state=SEED, n_jobs=2)
feature_names = [f'feature {i}' for i in range(fingerprint_to_model.shape[1])]

forest_importances = pd.Series(result.importances_mean, index=feature_names)
fig, ax = plt.subplots()
forest_importances.plot.bar(yerr=result.importances_std, ax=ax)
ax.set_title("Feature importances using permutation on full model")
ax.set_ylabel("Mean accuracy decrease")
fig.tight_layout()
plt.show()
211/2:
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
211/3:
path = r'Fingerprint/'

Fp13  = pd.read_csv(path + 'mordard.csv'               , header = 0)
211/4: Fp13.shape
211/5: Fp13
211/6:
df = pd.read_csv('df6_lipinski_kpu.csv')
df
211/7: name = df['compound']
211/8: name.shape
211/9:
df2 = df.rename({'compound': 'Name'}, axis=1)
df2 = df2.drop('Unnamed: 0', axis=1)
df3 = df2.set_index('Name')
df3
211/10:
LogKpu = df3[['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]
LogKpu
211/11:
Fp13 = Fp13.rename(columns={'name': 'Name'})
Fp13
211/12:

Fp13.Name = name
211/13:

Fpp13 = Fp13.drop('Name', axis=1)
Fpp13
211/14: Fpp13 = Fpp13.fillna(0)
211/15:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
211/16:

Fp13_n = normalized (Fpp13)
211/17: Fp13_n
211/18: np.any(np.isnan(Fp13_n))
211/19: Fp13_n["Name"] = Fp13.Name
211/20: Fp13_n
211/21: Fp13_n = Fp13_n.set_index('Name')
211/22: Fp13_n.head(20)
211/23: raw13  = LogKpu.merge(Fp13_n, on='Name', how='outer')
211/24: raw13
211/25: np.any(np.isnan(raw13))
211/26: raw13 =raw13[raw13.LogKpu.notna()]
211/27: print (len(raw13))
211/28: print (len(raw13 .columns))
211/29:
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.LogKpu.tolist()
211/30: fingerprint_to_model.head(3)
211/31: indices = fingerprint_to_model.index
211/32:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model.shape
211/33:
import random
SEED = random.seed(10)
SEED
211/34:
train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=SEED)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
211/35: indices_train, indices_test
211/36:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
211/37:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/38:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/39:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/40:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/41:
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='black')
plt.scatter(train_y, y_pred, c='blue')
211/42:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/43:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/44:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/45:
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='black')
plt.scatter(train_y, y_pred, c='blue')
211/46:
pd.DataFrame(train_x, indices_train).to_csv("Result/x_train.csv")
pd.DataFrame(test_x, indices_test).to_csv("Result/x_test.csv")
pd.DataFrame(y_cv, indices_train).to_csv("Result/y_cv.csv")
pd.DataFrame(train_y, indices_train).to_csv("Result/train_y.csv")
pd.DataFrame(test_y, indices_test).to_csv("Result/test_y.csv")
pd.DataFrame(y_pred, indices_train).to_csv("Result/y_pred.csv")
pd.DataFrame(y_ext, indices_test).to_csv("Result/y_SVR_ext.csv")
211/47:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

param = {
    "n_estimators": 200,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/48:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

param = {
    "n_estimators": 200,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/49:
plt.scatter(test_y, y_ext, c='red')

plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
211/50:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

param = {
    "n_estimators": 200,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/51:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

param = {
    "n_estimators": 200,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/52:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

param = {
    "n_estimators": 200,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/53:
train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
211/54: indices_train, indices_test
211/55:
train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
211/56: indices_train, indices_test
211/57:
train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
211/58:
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.LogKpu.tolist()
211/59: fingerprint_to_model.head(3)
211/60: indices = fingerprint_to_model.index
211/61:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model.shape
211/62:
train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
211/63: indices_train, indices_test
211/64:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/65:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/66:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/67:
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='black')
plt.scatter(train_y, y_pred, c='blue')
211/68:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/69:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/70:
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='black')
plt.scatter(train_y, y_pred, c='blue')
211/71:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='linear')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/72:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

param = {
    "n_estimators": 200,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/73:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

param = {
    "n_estimators": 200,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/74:
plt.scatter(test_y, y_ext, c='red')

plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
211/75:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

param = {
    "n_estimators": 200,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/76:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/77: indices_train, indices_test
211/78:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
211/79: fingerprint_to_model
211/80: fingerprint_to_model.shape
211/81:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/82:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/83:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/84:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/85:
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='black')
plt.scatter(train_y, y_pred, c='blue')
211/86:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='linear')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/87:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

param = {
    "n_estimators": 200,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/88:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

param = {
    "n_estimators": 200,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/89:
plt.scatter(test_y, y_ext, c='red')

plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
211/90:
plt.scatter(test_y, y_ext, c='red')

plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
211/91:
pd.DataFrame(train_x, indices_train).to_csv("Result_RF/x_train.csv")
pd.DataFrame(test_x, indices_test).to_csv("Result_RF/x_test.csv")
pd.DataFrame(y_cv, indices_train).to_csv("Result_RF/y_cv.csv")
pd.DataFrame(train_y, indices_train).to_csv("Result_RF/train_y.csv")
pd.DataFrame(test_y, indices_test).to_csv("Result_RF/test_y.csv")
pd.DataFrame(y_pred, indices_train).to_csv("Result_RF/y_pred.csv")
pd.DataFrame(y_ext, indices_test).to_csv("Result_RF/y_SVR_ext.csv")
211/92:
from sklearn.inspection import permutation_importance
result = permutation_importance(model_RF, test_x, test_y, n_repeats=10, random_state=SEED, n_jobs=2)
feature_names = [f'feature {i}' for i in range(fingerprint_to_model.shape[1])]

forest_importances = pd.Series(result.importances_mean, index=feature_names)
fig, ax = plt.subplots()
forest_importances.plot.bar(yerr=result.importances_std, ax=ax)
ax.set_title("Feature importances using permutation on full model")
ax.set_ylabel("Mean accuracy decrease")
fig.tight_layout()
plt.show()
211/93: forest_importances
211/94:
from sklearn.inspection import permutation_importance
result = permutation_importance(model_RF, test_x, test_y, n_repeats=10, random_state=42, n_jobs=2)
feature_names = [f'feature {i}' for i in range(fingerprint_to_model.shape[1])]

forest_importances = pd.Series(result.importances_mean, index=feature_names)
fig, ax = plt.subplots()
forest_importances.plot.bar(yerr=result.importances_std, ax=ax)
ax.set_title("Feature importances using permutation on full model")
ax.set_ylabel("Mean accuracy decrease")
fig.tight_layout()
plt.show()
211/95:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

param = {
    "n_estimators": 200,  # number of trees to grows
    'random'
}
model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/96:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/97:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/98:
plt.scatter(test_y, y_ext, c='red')

plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
211/99:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/100:
plt.scatter(test_y, y_ext, c='red')

plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
211/101:
pd.DataFrame(train_x, indices_train).to_csv("Result_RF/x_train.csv")
pd.DataFrame(test_x, indices_test).to_csv("Result_RF/x_test.csv")
pd.DataFrame(y_cv, indices_train).to_csv("Result_RF/y_cv.csv")
pd.DataFrame(train_y, indices_train).to_csv("Result_RF/train_y.csv")
pd.DataFrame(test_y, indices_test).to_csv("Result_RF/test_y.csv")
pd.DataFrame(y_pred, indices_train).to_csv("Result_RF/y_pred.csv")
pd.DataFrame(y_ext, indices_test).to_csv("Result_RF/y_SVR_ext.csv")
211/102:
from sklearn.inspection import permutation_importance
result = permutation_importance(model_RF, test_x, test_y, n_repeats=10, random_state=42, n_jobs=2)
feature_names = [f'feature {i}' for i in range(fingerprint_to_model.shape[1])]

forest_importances = pd.Series(result.importances_mean, index=feature_names)
fig, ax = plt.subplots()
forest_importances.plot.bar(yerr=result.importances_std, ax=ax)
ax.set_title("Feature importances using permutation on full model")
ax.set_ylabel("Mean accuracy decrease")
fig.tight_layout()
plt.show()
211/103: forest_importances
211/104: result
211/105: feature_names
211/106:
#Train features
from sklearn.inspection import permutation_importance
result = permutation_importance(model_RF, train_x, train_y, n_repeats=10, random_state=42, n_jobs=2)
feature_names = [f'feature {i}' for i in range(fingerprint_to_model.shape[1])]

forest_importances = pd.Series(result.importances_mean, index=feature_names)
fig, ax = plt.subplots()
forest_importances.plot.bar(yerr=result.importances_std, ax=ax)
ax.set_title("Feature importances using permutation on full model")
ax.set_ylabel("Mean accuracy decrease")
fig.tight_layout()
plt.show()
211/107: forest_importances
211/108: FI_mean = pd.DataFrame(forest_importances).to_csv('forest_importances.csv')
211/109: pd.DataFrame(forest_importances).to_csv('forest_importances.csv')
211/110: FI_mean = pd.DataFrame(forest_importances)
211/111:
FI_mean = pd.DataFrame(forest_importances)
FI_mean.to_csv('forest_importances.csv')
211/112:
FI_mean = pd.DataFrame(forest_importances)
FI_mean.to_csv('forest_importances.csv')
FI_mean
211/113:
FI_mean = pd.DataFrame(forest_importances, columns=['Feature', 'Mean'])
FI_mean.to_csv('forest_importances.csv')
FI_mean
211/114:
FI_mean = pd.DataFrame(forest_importances, head=['Feature', 'Mean'])
FI_mean.to_csv('forest_importances.csv')
FI_mean
211/115:
FI_mean = pd.DataFrame(forest_importances, head=['Mean'])
FI_mean.to_csv('forest_importances.csv')
FI_mean
211/116:
FI_mean = pd.DataFrame(forest_importances)
FI_mean.to_csv('forest_importances.csv')
FI_mean
211/117:
FI_mean = pd.DataFrame(forest_importances)
FI_mean.to_csv('forest_importances_RF.csv')
FI_mean
211/118: fingerprint_to_model.shape[1]
211/119: fingerprint_to_model
211/120: fingerprint_to_model_header = pd.DataFrame(fingerprint_to_model)
211/121:
fingerprint_to_model_header = pd.DataFrame(fingerprint_to_model)
fingerprint_to_model_header
211/122:
FI_RF_mean = pd.DataFrame(forest_importances)
FI_RF_mean.to_csv('forest_importances_RF.csv')
FI_RF_mean
211/123:
FI_RF_train_mean = pd.DataFrame(forest_importances)
FI_RF_train_mean.to_csv('forest_importances_train_RF.csv')
FI_RF_train_mean
211/124:
#Test features
from sklearn.inspection import permutation_importance
result = permutation_importance(model_RF, test_x, test_y, n_repeats=10, random_state=42, n_jobs=2)
feature_names = [f'feature {i}' for i in range(fingerprint_to_model.shape[1])]

forest_importances = pd.Series(result.importances_mean, index=feature_names)
fig, ax = plt.subplots()
forest_importances.plot.bar(yerr=result.importances_std, ax=ax)
ax.set_title("Feature importances using permutation on full model")
ax.set_ylabel("Mean accuracy decrease")
fig.tight_layout()
plt.show()
211/125:
FI_RF_test_mean = pd.DataFrame(forest_importances)
FI_RF_test_mean.to_csv('forest_importances_test_RF.csv')
FI_RF_test_mean
211/126:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/127:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/128:
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='black')
plt.scatter(train_y, y_pred, c='blue')
211/129:
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='black')
plt.scatter(train_y, y_pred, c='blue')
211/130:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/131:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/132:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/133:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/134:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/135:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='linear')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/136:
from sklearn.cross_decomposition import PLSRegression
model_PLS = PLSRegression(n_components=10)
scores=cross_val_score(model_PLS, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
model_fit = model_PLS.fit(train_x, train_y)
y_pred = model_PLS.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_PLS, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_PLS.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
211/137:
from sklearn.cross_decomposition import PLSRegression
model_PLS = PLSRegression(n_components=10)
scores=cross_val_score(model_PLS, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
model_fit = model_PLS.fit(train_x, train_y)
y_pred = model_PLS.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_PLS, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_PLS.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
211/138:
from sklearn.cross_decomposition import PLSRegression
model_PLS = PLSRegression(n_components=7)
scores=cross_val_score(model_PLS, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
model_fit = model_PLS.fit(train_x, train_y)
y_pred = model_PLS.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_PLS, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_PLS.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
211/139:
from sklearn.cross_decomposition import PLSRegression

model_PLS = PLSRegression(n_components=7)
scores=cross_val_score(model_PLS, train_x, train_y, cv=kf)

#calculate R2
model_fit = model_PLS.fit(train_x, train_y)
y_pred = model_PLS.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_PLS, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_PLS.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
211/140:
from sklearn.cross_decomposition import PLSRegression

model_PLS = PLSRegression(n_components=10)
scores=cross_val_score(model_PLS, train_x, train_y, cv=kf)

#calculate R2
model_fit = model_PLS.fit(train_x, train_y)
y_pred = model_PLS.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_PLS, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_PLS.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
211/141:
from sklearn.linear_model import Ridge
Ridge = Ridge(alpha=1.0)

#calculate R2
model_fit = Ridge.fit(train_x, train_y)
y_pred = Ridge.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(Ridge, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = Ridge.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
218/1:
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
218/2:
path = r'Fingerprint/'

Fp13  = pd.read_csv(path + 'mordard.csv'               , header = 0)
218/3: Fp13.shape
218/4: Fp13
218/5:
df = pd.read_csv('df6_lipinski_kpu.csv')
df
218/6: name = df['compound']
218/7: name.shape
218/8:
df2 = df.rename({'compound': 'Name'}, axis=1)
df2 = df2.drop('Unnamed: 0', axis=1)
df3 = df2.set_index('Name')
df3
218/9:
LogKpu = df3[['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]
LogKpu
218/10:
Fp13 = Fp13.rename(columns={'name': 'Name'})
Fp13
218/11:

Fp13.Name = name
218/12:

Fpp13 = Fp13.drop('Name', axis=1)
Fpp13
218/13: Fpp13 = Fpp13.fillna(0)
218/14:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
218/15:

Fp13_n = normalized (Fpp13)
218/16: Fp13_n
218/17: np.any(np.isnan(Fp13_n))
218/18: Fp13_n["Name"] = Fp13.Name
218/19: Fp13_n
218/20: Fp13_n = Fp13_n.set_index('Name')
218/21: Fp13_n.head(20)
218/22: raw13  = LogKpu.merge(Fp13_n, on='Name', how='outer')
218/23: raw13
218/24: np.any(np.isnan(raw13))
218/25: raw13 =raw13[raw13.LogKpu.notna()]
218/26: print (len(raw13))
218/27: print (len(raw13 .columns))
218/28:
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.LogKpu.tolist()
218/29: fingerprint_to_model.head(3)
218/30: indices = fingerprint_to_model.index
218/31:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model.shape
218/32:
import random
SEED = random.seed(10)
SEED
218/33:
train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
218/34: indices_train, indices_test
218/35:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
218/36:
# Set model parameter for random forest regression
param = {
    "n_estimators": 100,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
218/37:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
218/38:
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='black')
plt.scatter(train_y, y_pred, c='blue')
218/39:
pd.DataFrame(train_x, indices_train).to_csv("Result/x_train.csv")
pd.DataFrame(test_x, indices_test).to_csv("Result/x_test.csv")
pd.DataFrame(y_cv, indices_train).to_csv("Result/y_cv.csv")
pd.DataFrame(train_y, indices_train).to_csv("Result/train_y.csv")
pd.DataFrame(test_y, indices_test).to_csv("Result/test_y.csv")
pd.DataFrame(y_pred, indices_train).to_csv("Result/y_pred.csv")
pd.DataFrame(y_ext, indices_test).to_csv("Result/y_SVR_ext.csv")
218/40:
pd.DataFrame(train_x, indices_train).to_csv("Result/x_train.csv")
pd.DataFrame(test_x, indices_test).to_csv("Result/x_test.csv")
pd.DataFrame(y_cv, indices_train).to_csv("Result/y_cv.csv")
pd.DataFrame(train_y, indices_train).to_csv("Result/train_y.csv")
pd.DataFrame(test_y, indices_test).to_csv("Result/test_y.csv")
pd.DataFrame(y_pred, indices_train).to_csv("Result/y_pred.csv")
pd.DataFrame(y_ext, indices_test).to_csv("Result/y_SVR_ext.csv")
218/41:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='linear')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
218/42:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
218/43:
plt.scatter(test_y, y_ext, c='red')

plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
218/44:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
218/45:
plt.scatter(test_y, y_ext, c='red')

plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
218/46:
plt.scatter(test_y, y_ext, c='red')

plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
218/47:
pd.DataFrame(train_x, indices_train).to_csv("Result_RF/x_train.csv")
pd.DataFrame(test_x, indices_test).to_csv("Result_RF/x_test.csv")
pd.DataFrame(y_cv, indices_train).to_csv("Result_RF/y_cv.csv")
pd.DataFrame(train_y, indices_train).to_csv("Result_RF/train_y.csv")
pd.DataFrame(test_y, indices_test).to_csv("Result_RF/test_y.csv")
pd.DataFrame(y_pred, indices_train).to_csv("Result_RF/y_pred.csv")
pd.DataFrame(y_ext, indices_test).to_csv("Result_RF/y_SVR_ext.csv")
218/48:
#Test features RF
from sklearn.inspection import permutation_importance
result = permutation_importance(model_RF, test_x, test_y, n_repeats=10, random_state=42, n_jobs=2)
feature_names = [f'feature {i}' for i in range(fingerprint_to_model.shape[1])]

forest_importances = pd.Series(result.importances_mean, index=feature_names)
fig, ax = plt.subplots()
forest_importances.plot.bar(yerr=result.importances_std, ax=ax)
ax.set_title("Feature importances using permutation on full model")
ax.set_ylabel("Mean accuracy decrease")
fig.tight_layout()
plt.show()
218/49: fingerprint_to_model
218/50:
FI_RF_test_mean = pd.DataFrame(forest_importances)
FI_RF_test_mean.to_csv('Result_RF/forest_importances_test_RF.csv')
FI_RF_test_mean
218/51:
#Train features
from sklearn.inspection import permutation_importance
result = permutation_importance(model_RF, train_x, train_y, n_repeats=10, random_state=42, n_jobs=2)
feature_names = [f'feature {i}' for i in range(fingerprint_to_model.shape[1])]

forest_importances = pd.Series(result.importances_mean, index=feature_names)
fig, ax = plt.subplots()
forest_importances.plot.bar(yerr=result.importances_std, ax=ax)
ax.set_title("Feature importances using permutation on full model")
ax.set_ylabel("Mean accuracy decrease")
fig.tight_layout()
plt.show()
218/52:
from sklearn.linear_model import Ridge
Ridge = Ridge(alpha=1.0)

#calculate R2
model_fit = Ridge.fit(train_x, train_y)
y_pred = Ridge.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(Ridge, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = Ridge.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
218/53:
from sklearn.neural_network import MLPRegressor
MLP = MLPRegressor(activation='relu' random_state=42, max_iter=1000, solver='lbfgs')


#calculate R2
model_fit = MLP.fit(train_x, train_y)
y_pred = MLP.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(MLP, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = MLP.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
218/54:
from sklearn.neural_network import MLPRegressor
MLP = MLPRegressor(activation='relu', random_state=42, max_iter=1000, solver='lbfgs')


#calculate R2
model_fit = MLP.fit(train_x, train_y)
y_pred = MLP.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(MLP, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = MLP.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
218/55:
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.LogKpu.tolist()
218/56: fingerprint_to_model.head(3)
218/57:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model.shape
218/58: fingerprint_to_model
218/59: fingerprint_to_model.shape
218/60: fingerprint_to_model.to_frame
218/61: fingerprint_to_model.to_frame()
218/62: pd.DataFrame(Ffingerprint_to_model)
218/63: pd.DataFrame(fingerprint_to_model)
218/64:
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.LogKpu.tolist()
218/65: fingerprint_to_model.head(3)
218/66: indices = fingerprint_to_model.index
218/67:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
fingerprint_to_model2 = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model.shape
218/68:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
fingerprint_to_model2 = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model2.shape
218/69: fingerprint_to_model2_df = pd.DataFrame(fingerprint_to_model2, index=indices, columns=fingerprint_to_model.columns)
218/70: fingerprint_to_model2_df = pd.DataFrame(fingerprint_to_model2, index=fingerprint_to_model.index, columns=fingerprint_to_model.columns)
218/71: fingerprint_to_model2_df = pd.DataFrame(fingerprint_to_model2, index=fingerprint_to_model.index, columns=fingerprint_to_model.columns)
218/72: fingerprint_to_model2_df = pd.DataFrame(fingerprint_to_model2, index=fingerprint_to_model2.index, columns=fingerprint_to_model.columns)
218/73:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model.shape
218/74:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_modelo_model.get_support(index=True)
218/75:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_modelo.get_support(index=True)
218/76:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model.get_support(index=True)
218/77:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05)).get_support(index=True)
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model.get_support(index=True)
218/78:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05)).get_support(index=True)
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model
218/79:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model
218/80:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
fingerprint_to_model = sel.fit_transform(fingerprint_to_model).get_support(indices=True)
fingerprint_to_model
218/81:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model
218/82:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)
fingerprint_to_model = sel.fit(fingerprint_to_model)
fingerprint_to_model
218/83:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model
218/84:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model
218/85:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model.get_support(indices=True)
218/86:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
fingerprint_to_model = sel.fit_transform(fingerprint_to_model).get_support(indices=True)
fingerprint_to_model
218/87:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05),get_support(indices=True))
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model
218/88:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05), get_support(indices=True))
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model
218/89:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model
218/90:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05))
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model


def VarianceThreshold(df, threshold):
    sel = VarianceThreshold(threshold)
    sel.fit(df)
    df = df.loc[:, sel.get_support()]
    return df
218/91:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05))
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model


def VarianceThreshold_transform(df, threshold):
    sel = VarianceThreshold(threshold)
    sel.fit(df)
    df = df.loc[:, sel.get_support()]
    return df
218/92:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05))
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model


def VarianceThreshold_transform(df, threshold):
    sel = VarianceThreshold(threshold)
    sel.fit(df)
    df = df.loc[:, sel.get_support(indices=True)]
    return df
218/93: fingerprint_to_model = VarianceThreshold_transform(fingerprint_to_model)
218/94: fingerprint_to_model = VarianceThreshold_transform(fingerprint_to_model, 0.5)
218/95: fingerprint_to_model = VarianceThreshold_transform(fingerprint_to_model, 0.05)
218/96:
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.LogKpu.tolist()
218/97: fingerprint_to_model.head(3)
218/98: fingerprint_to_model = VarianceThreshold_transform(fingerprint_to_model, 0.05)
218/99:
sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)
fingerprint_to_model = sel.fit(fingerprint_to_model)
fingerprint_to_model
218/100:
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.LogKpu.tolist()
218/101: fingerprint_to_model.head(3)
218/102:
sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)
fingerprint_to_model = sel.fit(fingerprint_to_model)
fingerprint_to_model
218/103:
sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model
218/104:
sel = VarianceThreshold(threshold=(0.05)).get_support()
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model
218/105:
sel = VarianceThreshold(threshold=(0.05))
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model
218/106:
sel = VarianceThreshold(threshold=(0.05)).get_support()
fingerprint_to_model = sel.fit(fingerprint_to_model)
fingerprint_to_model
218/107:
sel = VarianceThreshold(threshold=(0.05))
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model
218/108:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05))
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model


def VarianceThreshold_transform(df, threshold):
    sel = VarianceThreshold(threshold)
    sel.fit(df)
    df2 = df.loc[:, sel.get_support(indices=True)]
    return df2
218/109:
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.LogKpu.tolist()
218/110: VarianceThreshold_transform(fingerprint_to_model, 0.05)
218/111:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05))
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model


def VarianceThreshold_selector(data):

    #Select Model
    selector = VarianceThreshold(0.05) #Defaults to 0.0, e.g. only remove features with the same value in all samples

    #Fit the Model
    selector.fit(data)
    features = selector.get_support(indices = True) #returns an array of integers corresponding to nonremoved features
    features = [column for column in data[features]] #Array of all nonremoved features' names

    #Format and Return
    selector = pd.DataFrame(selector.transform(data))
    selector.columns = features
    return selector
218/112:
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.LogKpu.tolist()
218/113: VarianceThreshold_selector(fingerprint_to_model)
218/114:
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.LogKpu.tolist()
218/115: fingerprint_to_model.head(3)
218/116:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05))
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model


selector = VarianceThreshold(0.05) #Defaults to 0.0, e.g. only remove features with the same value in all samples
elector.fit(fingerprint_to_model)
features = selector.get_support(indices = True) #returns an array of integers corresponding to nonremoved features
features = [column for column in data[features]] #Array of all nonremoved features' names
selector_df = pd.DataFrame(selector.transform(data), columns=features)
218/117:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05))
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model


selector = VarianceThreshold(0.05) #Defaults to 0.0, e.g. only remove features with the same value in all samples
selector.fit(fingerprint_to_model)
features = selector.get_support(indices = True) #returns an array of integers corresponding to nonremoved features
features = [column for column in data[features]] #Array of all nonremoved features' names
selector_df = pd.DataFrame(selector.transform(data), columns=features)
218/118:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05))
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model


selector = VarianceThreshold(0.05) #Defaults to 0.0, e.g. only remove features with the same value in all samples
selector.fit(fingerprint_to_model)
features = selector.get_support(indices = True) #returns an array of integers corresponding to nonremoved features
features = [column for column in fingerprint_to_model[features]] #Array of all nonremoved features' names
selector_df = pd.DataFrame(selector.transform(fingerprint_to_model), columns=features)
218/119:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05))
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model


selector = VarianceThreshold(0.05) #Defaults to 0.0, e.g. only remove features with the same value in all samples
selector.fit(fingerprint_to_model)
features = selector.get_support(indices = True) #returns an array of integers corresponding to nonremoved features
features = [column for column in fingerprint_to_model[features]] #Array of all nonremoved features' names
selector_df = pd.DataFrame(selector.transform(fingerprint_to_model), columns=features)
218/120:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05))
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model


selector = VarianceThreshold(0.05) #Defaults to 0.0, e.g. only remove features with the same value in all samples
selector.fit(fingerprint_to_model)
features = selector.get_support(indices = True) #returns an array of integers corresponding to nonremoved features
selector_df = pd.DataFrame(selector.transform(fingerprint_to_model), columns=features)
218/121: selector_df
218/122:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05))
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model


selector = VarianceThreshold(0.05) #Defaults to 0.0, e.g. only remove features with the same value in all samples
selector.fit(fingerprint_to_model)
features = selector.get_support(indices = True) #returns an array of integers corresponding to nonremoved features
features = fingerprint_to_model[features] #Array of all nonremoved features' names
selector_df = pd.DataFrame(selector.transform(fingerprint_to_model), columns=features)
218/123:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05))
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model


selector = VarianceThreshold(0.05) #Defaults to 0.0, e.g. only remove features with the same value in all samples
selector.fit(fingerprint_to_model)
features = selector.get_support(indices = True) #returns an array of integers corresponding to nonremoved features
selector_df = pd.DataFrame(selector.transform(fingerprint_to_model), columns=fingerprint_to_model[features])
218/124:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05))
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model


selector = VarianceThreshold(0.05) #Defaults to 0.0, e.g. only remove features with the same value in all samples
selector.fit(fingerprint_to_model)
features = selector.get_support(indices = True) #returns an array of integers corresponding to nonremoved features
selector_df = pd.DataFrame(selector.transform(fingerprint_to_model), columns=fingerprint_to_model)
218/125:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05))
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model


selector = VarianceThreshold(0.05) #Defaults to 0.0, e.g. only remove features with the same value in all samples
selector.fit(fingerprint_to_model)
features = selector.get_support(indices = True) #returns an array of integers corresponding to nonremoved features
selector_df = pd.DataFrame(selector.transform(fingerprint_to_model), columns=fingerprint_to_model[features])
218/126:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05))
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model


selector = VarianceThreshold(0.05) #Defaults to 0.0, e.g. only remove features with the same value in all samples
selector.fit(fingerprint_to_model)
features = selector.get_support(indices = True) #returns an array of integers corresponding to nonremoved features
selector_df = pd.DataFrame(selector.transform(fingerprint_to_model), columns=fingerprint_to_model)
218/127:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05))
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model


selector = VarianceThreshold(0.05) #Defaults to 0.0, e.g. only remove features with the same value in all samples
selector2 = selector.fit(fingerprint_to_model)
features = selector.get_support(indices = True) #returns an array of integers corresponding to nonremoved features
selector_df = pd.DataFrame(selector.transform(fingerprint_to_model), columns=fingerprint_to_model)
218/128:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05))
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model


selector = VarianceThreshold(0.05) #Defaults to 0.0, e.g. only remove features with the same value in all samples
selector2 = selector.fit(fingerprint_to_model)
features = selector.get_support(indices = True) #returns an array of integers corresponding to nonremoved features
selector_df = pd.DataFrame(selector2.transform(fingerprint_to_model), columns=fingerprint_to_model)
218/129:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05))
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model


selector = VarianceThreshold(0.05) #Defaults to 0.0, e.g. only remove features with the same value in all samples
selector2 = selector.fit(fingerprint_to_model)
features = selector.get_support(indices = True) #returns an array of integers corresponding to nonremoved features
selector_df = pd.DataFrame(selector, columns=fingerprint_to_model)
218/130:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05))
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model


selector = VarianceThreshold(0.05) #Defaults to 0.0, e.g. only remove features with the same value in all samples
selector2 = selector.fit(fingerprint_to_model)
features = selector.get_support(indices = True) #returns an array of integers corresponding to nonremoved features
selector_df = pd.DataFrame(selector2, columns=fingerprint_to_model)
218/131:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05))
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model


selector = VarianceThreshold(0.05) #Defaults to 0.0, e.g. only remove features with the same value in all samples
selector2 = selector.fit(fingerprint_to_model)
features = selector.get_support(indices = True) #returns an array of integers corresponding to nonremoved features
selector_df = pd.DataFrame(selector2, columns=fingerprint_to_model)
218/132: selector_df
218/133: VarianceThreshold_transform(fingerprint_to_model, 0.05)
218/134:
import random
SEED = random.seed(10)
SEED
218/135:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05))
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model


selector = VarianceThreshold(0.05) #Defaults to 0.0, e.g. only remove features with the same value in all samples
selector2 = selector.fit_transform(fingerprint_to_model)
features = selector.get_support(indices = True) #returns an array of integers corresponding to nonremoved features
selector_df = pd.DataFrame(selector2, columns=fingerprint_to_model)
218/136:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05))
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model


selector = VarianceThreshold(0.05) #Defaults to 0.0, e.g. only remove features with the same value in all samples
selector2 = selector.fit_transform(fingerprint_to_model)
features = selector.get_support(indices = True) #returns an array of integers corresponding to nonremoved features
selector_df = pd.DataFrame(selector2, columns=fingerprint_to_model[column])
218/137:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05))
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model


selector = VarianceThreshold(0.05) #Defaults to 0.0, e.g. only remove features with the same value in all samples
selector2 = selector.fit_transform(fingerprint_to_model)
features = selector.get_support(indices = True) #returns an array of integers corresponding to nonremoved features
selector_df = pd.DataFrame(selector2, columns=fingerprint_to_model.columns)
218/138:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05))
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model


selector = VarianceThreshold(0.05) #Defaults to 0.0, e.g. only remove features with the same value in all samples
selector2 = selector.fit_transform(fingerprint_to_model)
features = selector.get_support(indices = True) #returns an array of integers corresponding to nonremoved features
selector_df = pd.DataFrame(selector2, columns=fingerprint_to_model[features].columns)
218/139:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model
218/140:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
sel.get_support(indices=True)
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model
218/141:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
sel.get_support(indices=True)
fingerprint_to_model = sel.fit(fingerprint_to_model)
fingerprint_to_model
218/142:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)

fingerprint_to_model.get_support(indices=True)
218/143:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
fingerprint_to_model = sel.fit_transform(fingerprint_to_model).get_support(indices=True)
fingerprint_to_model
218/144:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
sel.fit_transform(fingerprint_to_model).get_support(indices=True)
fingerprint_to_model
218/145:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))
fingerprint_to_model = sel.fit_transform(fingerprint_to_model).get_support(indices=True)
fingerprint_to_model
218/146:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05))..get_support(indices=True)
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model
218/147:
from sklearn.feature_selection import VarianceThreshold
sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model
218/148:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model

selector.get_support()
218/149:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model
sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model
sel.get_support()
218/150:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model
sel = VarianceThreshold(threshold=(0.05))
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model
sel.get_support()
218/151:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model
sel = VarianceThreshold(threshold=(0.05))
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model
218/152:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model
sel = VarianceThreshold(threshold=(0.05))
fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
fingerprint_to_model
sel.get_support(indices=True)
218/153:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model
sel = VarianceThreshold(threshold=(0.05))
select = sel.fit(fingerprint_to_model)
fingerprint_to_model
sel.get_support(indices=True)
218/154:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model
sel = VarianceThreshold(threshold=(0.05))
sel = sel.fit(fingerprint_to_model)
mask = sel.get_support()
218/155: fingerprint_to_model = fingerprint_to_model.loc[:, mask]
218/156:
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.LogKpu.tolist()
218/157: fingerprint_to_model.head(3)
218/158: indices = fingerprint_to_model.index
218/159:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model
sel = VarianceThreshold(threshold=(0.05))
sel = sel.fit(fingerprint_to_model)
mask = sel.get_support()
218/160: fingerprint_to_model = fingerprint_to_model.loc[:, mask]
218/161:
fingerprint_to_model = fingerprint_to_model.loc[:, mask]
fingerprint_to_model
218/162:
fingerprint_to_model = sel.loc[:, mask]
fingerprint_to_model
218/163:
fingerprint_to_model = fingerprint_to_model.loc[:, mask]
fingerprint_to_model
218/164:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model
sel = VarianceThreshold(threshold=(0.05))
sel = sel.fit(fingerprint_to_model)
mask = sel.get_support()
218/165:
fingerprint_to_model2 = fingerprint_to_model.loc[:, mask]
fingerprint_to_model2
218/166:
train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
218/167: indices_train, indices_test
218/168:
train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model2, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
218/169: indices_train, indices_test
218/170:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
218/171:
plt.scatter(test_y, y_ext, c='red')

plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
218/172:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
218/173:
plt.scatter(test_y, y_ext, c='red')

plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
218/174:
#Test features RF
from sklearn.inspection import permutation_importance
result = permutation_importance(model_RF, test_x, test_y, n_repeats=10, random_state=42, n_jobs=2)
feature_names = [f'feature {i}' for i in range(fingerprint_to_model.shape[1])]

forest_importances = pd.Series(result.importances_mean, index=feature_names)
fig, ax = plt.subplots()
forest_importances.plot.bar(yerr=result.importances_std, ax=ax)
ax.set_title("Feature importances using permutation on full model")
ax.set_ylabel("Mean accuracy decrease")
fig.tight_layout()
plt.show()
218/175:
#Test features RF
from sklearn.inspection import permutation_importance
result = permutation_importance(model_RF, test_x, test_y, n_repeats=10, random_state=42, n_jobs=2)
feature_names = [f'feature {i}' for i in range(fingerprint_to_model2.shape[1])]

forest_importances = pd.Series(result.importances_mean, index=feature_names)
fig, ax = plt.subplots()
forest_importances.plot.bar(yerr=result.importances_std, ax=ax)
ax.set_title("Feature importances using permutation on full model")
ax.set_ylabel("Mean accuracy decrease")
fig.tight_layout()
plt.show()
218/176:
FI_RF_test_mean = pd.DataFrame(forest_importances)
FI_RF_test_mean.to_csv('Result_RF/forest_importances_test_RF.csv')
FI_RF_test_mean
218/177:
#Test features RF
from sklearn.inspection import permutation_importance
result = permutation_importance(model_RF, test_x, test_y, n_repeats=10, random_state=42, n_jobs=2)
feature_names = fingerprint_to_model2.columns

forest_importances = pd.Series(result.importances_mean, index=feature_names)
fig, ax = plt.subplots()
forest_importances.plot.bar(yerr=result.importances_std, ax=ax)
ax.set_title("Feature importances using permutation on full model")
ax.set_ylabel("Mean accuracy decrease")
fig.tight_layout()
plt.show()
218/178:
FI_RF_test_mean = pd.DataFrame(forest_importances)
FI_RF_test_mean.to_csv('Result_RF/forest_importances_test_RF.csv')
FI_RF_test_mean
218/179:
#Train features
from sklearn.inspection import permutation_importance
result = permutation_importance(model_RF, train_x, train_y, n_repeats=10, random_state=42, n_jobs=2)
feature_names = fingerprint_to_model2.column

forest_importances = pd.Series(result.importances_mean, index=feature_names)
fig, ax = plt.subplots()
forest_importances.plot.bar(yerr=result.importances_std, ax=ax)
ax.set_title("Feature importances using permutation on full model")
ax.set_ylabel("Mean accuracy decrease")
fig.tight_layout()
plt.show()
218/180:
#Train features
from sklearn.inspection import permutation_importance
result = permutation_importance(model_RF, train_x, train_y, n_repeats=10, random_state=42, n_jobs=2)
feature_names = fingerprint_to_model2.columns

forest_importances = pd.Series(result.importances_mean, index=feature_names)
fig, ax = plt.subplots()
forest_importances.plot.bar(yerr=result.importances_std, ax=ax)
ax.set_title("Feature importances using permutation on full model")
ax.set_ylabel("Mean accuracy decrease")
fig.tight_layout()
plt.show()
218/181:
FI_RF_train_mean = pd.DataFrame(forest_importances)
FI_RF_train_mean.to_csv('Result_RF/forest_importances_train_RF.csv')
FI_RF_train_mean
218/182:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model
sel = VarianceThreshold(threshold=(0.0))
sel = sel.fit(fingerprint_to_model)
mask = sel.get_support()
218/183:
fingerprint_to_model2 = fingerprint_to_model.loc[:, mask]
fingerprint_to_model2
218/184:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model
sel = VarianceThreshold(threshold=(0.05))
sel = sel.fit(fingerprint_to_model)
mask = sel.get_support()
218/185:
fingerprint_to_model2 = fingerprint_to_model.loc[:, mask]
fingerprint_to_model2
218/186:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model
sel = VarianceThreshold(threshold=(0))
sel = sel.fit(fingerprint_to_model)
mask = sel.get_support()
218/187:
fingerprint_to_model2 = fingerprint_to_model.loc[:, mask]
fingerprint_to_model2
223/1:
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
223/2:
path = r'Fingerprint/'

Fp13  = pd.read_csv(path + 'mordard.csv'               , header = 0)
223/3: Fp13.shape
223/4: Fp13
223/5:
df = pd.read_csv('df6_lipinski_kpu.csv')
df
223/6: name = df['compound']
223/7: name.shape
223/8:
df2 = df.rename({'compound': 'Name'}, axis=1)
df2 = df2.drop('Unnamed: 0', axis=1)
df3 = df2.set_index('Name')
df3
223/9:

raw13.to_csv('QSAR_Kpu/Mordard_Kpu.csv' , sep=',' ,index=True)
223/10:
df2 = df.rename({'compound': 'Name'}, axis=1)
df2 = df2.drop('Unnamed: 0', axis=1)
df3 = df2.set_index('Name')
df3
223/11:
LogKpu = df3[['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]
LogKpu
223/12:
Fp13 = Fp13.rename(columns={'name': 'Name'})
Fp13
223/13:

Fp13.Name = name
223/14:

Fpp13 = Fp13.drop('Name', axis=1)
Fpp13
223/15: Fpp13 = Fpp13.fillna(0)
223/16:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
223/17:

Fp13_n = normalized (Fpp13)
223/18: Fp13_n
223/19: np.any(np.isnan(Fp13_n))
223/20: Fp13_n["Name"] = Fp13.Name
223/21: Fp13_n
223/22: raw13  = LogKpu.merge(Fp13_n, on='Name', how='outer')
223/23: raw13
223/24: np.any(np.isnan(raw13))
223/25: raw13  = LogKpu.merge(Fp13_n, on='Name', how='outer')
223/26: raw13
223/27: np.any(np.isnan(raw13))
223/28:
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
223/29:
path = r'Fingerprint/'

Fp13  = pd.read_csv(path + 'mordard.csv'               , header = 0)
223/30: Fp13.shape
223/31: Fp13
223/32:
df = pd.read_csv('df6_lipinski_kpu.csv')
df
223/33: name = df['compound']
223/34: name.shape
223/35:
df2 = df.rename({'compound': 'Name'}, axis=1)
df2 = df2.drop('Unnamed: 0', axis=1)
df3 = df2.set_index('Name')
df3
223/36:
LogKpu = df3[['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]
LogKpu
223/37:
Fp13 = Fp13.rename(columns={'name': 'Name'})
Fp13
223/38:

Fp13.Name = name
223/39:

Fpp13 = Fp13.drop('Name', axis=1)
Fpp13
223/40: Fpp13 = Fpp13.fillna(0)
223/41:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
223/42:

Fp13_n = normalized (Fpp13)
223/43: Fp13_n
223/44: np.any(np.isnan(Fp13_n))
223/45: Fp13_n["Name"] = Fp13.Name
223/46: Fp13_n
223/47: Fp13_n = Fp13_n.set_index('Name')
223/48: Fp13_n.head(20)
223/49: raw13  = LogKpu.merge(Fp13_n, on='Name', how='outer')
223/50: raw13
223/51: np.any(np.isnan(raw13))
223/52: raw13 =raw13[raw13.LogKpu.notna()]
223/53:

raw13.to_csv('QSAR_Kpu/Mordard_Kpu.csv' , sep=',' ,index=True)
230/1:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
230/2: pwd
230/3: df = pd.read_csv('rawdataqsar_sm.csv')
230/4:
df = pd.read_csv('rawdataqsar_sm.csv')
df
230/5:
df2 = df.drop_duplicates(subset=['smiles'])
df2
230/6:
df3 = df2.set_index(['compound'], drop=True)
df3
230/7: d4 =df3[df3.kp.notna()]
230/8:
d4 = df3[df3.kp.notna()]
d4
230/9:
d5 = df4[df3.kpu.notna()]
d5
230/10:
d5 = df4[df4.kpu.notna()]
d5
230/11:
d4 = df3[df3.kp.notna()]
d4
230/12:
d5 = df4[df4.kpu.notna()]
d5
230/13:
df4 = df3[df3.kp.notna()]
df4
230/14:
d5 = df4[df4.kpu.notna()]
d5
230/15:
def LogKpu(input):
    LogKpu = []

    for i in input['kpu']:
        LogKpu.append(np.log10(i))

    input['LogKpu'] = LogKpu
    x = input.drop('kpu', 1)   
    return x
230/16:
df5 = df4[df4.kpu.notna()]
df5
230/17: df6 = LogKpu(df5)
230/18: df6 = LogKpu(df5)
230/19: df6
230/20:
def LogKpu(input):
    LogKpu = []

    for i in input['kpu']:
        LogKpu.append(np.log10(i))

    input['LogKpu'] = LogKpu
    x = input   
    return x
230/21: df6 = LogKpu(df5)
230/22: df6
230/23:
# Calculate Molecular Descriptors
from rdkit import Chem
from rdkit.Chem import Descriptors, Lipinski

def lipinski(smiles, verbose=False):

    moldata= []
    for elem in smiles:
        mol=Chem.MolFromSmiles(elem) 
        moldata.append(mol)
       
    baseData= np.arange(1,1)
    i=0  
    for mol in moldata:        
       
        desc_MolWt = Descriptors.MolWt(mol)
        desc_TPSA = Descriptors.TPSA(mol)
        desc_MolLogP = Descriptors.MolLogP(mol)
        desc_NumHDonors = Lipinski.NumHDonors(mol)
        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)
        desc_NumRotatableBonds = Lipinski.NumRotatableBonds(mol)
        desc_FpDensityMorgan1 = Descriptors.FpDensityMorgan1(mol)
        desc_FpDensityMorgan2 = Descriptors.FpDensityMorgan2(mol)
        desc_FpDensityMorgan3 = Descriptors.FpDensityMorgan3(mol)
        desc_NumRadicalElectrons = Descriptors.NumRadicalElectrons(mol)
        desc_NumValenceElectrons = Descriptors.NumValenceElectrons(mol)
           
        row = np.array([desc_MolWt,
                        desc_TPSA,
                        desc_MolLogP,
                        desc_NumHDonors,
                        desc_NumHAcceptors,
                        desc_NumRotatableBonds,
                        desc_FpDensityMorgan1,
                        desc_FpDensityMorgan2,
                        desc_FpDensityMorgan3,
                        desc_NumRadicalElectrons,
                        desc_NumValenceElectrons])   
    
        if(i==0):
            baseData=row
        else:
            baseData=np.vstack([baseData, row])
        i=i+1      
    
    columnNames=["MW","TPSA", "LogP","NumHDonors","NumHAcceptors", "NumRotatableBonds", 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'NumRadicalElectrons', 'NumValenceElectrons']   
    descriptors = pd.DataFrame(data=baseData,columns=columnNames)
    
    return descriptors
230/24:
df6_lipinski = lipinski(df6.smiles)
df6_lipinski
230/25:
data = pd.concat([df6, df6_lipinski], axis=1)
data.to_csv('df6_lipinski_Kpu_Kp_cells.csv')
data
230/26:
df6_lipinski = lipinski(df6.smiles)
df6_lipinski.set_index(df6['compound'])
230/27:
df6_lipinski = lipinski(df6.smiles)
df6_lipinski.set_index(df6[compound])
230/28:
df6_lipinski = lipinski(df6.smiles)
df6_lipinski.set_index(df6['compound'])
230/29: indices = df6.index
230/30:
df6_lipinski = lipinski(df6.smiles)
df6_lipinski.set_index([indices])
230/31:
data = pd.concat([df6, df6_lipinski], axis=1)
data.to_csv('df6_lipinski_Kpu_Kp_cells.csv')
data
230/32:
df6_lipinski = lipinski(df6.smiles)
df6_lipinski = df6_lipinski.set_index([indices])
230/33:
data = pd.concat([df6, df6_lipinski], axis=1)
data.to_csv('df6_lipinski_Kpu_Kp_cells.csv')
data
230/34:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2) = plt.subplots(1, 1)
figure.set_size_inches(15,15)

# Histogram for Kp
plt1.hist(data['kp'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)
plt1.set_xlabel('Kp', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Kpu
plt2.hist(data['kpu'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)
plt2.set_xlabel("Kpu", fontsize=16, fontweight='bold')
plt2.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)
plt2.set_ylim(0, 50)
plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)



#save file
plt.tight_layout()
plt.savefig('Descriptor_outlier.pdf', dpi=300)
230/35:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2)) = plt.subplots(1, 1)
figure.set_size_inches(15,15)

# Histogram for Kp
plt1.hist(data['kp'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)
plt1.set_xlabel('Kp', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Kpu
plt2.hist(data['kpu'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)
plt2.set_xlabel("Kpu", fontsize=16, fontweight='bold')
plt2.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)
plt2.set_ylim(0, 50)
plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)



#save file
plt.tight_layout()
plt.savefig('Descriptor_outlier.pdf', dpi=300)
230/36:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2)) = plt.subplots(2, 2)
figure.set_size_inches(15,15)

# Histogram for Kp
plt1.hist(data['kp'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)
plt1.set_xlabel('Kp', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Kpu
plt2.hist(data['kpu'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)
plt2.set_xlabel("Kpu", fontsize=16, fontweight='bold')
plt2.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)
plt2.set_ylim(0, 50)
plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)



#save file
plt.tight_layout()
plt.savefig('Descriptor_outlier.pdf', dpi=300)
230/37:
data = pd.concat([df6, df6_lipinski], axis=1)
data.to_csv('df6_lipinski_Kpu_Kp_cells.csv')
data
230/38:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2)) = plt.subplots(2, 2)
figure.set_size_inches(15,15)

# Histogram for Kp
plt1.hist(data['kp'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)
plt1.set_xlabel('Kp', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Kpu
plt2.hist(data['kpu'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)
plt2.set_xlabel("Kpu", fontsize=16, fontweight='bold')
plt2.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)
plt2.set_ylim(0, 50)
plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)



#save file
plt.tight_layout()
plt.savefig('Descriptor_outlier.pdf', dpi=300)
230/39: data.kp
230/40:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2)) = plt.subplots(2, 2)
figure.set_size_inches(15,15)

# Histogram for Kp
plt1.hist(data['kp'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)
plt1.set_xlabel('Kp', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Kpu
plt2.hist(data['kpu'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)
plt2.set_xlabel("Kpu", fontsize=16, fontweight='bold')
plt2.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)
plt2.set_ylim(0, 50)
plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)



#save file
plt.tight_layout()
plt.savefig('Descriptor_outlier.pdf', dpi=300)
230/41: dat
230/42: data
230/43:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt



# Histogram for Kp
plt1.hist(data['kp'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)
plt1.set_xlabel('Kp', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Kpu
plt2.hist(data['kpu'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)
plt2.set_xlabel("Kpu", fontsize=16, fontweight='bold')
plt2.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)
plt2.set_ylim(0, 50)
plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)



#save file
plt.tight_layout()
plt.savefig('Descriptor_outlier.pdf', dpi=300)
230/44:
#Here we are ploting the data
%matplotlib inline
import matplotlib as mpl
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt

figure, ((plt1,plt2)) = plt.subplots(2, 2)
figure.set_size_inches(15,15)

# Histogram for Kp
plt1.his(data['kp'], density=False, bins= 30, color='#3EC1D3', edgecolor='black', linewidth=0.5)
plt1.set_xlabel('Kp', fontsize=16, fontweight='bold')
plt1.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt1.tick_params(axis='both', which='major', labelsize=14)
plt1.set_ylim(0, 50)
#plt1.grid(True)
plt1.axvline(500, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)

#Histogram for Kpu
plt2.hist(data['kpu'], density=False, bins= 30, color='#F6F7D7', edgecolor='black', linewidth=0.5)
plt2.set_xlabel("Kpu", fontsize=16, fontweight='bold')
plt2.set_ylabel("Frequency", fontsize=16, fontweight='bold')
plt2.tick_params(axis='both', which='major', labelsize=14)
plt2.set_ylim(0, 50)
plt2.axvline(5, color='gray',linestyle='--', dashes=(5, 10), linewidth=0.5)



#save file
plt.tight_layout()
plt.savefig('Descriptor_outlier.pdf', dpi=300)
230/45: data.boxplot(lp)
230/46: data.boxplot(kp)
230/47: data.boxplot(kp)
230/48: data.boxplot('kp')
230/49: q3 = data.kp.quantile(.75)
230/50:
q3 = data.kp.quantile(.75)
q1 = data.kp.quantile(.25)
230/51:
q3 = data.kp.quantile(.75)
q1 = data.kp.quantile(.25)
iqr = q3-q1
230/52:
q3 = data.kp.quantile(.75)
q1 = data.kp.quantile(.25)
iqr = q3-q1
upper = q3 + (1.5*iqr)
lower = q1 - (1.5*iqr)
print('q1 = {}'. format(q1))
230/53:
q3 = data.kp.quantile(.75)
q1 = data.kp.quantile(.25)
iqr = q3-q1
upper = q3 + (1.5*iqr)
lower = q1 - (1.5*iqr)
print('q1 = {}'. format(q1))
print('q3 = {}'. format(q3))
230/54:
q3 = data.kp.quantile(.75)
q1 = data.kp.quantile(.25)
iqr = q3-q1
upper = q3 + (1.5*iqr)
lower = q1 - (1.5*iqr)
print('q1 = {}'. format(q1))
print('q3 = {}'. format(q3))
print('iqr = {}'. format(iqr))
231/1:
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
231/2:
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
231/3: Fp13.shape
231/4:
path = r'Fingerprint/'

Fp13  = pd.read_csv(path + 'mordard.csv'               , header = 0)
231/5: Fp13.shape
231/6: Fp13
231/7:
df = pd.read_csv('df6_lipinski_kpu.csv')
df
231/8: name = df['compound']
231/9: name.shape
231/10:
df2 = df.rename({'compound': 'Name'}, axis=1)
df2 = df2.drop('Unnamed: 0', axis=1)
df3 = df2.set_index('Name')
df3
230/55:
q3 = data.kp.quantile(.75)
q1 = data.kp.quantile(.25)
iqr = q3-q1
upper = q3 + (1.5*iqr)
lower = q1 - (1.5*iqr)
print('q1 = {}'. format(q1))
print('q3 = {}'. format(q3))
print('iqr = {}'. format(iqr))
print('upper = {}'. format(upper))
print('lower = {}'. format(lower))
230/56: df7 = data[data.kp>314.55]
230/57: df7
231/11:
df = pd.read_csv('df6_lipinski.csv')
df
231/12: name = df['compound']
231/13: name.shape
231/14:
df2 = df.rename({'compound': 'Name'}, axis=1)
df2 = df2.drop('Unnamed: 0', axis=1)
df3 = df2.set_index('Name')
df3
231/15:
LogKpu = df3[['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]
LogKpu
231/16:
LogKp = df3[['LogKp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]
LogKp
231/17:
Fp13 = Fp13.rename(columns={'name': 'Name'})
Fp13
231/18:

Fpp13 = Fp13.drop('Name', axis=1)
Fpp13
231/19:

Fp13.Name = name
231/20:

Fpp13 = Fp13.drop('Name', axis=1)
Fpp13
231/21: Fpp13 = Fpp13.fillna(0)
231/22:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
231/23:

Fp13_n = normalized (Fpp13)
231/24: Fp13_n
231/25: np.any(np.isnan(Fp13_n))
231/26: Fp13_n["Name"] = Fp13.Name
231/27: Fp13_n
231/28: Fp13_n = Fp13_n.set_index('Name')
231/29: Fp13_n.head(20)
231/30: raw13  = LogKp.merge(Fp13_n, on='Name', how='outer')
231/31: raw13
231/32: np.any(np.isnan(raw13))
231/33: raw13 =raw13[raw13.LogKpu.notna()]
231/34: raw13 =raw13[raw13.LogKp.notna()]
231/35:

raw13.to_csv('QSAR_Kp/Mordard_Kp.csv' , sep=',' ,index=True)
231/36:

raw13.to_csv('QSAR_Kp/Mordard_Kp.csv' , sep=',' ,index=True)
231/37: print (len(raw13))
231/38: print (len(raw13 .columns))
231/39:
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['LogKp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.LogKp.tolist()
231/40: fingerprint_to_model.head(3)
231/41: indices = fingerprint_to_model.index
231/42:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model
sel = VarianceThreshold(threshold=(0))
sel = sel.fit(fingerprint_to_model)
mask = sel.get_support()
231/43:
fingerprint_to_model2 = fingerprint_to_model.loc[:, mask]
fingerprint_to_model2
231/44:
train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model2, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
231/45: indices_train, indices_test
231/46:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=SEED)
231/47:
import random
SEED = random.seed(10)
SEED
231/48:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=42)
231/49:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
231/50:
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='black')
plt.scatter(train_y, y_pred, c='blue')
223/54: # remove SM SMI SMO
223/55: raw13
223/56: raw14 = raw.drop['sesamol', 'sesamin', 'sesamolin']
223/57: raw14 = raw13.drop['sesamol', 'sesamin', 'sesamolin']
223/58: raw14 = raw13.drop(['sesamol', 'sesamin', 'sesamolin'])
223/59: raw14
223/60:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=42)
223/61:
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['LogKpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.LogKpu.tolist()
indices = fingerprint_to_model.index
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model
sel = VarianceThreshold(threshold=(0))
sel = sel.fit(fingerprint_to_model)
mask = sel.get_support()
fingerprint_to_model2 = fingerprint_to_model.loc[:, mask]
train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model2, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
indices_train, indices_test
223/62:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=42)
223/63:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
223/64:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
231/51:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
231/52:
plt.scatter(test_y, y_ext, c='red')

plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
231/53:
raw13 =raw13[raw13.LogKp.notna()]
raw13 =raw13.drop(['GSK236090'])
231/54:
raw13 =raw13[raw13.LogKp.notna()]
raw13 =raw13.drop(['GSK326090'])
231/55:
raw13 =raw13[raw13.LogKp.notna()]
raw13 =raw13.drop(['GSK326093'])
231/56:
raw13 =raw13[raw13.LogKp.notna()]
raw13 =raw13.drop(['GSK326090'])
231/57:
raw13 =raw13[raw13.LogKp.notna()]
raw13 =raw13.drop(['GSK326090'], axis=0)
231/58: raw13.GSK326090
231/59: raw13.index
231/60: print(raw13.index)
231/61:
raw13 =raw13[raw13.LogKp.notna()]
raw13
231/62: raw13.LogKp.max
231/63: raw13.LogKp.max()
231/64:
raw13 =raw13[raw13.LogKp.notna()]
raw13 =raw.drop(['GSK326090A'])
231/65:
raw13 =raw13[raw13.LogKp.notna()]
raw13 =raw13.drop(['GSK326090A'])
231/66:
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='black')
plt.scatter(train_y, y_pred, c='blue')
223/65:
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
231/67:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
231/68:
raw13 =raw13[raw13.LogKp.notna()]
raw13 =raw13.drop([GSK326090A])
231/69:
#Test features RF
from sklearn.inspection import permutation_importance
result = permutation_importance(model_RF, test_x, test_y, n_repeats=10, random_state=42, n_jobs=2)
feature_names = fingerprint_to_model2.columns

forest_importances = pd.Series(result.importances_mean, index=feature_names)
fig, ax = plt.subplots()
forest_importances.plot.bar(yerr=result.importances_std, ax=ax)
ax.set_title("Feature importances using permutation on full model")
ax.set_ylabel("Mean accuracy decrease")
fig.tight_layout()
plt.show()
231/70:
pd.DataFrame(train_x, indices_train).to_csv("Result_RF/x_train_kp.csv")
pd.DataFrame(test_x, indices_test).to_csv("Result_RF/x_test_kp.csv")
pd.DataFrame(y_cv, indices_train).to_csv("Result_RF/y_cv_kp.csv")
pd.DataFrame(train_y, indices_train).to_csv("Result_RF/train_y_kp.csv")
pd.DataFrame(test_y, indices_test).to_csv("Result_RF/test_y_kp.csv")
pd.DataFrame(y_pred, indices_train).to_csv("Result_RF/y_pred_kp.csv")
pd.DataFrame(y_ext, indices_test).to_csv("Result_RF/y_SVR_ext_kp.csv")
231/71: feature_names
231/72:
#Train features
from sklearn.inspection import permutation_importance
result = permutation_importance(model_RF, train_x, train_y, n_repeats=10, random_state=42, n_jobs=2)
feature_names = fingerprint_to_model2.columns

forest_importances = pd.Series(result.importances_mean, index=feature_names)
fig, ax = plt.subplots()
forest_importances.plot.bar(yerr=result.importances_std, ax=ax)
ax.set_title("Feature importances using permutation on full model")
ax.set_ylabel("Mean accuracy decrease")
fig.tight_layout()
plt.show()
230/58:
q3 = data.kp.quantile(.75)
q1 = data.kp.quantile(.25)
iqr = q3-q1
upper = q3 + (1.5*iqr)
lower = q1 - (1.5*iqr)
print('q1 = {}'. format(q1))
print('q3 = {}'. format(q3))
print('iqr = {}'. format(iqr))
print('upper = {}'. format(upper))
print('lower = {}'. format(lower))
230/59:
selection = ['smiles']
smiles = data[selection]
smiles.to_csv(r'smiles/new_smiles_Kp.smi', sep='\t', index=False, header=False)
225/1: !python -m mordred -t smi smiles/new_smiles_Kp.smi -o smiles/mordard_Kp.csv -d ABCIndex -d AcidBase -d AdjacencyMatrix -d Aromatic -d AtomCount -d Autocorrelation -d BCUT -d BalabanJ -d BertzCT -d BondCount -d CarbonTypes -d Chi -d Constitutional -d DistanceMatrix -d EState -d EccentricConnectivityIndex -d ExtendedTopochemicalAtom -d FragmentComplexity -d Framework -d GeometricalIndex -d GravitationalIndex -d HydrogenBond -d InformationContent -d KappaShapeIndex -d Lipinski -d LogS -d McGowanVolume -d MoeType -d MolecularDistanceEdge -d MolecularId -d PathCount -d Polarizability -d RingCount -d RotatableBond -d SLogP -d TopoPSA -d TopologicalCharge -d TopologicalIndex -d VdwVolumeABC -d VertexAdjacencyInformation -d WalkCount -d Weight -d WienerIndex -d ZagrebIndex
231/73:
path = r'Fingerprint/'

Fp13  = pd.read_csv(path + 'mordard_Kp.csv'               , header = 0)
231/74:
path = r'smiles/'

Fp13  = pd.read_csv(path + 'mordard_Kp.csv'               , header = 0)
231/75: Fp13.shape
231/76: Fp13
231/77:
df = pd.read_csv('df6_lipinski_Kpu_Kp_cells.csv')
df
231/78: name = df['compound']
231/79: name.shape
231/80:
df2 = df.rename({'compound': 'Name'}, axis=1)
df2 = df2.drop('Unnamed: 0', axis=1)
df3 = df2.set_index('Name')
df3
231/81:
df2 = df.rename({'compound': 'Name'}, axis=1)
df3 = df2.set_index('Name')
df3
231/82:
LogKp = df3[['LogKp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]
LogKp
231/83:
LogKp = df3[['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]
LogKp
231/84:
Fp13 = Fp13.rename(columns={'name': 'Name'})
Fp13
231/85:

Fp13.Name = name
231/86:

Fpp13 = Fp13.drop('Name', axis=1)
Fpp13
231/87: Fpp13 = Fpp13.fillna(0)
231/88:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
231/89:

Fp13_n = normalized (Fpp13)
231/90: Fp13_n
231/91: np.any(np.isnan(Fp13_n))
231/92: Fp13_n["Name"] = Fp13.Name
231/93: Fp13_n
231/94: Fp13_n = Fp13_n.set_index('Name')
231/95: Fp13_n.head(20)
231/96: Fp13_n.head(3)
231/97: raw13  = LogKp.merge(Fp13_n, on='Name', how='outer')
231/98: raw13
231/99: np.any(np.isnan(raw13))
231/100:
raw13 =raw13[raw13.LogKp.notna()]
raw13 =raw13.drop([GSK326090A])
231/101:
raw13 =raw13[raw13.logkp.notna()]
raw13 =raw13.drop([GSK326090A])
231/102:
raw13 =raw13[raw13.logkp.notna()]
raw13.shape
231/103: print (len(raw13))
231/104: print (len(raw13 .columns))
231/105: raw13.to_csv('QSAR_Kp/Mordard_Kp_144.csv' , sep=',' ,index=True)
231/106:
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.LogKp.tolist()
231/107:
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.logkp.tolist()
231/108: indices = fingerprint_to_model.index
231/109:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model
sel = VarianceThreshold(threshold=(0))
sel = sel.fit(fingerprint_to_model)
mask = sel.get_support()
231/110:
fingerprint_to_model2 = fingerprint_to_model.loc[:, mask]
fingerprint_to_model2
231/111:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model
sel = VarianceThreshold(threshold=(0.05))
sel = sel.fit(fingerprint_to_model)
mask = sel.get_support()
231/112:
fingerprint_to_model2 = fingerprint_to_model.loc[:, mask]
fingerprint_to_model2
231/113:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model
sel = VarianceThreshold(threshold=(0))
sel = sel.fit(fingerprint_to_model)
mask = sel.get_support()
231/114:
fingerprint_to_model2 = fingerprint_to_model.loc[:, mask]
fingerprint_to_model2
231/115:
train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model2, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
231/116: indices_train, indices_test
231/117: indices_train, indices_test
231/118:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=42)
231/119: fingerprint_to_model.shape
231/120:
fingerprint_to_model2 = fingerprint_to_model.loc[:, mask]
fingerprint_to_model2
231/121:
train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model2, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
231/122: indices_train, indices_test
231/123:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=42)
231/124: fingerprint_to_model2.shape
231/125: train_x.shape
231/126:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
231/127:
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='black')
plt.scatter(train_y, y_pred, c='blue')
231/128:
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
231/129:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='linear')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
231/130:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
231/131:
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
231/132:
from sklearn.cross_decomposition import PLSRegression

model_PLS = PLSRegression(n_components=10)
scores=cross_val_score(model_PLS, train_x, train_y, cv=kf)

#calculate R2
model_fit = model_PLS.fit(train_x, train_y)
y_pred = model_PLS.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_PLS, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_PLS.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
231/133:
from sklearn.cross_decomposition import PLSRegression

model_PLS = PLSRegression(n_components=10)
scores=cross_val_score(model_PLS, train_x, train_y, cv=kf)

#calculate R2
model_fit = model_PLS.fit(train_x, train_y)
y_pred = model_PLS.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_PLS, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_PLS.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
231/134:
from sklearn.linear_model import Ridge
Ridge = Ridge(alpha=1.0)

#calculate R2
model_fit = Ridge.fit(train_x, train_y)
y_pred = Ridge.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(Ridge, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = Ridge.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
231/135:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
231/136:
from sklearn.linear_model import Ridge
Ridge = Ridge(alpha=1.0)

#calculate R2
model_fit = Ridge.fit(train_x, train_y)
y_pred = Ridge.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(Ridge, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = Ridge.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
231/137:
from sklearn.cross_decomposition import PLSRegression

model_PLS = PLSRegression(n_components=10)
scores=cross_val_score(model_PLS, train_x, train_y, cv=kf)

#calculate R2
model_fit = model_PLS.fit(train_x, train_y)
y_pred = model_PLS.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_PLS, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_PLS.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
231/138:
from sklearn.neural_network import MLPRegressor
MLP = MLPRegressor(activation='relu', random_state=42, max_iter=1000, solver='lbfgs')


#calculate R2
model_fit = MLP.fit(train_x, train_y)
y_pred = MLP.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(MLP, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = MLP.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
231/139:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model
sel = VarianceThreshold(threshold=(0.05))
sel = sel.fit(fingerprint_to_model)
mask = sel.get_support()
231/140:
fingerprint_to_model2 = fingerprint_to_model.loc[:, mask]
fingerprint_to_model2
231/141: fingerprint_to_model2.columns
231/142:
train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model2, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
231/143: indices_train, indices_test
231/144:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=42)
231/145: fingerprint_to_model2.shape
231/146: train_x.shape
231/147:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
231/148:
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
231/149:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
231/150:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='linear')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
231/151:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
231/152:
#Test features RF
from sklearn.inspection import permutation_importance
result = permutation_importance(model_RF, test_x, test_y, n_repeats=10, random_state=42, n_jobs=2)
feature_names = fingerprint_to_model2.columns
forest_importances = pd.Series(result.importances_mean, index=feature_names)
fig, ax = plt.subplots()
forest_importances.plot.bar(yerr=result.importances_std, ax=ax)
ax.set_title("Feature importances using permutation on full model")
ax.set_ylabel("Mean accuracy decrease")
fig.tight_layout()
plt.show()
231/153: feature_names
231/154:
FI_RF_test_mean = pd.DataFrame(forest_importances)
FI_RF_test_mean.to_csv('Result_RF/forest_importances_test_RF_Kp_156.csv')
FI_RF_test_mean
231/155:
#Train features
from sklearn.inspection import permutation_importance
result = permutation_importance(model_RF, train_x, train_y, n_repeats=10, random_state=42, n_jobs=2)
feature_names = fingerprint_to_model2.columns

forest_importances = pd.Series(result.importances_mean, index=feature_names)
fig, ax = plt.subplots()
forest_importances.plot.bar(yerr=result.importances_std, ax=ax)
ax.set_title("Feature importances using permutation on full model")
ax.set_ylabel("Mean accuracy decrease")
fig.tight_layout()
plt.show()
237/1:
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
237/2:
path = r'smiles/'

Fp13  = pd.read_csv(path + 'mordard_Kp.csv'               , header = 0)
237/3: Fp13.shape
237/4: Fp13
237/5:
df = pd.read_csv('df6_lipinski_Kpu_Kp_cells.csv')
df
237/6: name = df['compound']
237/7: name.shape
237/8:
df2 = df.rename({'compound': 'Name'}, axis=1)
df3 = df2.set_index('Name')
df3
237/9:
LogKp = df3[['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]
LogKp
237/10:
Fp13 = Fp13.rename(columns={'name': 'Name'})
Fp13
237/11:

Fp13.Name = name
237/12:

Fpp13 = Fp13.drop('Name', axis=1)
Fpp13
237/13: Fpp13 = Fpp13.fillna(0)
237/14:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
237/15:

Fp13_n = normalized (Fpp13)
237/16: Fp13_n
237/17: np.any(np.isnan(Fp13_n))
237/18: Fp13_n["Name"] = Fp13.Name
237/19: Fp13_n
237/20: Fp13_n = Fp13_n.set_index('Name')
237/21: Fp13_n.head(3)
237/22: raw13  = LogKp.merge(Fp13_n, on='Name', how='outer')
237/23: raw13
237/24: np.any(np.isnan(raw13))
237/25:
raw13 =raw13[raw13.logkp.notna()]
raw13.shape
237/26: print (len(raw13))
237/27: print (len(raw13 .columns))
237/28:
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.logkp.tolist()
237/29: fingerprint_to_model.head(3)
237/30: indices = fingerprint_to_model.index
237/31:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model
sel = VarianceThreshold(threshold=(0.05))
sel = sel.fit(fingerprint_to_model)
mask = sel.get_support()
237/32:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model
sel = VarianceThreshold(threshold=(0.00))
sel = sel.fit(fingerprint_to_model)
mask = sel.get_support()
237/33:
fingerprint_to_model2 = fingerprint_to_model.loc[:, mask]
fingerprint_to_model2
237/34:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model
sel = VarianceThreshold(threshold=(0.05))
sel = sel.fit(fingerprint_to_model)
mask = sel.get_support()
237/35:
fingerprint_to_model2 = fingerprint_to_model.loc[:, mask]
fingerprint_to_model2
237/36: fingerprint_to_model2.set_option('display.max_columns", None)
237/37: fingerprint_to_model2.columns
237/38:
import random
SEED = random.seed(10)
SEED
237/39:
train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model2, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
237/40: indices_train, indices_test
237/41:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=42)
237/42: fingerprint_to_model2.shape
237/43: train_x.shape
237/44:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
237/45:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='linear')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
237/46:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
246/1:
from rdkit import Chem #RDKit Chemistry
from rdkit.Chem.Draw import IPythonConsole #RDKit drawing
from rdkit.Chem import Draw #RDKit drawing
# A few settings to improve the quality of structures 
from rdkit.Chem import rdDepictor
IPythonConsole.ipython_useSVG = True
rdDepictor.SetPreferCoordGen(True)
from rdkit.Chem import PandasTools #Add the ability to add a molecule to a dataframegrid
import mols2grid #The mols2grid library provides a convenient way of displaying molecules in a grid
246/2: ! pip install mol2grid
246/3:
from rdkit import Chem #RDKit Chemistry
from rdkit.Chem.Draw import IPythonConsole #RDKit drawing
from rdkit.Chem import Draw #RDKit drawing
# A few settings to improve the quality of structures 
from rdkit.Chem import rdDepictor
IPythonConsole.ipython_useSVG = True
rdDepictor.SetPreferCoordGen(True)
from rdkit.Chem import PandasTools #Add the ability to add a molecule to a dataframegrid
import mols2grid #The mols2grid library provides a convenient way of displaying molecules in a grid
246/4: ! pip install mols2grid
246/5:
from rdkit import Chem #RDKit Chemistry
from rdkit.Chem.Draw import IPythonConsole #RDKit drawing
from rdkit.Chem import Draw #RDKit drawing
# A few settings to improve the quality of structures 
from rdkit.Chem import rdDepictor
IPythonConsole.ipython_useSVG = True
rdDepictor.SetPreferCoordGen(True)
from rdkit.Chem import PandasTools #Add the ability to add a molecule to a dataframegrid
import mols2grid #The mols2grid library provides a convenient way of displaying molecules in a grid
246/6: mol = Chem.MolFromSmiles("c1ccccc1")
246/7: mol
246/8: mol = Chem.MolFromSmiles("c1=ccccc1")
246/9:
mol = Chem.MolFromSmiles("c1=ccccc1")
mol
246/10:
mol = Chem.MolFromSmiles("c1(=0)cccc1")
mol
246/11:
mol = Chem.MolFromSmiles("c(=0)cccc1")
mol
246/12:
mol = Chem.MolFromSmiles("c(=0)ccc1")
mol
246/13:
mol = Chem.MolFromSmiles("c(=0)cc")
mol
246/14:
mol = Chem.MolFromSmiles("c(=o)cc")
mol
246/15:
mol = Chem.MolFromSmiles("c(=o)c")
mol
246/16:
mol = Chem.MolFromSmiles("cc(=o)c")
mol
246/17:
mol = Chem.MolFromSmiles("c1c=oc")
mol
246/18:
mol = Chem.MolFromSmiles("c1ccc1")
mol
246/19:
mol = Chem.MolFromSmiles("c1cccc1")
mol
246/20:
mol = Chem.MolFromSmiles("c1ccccc1")
mol
246/21:
mol = Chem.MolFromSmiles("c1cccccc1")
mol
246/22:
mol = Chem.MolFromSmiles("c1cccc1")
mol
246/23:
mol = Chem.MolFromSmiles("c1ccc1")
mol
246/24:
mol = Chem.MolFromSmiles("c1ccc")
mol
246/25:
mol = Chem.MolFromSmiles("c1cc1c")
mol
246/26:
mol = Chem.MolFromSmiles("c1ccc1")
mol
246/27:
mol = Chem.MolFromSmiles("c1cccc1")
mol
246/28:
mol = Chem.MolFromSmiles("c1ccccn1")
mol
246/29: glvc = Chem.MolFromSmiles("CN1CCN(Cc2ccc(cc2)C(=O)Nc3ccc(C)c(Nc4nccc(n4)c5cccnc5)c3)CC1")
246/30: glvc
246/31: mols = [x for x in Chem.SDMolSupplier("example_compounds.sdf")]
246/32: mols
246/33: Draw.MolsToGridImage(mols,molsPerRow=4,useSVG=True)
246/34: mols2grid.display(mols)
247/1:
import pandas as pd #data table manipulation
from rdkit import Chem # basic cheminformatics
from rdkit.Chem.Descriptors import MolWt, MolLogP, NumAromaticRings, NumHDonors, NumHAcceptors
import math #needed for log10
import seaborn as sns #plotting
from sklearn.tree import DecisionTreeClassifier, plot_tree # descision trees 
from sklearn.model_selection import train_test_split # split a dataset
from tqdm import tqdm # progress bar
from matplotlib import pyplot as plt # plotting
from dtreeviz.trees import * #plotting decision trees
from sklearn.metrics import roc_auc_score, matthews_corrcoef, cohen_kappa_score, plot_roc_curve, plot_confusion_matrix # model stats
247/2: !pip install dtreeviz
247/3:
import pandas as pd #data table manipulation
from rdkit import Chem # basic cheminformatics
from rdkit.Chem.Descriptors import MolWt, MolLogP, NumAromaticRings, NumHDonors, NumHAcceptors
import math #needed for log10
import seaborn as sns #plotting
from sklearn.tree import DecisionTreeClassifier, plot_tree # descision trees 
from sklearn.model_selection import train_test_split # split a dataset
from tqdm import tqdm # progress bar
from matplotlib import pyplot as plt # plotting
from dtreeviz.trees import * #plotting decision trees
from sklearn.metrics import roc_auc_score, matthews_corrcoef, cohen_kappa_score, plot_roc_curve, plot_confusion_matrix # model stats
247/4: tqdm.pandas()
247/5:
def calc_descriptors(smi):
    mol = Chem.MolFromSmiles(smi)
    if mol:
        mw, logp, num_arom_rings, hbd, hba = [x(mol) for x in [MolWt, MolLogP, NumAromaticRings, NumHDonors, NumHAcceptors]]
        res = [mw, logp, num_arom_rings, hbd, hba]
    else:
        res = [None] * 5
    return res
247/6: df = pd.read_csv("delaney.csv")
247/7: df.columns
247/8:
df = pd.read_csv("delaney.csv")
df
247/9: df.columns
247/10:
cols = list(df.columns)
cols[1] = 'LogS'
df.columns = cols
247/11: df['IsSol'] = df.LogS > math.log10(200 * 1e-6)
247/12: sns.displot(x='LogS',hue="IsSol",data=df)
247/13: df['desc'] = df.SMILES.progress_apply(calc_descriptors)
247/14:
desc_cols = ['MW','LogP','NumAromatic','HBD','HBA']
df[desc_cols] = df.desc.to_list()
247/15: df.drop("desc",axis=1,inplace=True)
247/16: df
247/17:
desc_cols = ['MW','LogP','NumAromatic','HBD','HBA']
df[desc_cols] = df.desc.to_list()
247/18:
desc_cols = ['MW','LogP','NumAromatic','HBD','HBA']
df[desc_cols] = df.desc.to_list()
247/19: df
247/20: train, test = train_test_split(df)
247/21:
train_X = train[desc_cols]
train_y = train.IsSol
test_X = test[desc_cols]
test_y = test.IsSol
247/22:
cls = DecisionTreeClassifier(max_depth=2)
cls.fit(train_X,train_y)
247/23:
pred = cls.predict(test_X)
auc, mcc, kappa = roc_auc_score(test_y, pred),matthews_corrcoef(test_y,pred),cohen_kappa_score(test_y,pred)
print(f"ROC AUC = {auc:.2f}")
print(f"Matthews Correlation Coefficient = {mcc:.2f}")
print(f"Cohen\'s Kappa = {kappa:.2f}")
247/24: plot_confusion_matrix(cls,test_X,test_y)
247/25: plot_roc_curve(cls,test_X,test_y)
247/26:
plt.rcParams["figure.figsize"] = (10,10)
plot_tree(cls,feature_names=desc_cols)
247/27:
viz = dtreeviz(cls, train_X, train_y, feature_names = desc_cols, 
               target_name = "Solubility",class_names=["False","True"],scale=2)
viz
247/28:
plt.rcParams["figure.figsize"] = (10,10)
plot_tree(cls,feature_names=desc_cols)
247/29:
import pandas as pd #data table manipulation
from rdkit import Chem # basic cheminformatics
from rdkit.Chem.Descriptors import MolWt, MolLogP, NumAromaticRings, NumHDonors, NumHAcceptors
import math #needed for log10
import seaborn as sns #plotting
from sklearn.tree import DecisionTreeClassifier, plot_tree # descision trees 
from sklearn.model_selection import train_test_split # split a dataset
from tqdm import tqdm # progress bar
from matplotlib import pyplot as plt # plotting
from dtreeviz.trees import * #plotting decision trees
from sklearn.metrics import roc_auc_score, matthews_corrcoef, cohen_kappa_score, plot_roc_curve, plot_confusion_matrix # model stats
247/30:
viz = dtreeviz(cls, train_X, train_y, feature_names = desc_cols, 
               target_name = "Solubility",class_names=["False","True"],scale=2)
viz
253/1:
from rdkit import Chem #RDKit Chemistry
from rdkit.Chem.Draw import IPythonConsole #RDKit drawing
from rdkit.Chem import Draw #RDKit drawing
# A few settings to improve the quality of structures 
from rdkit.Chem import rdDepictor
IPythonConsole.ipython_useSVG = True
rdDepictor.SetPreferCoordGen(True)
from rdkit.Chem import PandasTools #Add the ability to add a molecule to a dataframegrid
import mols2grid #The mols2grid library provides a convenient way of displaying molecules in a grid
253/2: mol = Chem.MolFromSmiles("c1cccc1")
253/3: mol = Chem.MolFromSmiles("c1ccc1")
253/4: mol
273/1:
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
273/2:
path = r'smiles/'

Fp13  = pd.read_csv(path + 'mordard_Kp.csv'               , header = 0)
273/3: Fp13.shape
273/4: Fp13
273/5:
df = pd.read_csv('df6_lipinski_Kpu_Kp_cells.csv')
df
273/6: Fp13
273/7:
df = pd.read_csv('df6_lipinski_Kpu_Kp_cells.csv')
df
273/8: name.shape
273/9: name = df['compound']
273/10: name.shape
273/11:
df2 = df.rename({'compound': 'Name'}, axis=1)
df3 = df2.set_index('Name')
df3
273/12:
LogKp = df3[['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]
LogKp
273/13:
Fp13 = Fp13.rename(columns={'name': 'Name'})
Fp13
273/14:

Fp13.Name = name
273/15:

Fpp13 = Fp13.drop('Name', axis=1)
Fpp13
273/16: Fpp13 = Fpp13.fillna(0)
273/17:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
273/18:

Fp13_n = normalized (Fpp13)
273/19: Fp13_n
273/20: np.any(np.isnan(Fp13_n))
273/21: Fp13_n["Name"] = Fp13.Name
273/22: Fp13_n
273/23: Fp13_n = Fp13_n.set_index('Name')
273/24: Fp13_n.head(3)
273/25: raw13  = LogKp.merge(Fp13_n, on='Name', how='outer')
273/26: raw13
273/27: np.any(np.isnan(raw13))
273/28:
raw13 =raw13[raw13.logkp.notna()]
raw13.shape
273/29: print (len(raw13))
273/30: print (len(raw13 .columns))
273/31:
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.logkp.tolist()
273/32: fingerprint_to_model.head(3)
273/33:
train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model2, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
273/34: indices = fingerprint_to_model.index
273/35:
train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
273/36:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model
sel = VarianceThreshold(threshold=(0.05))
sel = sel.fit(train_x)
mask = sel.get_support()
273/37:
fingerprint_to_model2 = sel.loc[:, mask]
fingerprint_to_model2
273/38:
fingerprint_to_model2 = train_x.loc[:, mask]
fingerprint_to_model2
273/39:
train_x_selected = train_x.loc[:, mask]
train_x_selected
273/40: train_x_selected.columns
273/41: indices_train, indices_test
273/42: indices_train, indices_test
273/43:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=42)
273/44: train_x_selected.shape
273/45:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x_selected, train_y)
y_pred = SVR.predict(train_x_selected)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x_selected, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
273/46:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x_selected, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x_selected, train_y)
y_pred = SVR.predict(train_x_selected)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x_selected, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test

test_x_selected = sel.transform(test_x)

y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
273/47:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x_selected, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x_selected, train_y)
y_pred = SVR.predict(train_x_selected)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x_selected, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test

test_x_selected = sel.transform(test_x)

y_ext = SVR.predict(test_x_selected)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
273/48:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import accuracy_score

SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x_selected, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x_selected, train_y)
y_pred = SVR.predict(train_x_selected)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x_selected, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test

test_x_selected = sel.transform(test_x)

y_ext = SVR.predict(test_x_selected)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)
Test_accuracy = accuracy_score(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f CV accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
print(f'Test accuracy is {Test_accuracy}')
print('===================================')
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
273/49:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x_selected, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x_selected, train_y)
y_pred = SVR.predict(train_x_selected)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x_selected, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test

test_x_selected = sel.transform(test_x)

y_ext = SVR.predict(test_x_selected)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)


print('Results')
print('===================================')
print("%0.2f CV accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
273/50: test_x_selected = sel.transform(test_x)
273/51:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='linear')
scores=cross_val_score(SVR, train_x_selected, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x_selected, train_y)
y_pred = SVR.predict(train_x_selected)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x_selected, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test

y_ext = SVR.predict(test_x_selected)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
273/52:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='linear')
scores=cross_val_score(SVR, train_x_selected, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x_selected, train_y)
y_pred = SVR.predict(train_x_selected)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x_selected, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test

y_ext = SVR.predict(test_x_selected)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
273/53:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x_selected, train_y, cv=kf)


#calculate R2
SVR_fit = model_RF.fit(train_x_selected, train_y)
y_pred = model_RF.predict(train_x_selected)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x_selected, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x_selected)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
273/54:
#Train features
from sklearn.inspection import permutation_importance
result = permutation_importance(model_RF, train_x_selected, train_y, n_repeats=10, random_state=42, n_jobs=2)
feature_names = fingerprint_to_model2.columns

forest_importances = pd.Series(result.importances_mean, index=feature_names)
fig, ax = plt.subplots()
forest_importances.plot.bar(yerr=result.importances_std, ax=ax)
ax.set_title("Feature importances using permutation on full model")
ax.set_ylabel("Mean accuracy decrease")
fig.tight_layout()
plt.show()
273/55:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x_selected, train_y, cv=kf)


#calculate R2
model_RF_fit = model_RF.fit(train_x_selected, train_y)
y_pred = model_RF.predict(train_x_selected)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x_selected, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x_selected)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
273/56: model_RF_fit
273/57: model_RF
273/58:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x_selected, train_y, cv=kf)


#calculate R2

y_pred = model_RF.predict(train_x_selected)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x_selected, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x_selected)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
273/59:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x_selected, train_y, cv=kf)


#calculate R2
model_RF_fit = model_RF.fit(train_x_selected, train_y)
y_pred = model_RF.predict(train_x_selected)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x_selected, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x_selected)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
273/60:
from sklearn.cross_decomposition import PLSRegression

model_PLS = PLSRegression(n_components=10)
scores=cross_val_score(model_PLS, train_x_selected, train_y, cv=kf)

#calculate R2
model_fit = model_PLS.fit(train_x_selected, train_y)
y_pred = model_PLS.predict(train_x_selected)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_PLS, train_x_selected, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_PLS.predict(test_x_selected)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
273/61:
from sklearn.linear_model import Ridge
Ridge = Ridge(alpha=1.0)

#calculate R2
model_fit = Ridge.fit(train_x_selected, train_y)
y_pred = Ridge.predict(train_x_selected)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(Ridge, train_x_selected, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = Ridge.predict(test_x_selected)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
273/62:
from sklearn.neural_network import MLPRegressor
MLP = MLPRegressor(activation='relu', random_state=42, max_iter=1000, solver='lbfgs')


#calculate R2
model_fit = MLP.fit(train_x_selected, train_y)
y_pred = MLP.predict(train_x_selected)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(MLP, train_x_selected, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = MLP.predict(test_x_selected)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
273/63:
from sklearn.linear_model import Ridge
Ridge = Ridge(alpha=1.0)

#calculate R2
model_fit = Ridge.fit(train_x_selected, train_y)
y_pred = Ridge.predict(train_x_selected)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(Ridge, train_x_selected, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = Ridge.predict(test_x_selected)
#Rsqure_ext
r2_ext = score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
273/64:
from sklearn.linear_model import Ridge
Ridge = Ridge(alpha=1.0)

#calculate R2
model_fit = Ridge.fit(train_x_selected, train_y)
y_pred = Ridge.predict(train_x_selected)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(Ridge, train_x_selected, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = Ridge.predict(test_x_selected)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
273/65:
pd.DataFrame(train_x, indices_train).to_csv("Result_RF/x_train_kp.csv")
pd.DataFrame(test_x, indices_test).to_csv("Result_RF/x_test_kp.csv")
pd.DataFrame(y_cv, indices_train).to_csv("Result_RF/y_cv_kp.csv")
pd.DataFrame(train_y, indices_train).to_csv("Result_RF/train_y_kp.csv")
pd.DataFrame(test_y, indices_test).to_csv("Result_RF/test_y_kp.csv")
pd.DataFrame(y_pred, indices_train).to_csv("Result_RF/y_pred_kp.csv")
pd.DataFrame(y_ext, indices_test).to_csv("Result_RF/y_SVR_ext_kp.csv")
273/66:
FI_RF_train_mean = pd.DataFrame(forest_importances)
FI_RF_train_mean.to_csv('Result_RF/forest_importances_train_RF_Kp_181.csv')
FI_RF_train_mean
273/67:
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.logkp.tolist()
273/68: fingerprint_to_model.head(3)
273/69: indices = fingerprint_to_model.index
273/70:
train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
273/71: indices_train, indices_test
273/72: train_x.columns
273/73:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=42)
273/74: train_x.shape
273/75:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test

y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)


print('Results')
print('===================================')
print("%0.2f CV accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
273/76:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='linear')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test

y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
273/77:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)


#calculate R2
model_RF_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
273/78: train_x.shape
273/79:
#Train features
from sklearn.inspection import permutation_importance
result = permutation_importance(model_RF, train_x, train_y, n_repeats=10, random_state=42, n_jobs=2)
feature_names = fingerprint_to_model2.columns

forest_importances = pd.Series(result.importances_mean, index=feature_names)
fig, ax = plt.subplots()
forest_importances.plot.bar(yerr=result.importances_std, ax=ax)
ax.set_title("Feature importances using permutation on full model")
ax.set_ylabel("Mean accuracy decrease")
fig.tight_layout()
plt.show()
276/1:
#Train features
from sklearn.inspection import permutation_importance
result = permutation_importance(model_RF, train_x_selected, train_y, n_repeats=10, random_state=42, n_jobs=2)
feature_names = train_x_selected.columns

forest_importances = pd.Series(result.importances_mean, index=feature_names)
fig, ax = plt.subplots()
forest_importances.plot.bar(yerr=result.importances_std, ax=ax)
ax.set_title("Feature importances using permutation on full model")
ax.set_ylabel("Mean accuracy decrease")
fig.tight_layout()
plt.show()
276/2:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x_selected, train_y, cv=kf)


#calculate R2
model_RF_fit = model_RF.fit(train_x_selected, train_y)
y_pred = model_RF.predict(train_x_selected)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x_selected, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x_selected)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
276/3:
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
276/4:
path = r'smiles/'

Fp13  = pd.read_csv(path + 'mordard_Kp.csv'               , header = 0)
276/5: Fp13.shape
276/6: Fp13
276/7:
df = pd.read_csv('df6_lipinski_Kpu_Kp_cells.csv')
df
276/8: name = df['compound']
276/9: name.shape
276/10:
df2 = df.rename({'compound': 'Name'}, axis=1)
df3 = df2.set_index('Name')
df3
276/11:
LogKp = df3[['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]
LogKp
276/12:
Fp13 = Fp13.rename(columns={'name': 'Name'})
Fp13
276/13:

Fp13.Name = name
276/14:

Fpp13 = Fp13.drop('Name', axis=1)
Fpp13
276/15: Fpp13 = Fpp13.fillna(0)
276/16:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
276/17:

Fp13_n = normalized (Fpp13)
276/18: Fp13_n
276/19: np.any(np.isnan(Fp13_n))
276/20: Fp13_n["Name"] = Fp13.Name
276/21: Fp13_n
276/22: Fp13_n = Fp13_n.set_index('Name')
276/23: Fp13_n.head(3)
276/24: raw13  = LogKp.merge(Fp13_n, on='Name', how='outer')
276/25: raw13
276/26: np.any(np.isnan(raw13))
276/27:
raw13 =raw13[raw13.logkp.notna()]
raw13.shape
276/28: print (len(raw13))
276/29: print (len(raw13 .columns))
276/30:
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.logkp.tolist()
276/31: fingerprint_to_model.head(3)
276/32: indices = fingerprint_to_model.index
276/33:
train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
276/34: indices_train, indices_test
276/35:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model
sel = VarianceThreshold(threshold=(0.05))
sel = sel.fit(train_x)
mask = sel.get_support()
276/36:
train_x_selected = train_x.loc[:, mask]
train_x_selected
276/37: train_x_selected.columns
276/38:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=42)
276/39: train_x_selected.shape
276/40: test_x_selected = sel.transform(test_x)
276/41:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x_selected, train_y, cv=kf)


#calculate R2
model_RF_fit = model_RF.fit(train_x_selected, train_y)
y_pred = model_RF.predict(train_x_selected)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x_selected, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x_selected)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
276/42:
#Train features
from sklearn.inspection import permutation_importance
result = permutation_importance(model_RF, train_x_selected, train_y, n_repeats=10, random_state=42, n_jobs=2)
feature_names = train_x_selected.columns

forest_importances = pd.Series(result.importances_mean, index=feature_names)
fig, ax = plt.subplots()
forest_importances.plot.bar(yerr=result.importances_std, ax=ax)
ax.set_title("Feature importances using permutation on full model")
ax.set_ylabel("Mean accuracy decrease")
fig.tight_layout()
plt.show()
273/80:
#Train features
from sklearn.inspection import permutation_importance
result = permutation_importance(model_RF, train_x, train_y, n_repeats=10, random_state=42, n_jobs=2)
feature_names = train_x.columns

forest_importances = pd.Series(result.importances_mean, index=feature_names)
fig, ax = plt.subplots()
forest_importances.plot.bar(yerr=result.importances_std, ax=ax)
ax.set_title("Feature importances using permutation on full model")
ax.set_ylabel("Mean accuracy decrease")
fig.tight_layout()
plt.show()
276/43:
FI_RF_train_mean = pd.DataFrame(forest_importances)
FI_RF_train_mean.to_csv('Result_RF/forest_importances_train_RF_Kp_181.csv')
FI_RF_train_mean
273/81:
FI_RF_train_mean = pd.DataFrame(forest_importances)
FI_RF_train_mean.to_csv('Result_RF/forest_importances_train_RF_Kp_1493_full.csv')
FI_RF_train_mean
273/82:
from sklearn.cross_decomposition import PLSRegression

model_PLS = PLSRegression(n_components=10)
scores=cross_val_score(model_PLS, train_x, train_y, cv=kf)

#calculate R2
model_fit = model_PLS.fit(train_x, train_y)
y_pred = model_PLS.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_PLS, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_PLS.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
273/83:
from sklearn.linear_model import Ridge
Ridge = Ridge(alpha=1.0)

#calculate R2
model_fit = Ridge.fit(train_x, train_y)
y_pred = Ridge.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(Ridge, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = Ridge.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
273/84:
from sklearn.neural_network import MLPRegressor
MLP = MLPRegressor(activation='relu', random_state=42, max_iter=1000, solver='lbfgs')


#calculate R2
model_fit = MLP.fit(train_x, train_y)
y_pred = MLP.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(MLP, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = MLP.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
273/85:
pd.DataFrame(train_x, indices_train).to_csv("Result_RF/RF_full_features/x_train_kp.csv")
pd.DataFrame(test_x, indices_test).to_csv("Result_RF/RF_full_features/x_test_kp.csv")
pd.DataFrame(y_cv, indices_train).to_csv("Result_RF/RF_full_features/y_cv_kp.csv")
pd.DataFrame(train_y, indices_train).to_csv("Result_RF/RF_full_features/train_y_kp.csv")
pd.DataFrame(test_y, indices_test).to_csv("Result_RF/RF_full_features/test_y_kp.csv")
pd.DataFrame(y_pred, indices_train).to_csv("Result_RF/RF_full_features/y_pred_kp.csv")
pd.DataFrame(y_ext, indices_test).to_csv("Result_RF/RF_full_features/y_SVR_ext_kp.csv")
273/86:
from sklearn.decomposition import PCA
pca = PCA(n_components=7)
pca.fit(fingerprint_to_model)
273/87:
from sklearn.decomposition import PCA
pca = PCA(n_components=7)
pca.fit(fingerprint_to_model)
print(pca.explained_variance_ratio_)
273/88:
from sklearn.decomposition import PCA
pca = PCA(n_components=7)
pca.fit(train_x)
print(pca.explained_variance_ratio_)
276/44:
from sklearn.decomposition import PCA
pca = PCA(n_components=7)
pca.fit(train_x_selected)
print(pca.explained_variance_ratio_)
285/1: from scipy import stats
285/2:
import numpy as np
import pandas as pd
from scipy import stats
285/3: pwd
285/4: df = pd.read_csv('LogCMratio.csv')
285/5: df = pd.read_csv('LogCMratio.csv')
285/6: df
285/7: stats.pearsonr(df[Log_CM_SK_37], df[Log_CM_SK_4])
285/8: df
285/9: stats.pearsonr(df['Log_CM_SK_37'], df['Log_CM_SK_4'])
285/10: stats.pearsonr(df['Log_CM_Ve_37'], df['Log_CM_Ve_4'])
287/1:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
287/2: df = pd.read_csv('rawcorrelation.csv')
287/3: df
287/4: df2 = df.drop(columns=['Unnamed: 0'])
287/5: df2
287/6:
from scipy.stats import pearsonr
rho = df2.corr()
pval = df2.corr(method=lambda x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)
p = pval.applymap(lambda x: ''.join(['*' for t in [0.001,0.01,0.05] if x<=t]))
Res_pearson_sig = rho.round(3).astype(str) + p
Res_pearson_sig.to_csv('Res_pearson_sig.csv')
Res_pearson_sig
287/7:
Res_pearson = df2.corr(method='pearson').round(3)
Res_pearson.to_csv('Res_pearson.csv')
Res_pearson
287/8: pval
287/9:
from scipy.stats import pearsonr
rho = df2.corr()
pval = df2.corr(method=lambda x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)
p = pval.applymap(lambda x: ''.join(['*' for t in [0.01,0.05,0.1] if x<=t]))
Res_pearson_sig = rho.round(3).astype(str) + p
Res_pearson_sig.to_csv('Res_pearson_sig.csv')
Res_pearson_sig
287/10: pval
287/11:
from scipy.stats import pearsonr
rho = df2.corr()
pval = df2.corr(method=lambda x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)
p = pval.applymap(lambda x: ''.join(['*' for t in [0.001,0.01,0.05] if x<=t]))
Res_pearson_sig = rho.round(3).astype(str) + p
Res_pearson_sig.to_csv('Res_pearson_sig.csv')
Res_pearson_sig
287/12: pval
290/1: df1 = pd.read_csv('spheroid.csv')
290/2:
import numpy as np
import pandas as pd
from scipy import stats
290/3: pwd
290/4: df = pd.read_csv('LogCMratio.csv')
290/5: df
290/6: stats.pearsonr(df['Log_CM_SK_37'], df['Log_CM_SK_4'])
290/7: stats.pearsonr(df['Log_CM_Ve_37'], df['Log_CM_Ve_4'])
290/8: df1 = pd.read_csv('spheroid.csv')
290/9: df1
290/10: shapiro_test = stats.shapiro(df1[volume])
290/11: shapiro_test = stats.shapiro(df1[Volume])
290/12: shapiro_test = stats.shapiro(df1['Volume'])
290/13: shapiro_test
290/14: shapiro_test = stats.shapiro(df1['CTCF_Green'])
290/15: shapiro_test
290/16: shapiro_test = stats.shapiro(df1['CTCF_Red'])
290/17: shapiro_test
290/18: df1['Volume']
290/19: df2 = df1.loc[df1['Samples'] == 'Control']
290/20: shapiro_test = stats.shapiro(df2['Volume'])
290/21: shapiro_test
290/22:
df2 = df1.loc[df1['Samples'] == 'Control']
df2
290/23: shapiro_test = stats.shapiro(df2['CTCF_Green'])
290/24: shapiro_test
290/25: shapiro_test = stats.shapiro(df2['CTCF_Red'])
290/26: shapiro_test
290/27:
df3 = df1.loc[df1['Samples'] == '3 mM']
df3
290/28: shapiro_test = stats.shapiro(df3['Volume'])
290/29:
shapiro_test = stats.shapiro(df3['Volume'])
shapiro_test
290/30:
shapiro_test = stats.shapiro(df3['CTCF_Green'])
shapiro_test
290/31:
shapiro_test = stats.shapiro(df3['CTCF_Red'])
shapiro_test
290/32:
df4 = df1.loc[df1['Samples'] == '6 mM']
df4
290/33:
shapiro_test = stats.shapiro(df4['Volume'])
shapiro_test
290/34:
shapiro_test = stats.shapiro(df4['CTCF_Green'])
shapiro_test
290/35:
shapiro_test = stats.shapiro(df4['CTCF_Red'])
shapiro_test
290/36: stats.kruskal(df1['Samples'], df1['Volume'])
290/37: stats.kruskal(df1['Volume'])
290/38: stats.kruskal(df2['Volume'], df3['Volume'], df4['Volume'])
290/39: stats.kruskal(df2['Volume'], df3['Volume'])
290/40: stats.kruskal(df2['Volume'], df3['Volume'])
290/41: stats.kruskal(df2['Volume'], df4['Volume'])
290/42: stats.kruskal(df2['Volume'], df4['Volume'])
290/43: stats.kruskal(df3['Volume'], df4['Volume'])
307/1:
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
307/2:
path = r'smiles/'

Fp13  = pd.read_csv(path + 'mordard_Kp.csv'               , header = 0)
307/3: Fp13.shape
307/4: Fp13
307/5:
df = pd.read_csv('df6_lipinski_Kpu_Kp_cells.csv')
df
307/6: name = df['compound']
307/7: name.shape
307/8:
df2 = df.rename({'compound': 'Name'}, axis=1)
df3 = df2.set_index('Name')
df3
307/9:
LogKp = df3[['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]
LogKp
307/10:
Fp13 = Fp13.rename(columns={'name': 'Name'})
Fp13
306/1:
import numpy as np
import pandas as pd
from scipy import stats
306/2: pwd
306/3: df = pd.read_csv('/Result_RF/forest_importances_train_RF_Kp_181.csv')
306/4: pwd
306/5: df = pd.read_csv('OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Result_RF/forest_importances_train_RF_Kp_181.csv')
306/6: df = pd.read_csv('r/Result_RF/forest_importances_train_RF_Kp_181.csv')
306/7: df = pd.read_csv('rResult_RF/forest_importances_train_RF_Kp_181.csv')
306/8: df = pd.read_csv(r'Result_RF/forest_importances_train_RF_Kp_181.csv')
306/9: df
307/11:
Fp13 = Fp13.rename(columns={'name': 'Name'})
Fp13
307/12: name = df['compound']
307/13: name.shape
307/14:
df2 = df.rename({'compound': 'Name'}, axis=1)
df3 = df2.set_index('Name')
df3
307/15:
LogKp = df3[['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]
LogKp
307/16:
Fp13 = Fp13.rename(columns={'name': 'Name'})
Fp13
307/17:

Fp13.Name = name
307/18:

Fpp13 = Fp13.drop('Name', axis=1)
Fpp13
307/19: Fpp13 = Fpp13.fillna(0)
307/20:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
307/21:

Fp13_n = normalized (Fpp13)
307/22: Fp13_n
307/23: np.any(np.isnan(Fp13_n))
307/24: Fp13_n["Name"] = Fp13.Name
307/25: Fp13_n
307/26: Fp13_n = Fp13_n.set_index('Name')
307/27: Fp13_n.head(3)
307/28: raw13  = LogKp.merge(Fp13_n, on='Name', how='outer')
307/29: raw13
307/30: np.any(np.isnan(raw13))
307/31:
raw13 =raw13[raw13.logkp.notna()]
raw13.shape
307/32: raw13.to_csv('QSAR_Kp/Mordard_Kp_144.csv' , sep=',' ,index=True)
307/33: print (len(raw13))
307/34: print (len(raw13 .columns))
307/35:
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.logkp.tolist()
307/36: fingerprint_to_model.head(3)
307/37: features_20 = fingerprint_to_model[['nFaRing', 'n9FaHRing', "n9FaRing", 'VSA_EState3', 'MAXaaaC', 'AXp-4dv', 'ATSC1m', 'SRW05', 'ATSC1Z', 'SlogP_VSA3', 'GGI5', 'PEOE_VSA6', 'C3SP2', 'MDEN-13', 'MINsCH3', 'EState_VSA9', 'GGI6', 'SaasN', 'VSA_EState8', 'EState_VSA4]]
307/38:
features_20 = fingerprint_to_model[['nFaRing', 'n9FaHRing', "n9FaRing", 'VSA_EState3', 'MAXaaaC', 'AXp-4dv', 
                                    'ATSC1m', 'SRW05', 'ATSC1Z', 'SlogP_VSA3', 'GGI5', 'PEOE_VSA6', 'C3SP2', 
                                    'MDEN-13', 'MINsCH3', 'EState_VSA9', 'GGI6', 'SaasN', 'VSA_EState8', 'EState_VSA4']]
307/39: features_20
307/40: indices = fingerprint_to_model.index
307/41:
train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(fingerprints_20, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
307/42:
train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(fingerprint_20, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
307/43:
train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(features_20, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
307/44:
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
308/1:
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
308/2:
path = r'smiles/'

Fp13  = pd.read_csv(path + 'mordard_Kp.csv'               , header = 0)
308/3: Fp13.shape
308/4: Fp13
308/5:
df = pd.read_csv('df6_lipinski_Kpu_Kp_cells.csv')
df
308/6: name = df['compound']
308/7: name.shape
308/8:
df2 = df.rename({'compound': 'Name'}, axis=1)
df3 = df2.set_index('Name')
df3
308/9:
LogKp = df3[['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]
LogKp
308/10:
Fp13 = Fp13.rename(columns={'name': 'Name'})
Fp13
308/11: Fp13.Name = name
308/12:

Fpp13 = Fp13.drop('Name', axis=1)
Fpp13
308/13: Fpp13 = Fpp13.fillna(0)
308/14:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
308/15: Fp13_n = normalized (Fpp13)
308/16: Fp13_n
308/17: np.any(np.isnan(Fp13_n))
308/18: Fp13_n["Name"] = Fp13.Name
308/19: Fp13_n
308/20: Fp13_n = Fp13_n.set_index('Name')
308/21: Fp13_n.head(3)
308/22: Fp13_n.to_csv('Fp_normalized/Morderd_LogKp.csv' , sep=',' ,index=True)
308/23: raw13  = LogKp.merge(Fp13_n, on='Name', how='outer')
308/24: raw13
308/25: np.any(np.isnan(raw13))
308/26: raw13
308/27: np.any(np.isnan(raw13))
308/28:
raw13 =raw13[raw13.logkp.notna()]
raw13.shape
308/29: print (len(raw13))
308/30: print (len(raw13 .columns))
308/31:
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.logkp.tolist()
308/32: fingerprint_to_model.head(3)
308/33: indices = fingerprint_to_model.index
308/34: indices = fingerprint_to_model.index
308/35:
features_20 = fingerprint_to_model[['nFaRing', 'n9FaHRing', "n9FaRing", 'VSA_EState3', 'MAXaaaC', 'AXp-4dv', 
                                    'ATSC1m', 'SRW05', 'ATSC1Z', 'SlogP_VSA3', 'GGI5', 'PEOE_VSA6', 'C3SP2', 
                                    'MDEN-13', 'MINsCH3', 'EState_VSA9', 'GGI6', 'SaasN', 'VSA_EState8', 'EState_VSA4']]
308/36: features_20
308/37:
train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(features_20, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
308/38: ##RF
308/39:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)


#calculate R2
model_RF_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
308/40:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=42)
308/41:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)


#calculate R2
model_RF_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
308/42:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test

test_x_selected = sel.transform(test_x)

y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)


print('Results')
print('===================================')
print("%0.2f CV accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
308/43:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test

test_x = sel.transform(test_x)

y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)


print('Results')
print('===================================')
print("%0.2f CV accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
308/44:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)


print('Results')
print('===================================')
print("%0.2f CV accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
308/45: features_20.tail(3)
308/46:
df = features_20.tail(3)
df
308/47: df.to_csv('Result_RF/Features_20_SM.csv')
308/48: indices = fingerprint_to_model.index
308/49:
train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
308/50: indices_train, indices_test
308/51:
train_x_selected = train_x.loc[:, mask]
train_x_selected
308/52:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model
sel = VarianceThreshold(threshold=(0.05))
sel = sel.fit(train_x)
mask = sel.get_support()
308/53:
train_x_selected = train_x.loc[:, mask]
train_x_selected
308/54: train_x_selected.columns
308/55:
import random
SEED = random.seed(10)
SEED
308/56:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=42)
308/57: train_x_selected.shape
308/58: test_x_selected = sel.transform(test_x)
308/59:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x_selected, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x_selected, train_y)
y_pred = SVR.predict(train_x_selected)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x_selected, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test

test_x_selected = sel.transform(test_x)

y_ext = SVR.predict(test_x_selected)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)


print('Results')
print('===================================')
print("%0.2f CV accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
308/60:
from sklearn.cross_decomposition import PLSRegression

model_PLS = PLSRegression(n_components=10)
scores=cross_val_score(model_PLS, train_x_selected, train_y, cv=kf)

#calculate R2
model_fit = model_PLS.fit(train_x_selected, train_y)
y_pred = model_PLS.predict(train_x_selected)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_PLS, train_x_selected, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_PLS.predict(test_x_selected)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
308/61: print(f"PLS r-squared {pls.score(test_x, test_y):.3f}")
308/62: print(f"PLS r-squared {model_fit.score(test_x, test_y):.3f}")
308/63: print(f"PLS r-squared {model_fit.score(test_y, y_exxt):.3f}")
308/64: print(f"PLS r-squared {model_fit.score(test_y, y_ext):.3f}")
308/65: model_fit.score(test_y, y_ext)
308/66:
from sklearn.cross_decomposition import PLSRegression

model_PLS = PLSRegression(n_components=10)
scores=cross_val_score(model_PLS, train_x_selected, train_y, cv=kf)

#calculate R2
model_fit = model_PLS.fit(train_x_selected, train_y)
y_pred = model_PLS.predict(train_x_selected)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_PLS, train_x_selected, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_PLS.predict(test_x_selected)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
308/67: y_exxt
308/68: y_ext
308/69: test_y
308/70: y_ext
308/71: model_PLS.score
308/72: model_PLS.score(train_x, train_y)
308/73: model_PLS.score(train_x_selected, train_y)
308/74: model_PLS.score(test_x_selected, y_ext)
308/75: model_PLS.score(test_x_selected, y_test)
308/76: model_PLS.score(test_x_selected, test_y)
309/1:
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
309/2: pwd
309/3:
list_Fp = ['Descriptor.xml', 'Fingerprinter.xml', 'ExtendedFingerprinter.xml', 'EStateFingerprinter.xml',
           'GraphOnlyFingerprinter.xml', 'MACCSFingerprinter.xml', 'PubchemFingerprinter.xml',
           'SubstructureFingerprinter.xml', 'KlekotaRothFingerprinter.xml', 'AtomPairs2DFingerprinter.xml', 'AtomPairs2DFingerprintCount.xml', 
           'KlekotaRothFingerprintCount.xml',
           'SubstructureFingerprintCount.xml' ]
309/4:
def Fingerprint(name , Fp):
    
    group = name
    group = [name.replace(".smi", ".csv") for name in group]
    result = ', '.join(group)
    
    Fp_name = Fp
    Fp_name = Fp.replace(".xml", "_")
    
    print (name)
    print (result)
    print (Fp)
    print (Fp_name)

    ! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/Fingerprinter.xml' \
    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/$name' \
    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/$Fp_name$result'
309/5:
! java -jar -Xms64m -Xmx200m '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/PaDEL-Descriptor.jar' \
    -removesalt -standardizenitro  \
    -fingerprints -descriptortypes '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/PaDEL-Descriptor/SubstructureFingerprintCount.xml' \
    -dir '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/smiles/new_smiles_Kp.smi' \
    -file '/Users/tarapongsrisongkram/OneDrive - Khon Kaen University (1)/Manuscript 4/Model/Fingerprint/SubstructureFingerprintCount_Kp.csv'
318/1:
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
318/2:
path = r'smiles/'

Fp13  = pd.read_csv(path + 'SubstructureFingerprintCount_Kp.csv'               , header = 0)
318/3:
path = r'Fingerprint/'

Fp13  = pd.read_csv(path + 'SubstructureFingerprintCount_Kp.csv'               , header = 0)
318/4: Fp13.shape
318/5: Fp13
318/6:
df = pd.read_csv('df6_lipinski_Kpu_Kp_cells.csv')
df
318/7: name = df['compound']
318/8: name.shape
318/9:
df2 = df.rename({'compound': 'Name'}, axis=1)
df3 = df2.set_index('Name')
df3
318/10:
LogKp = df3[['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]
LogKp
318/11:
Fp13 = Fp13.rename(columns={'name': 'Name'})
Fp13
318/12: Fp13.Name = name
318/13:
Fpp13 = Fp13.drop('Name', axis=1)
Fpp13
318/14: Fpp13 = Fpp13.fillna(0)
318/15: Fpp13 = Fpp13.fillna(0)
318/16:
#dont need to normalization becuase it only one class of finggerprint
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
318/17: #Fp13_n = normalized (Fpp13)
318/18: #Fp13_n
318/19: np.any(np.isnan(Fp13))
318/20: np.any(np.isnan(Fpp13))
318/21:
#Fp13_n = normalized (Fpp13)
Fp13_n = Fpp13
318/22: Fp13_n
318/23: np.any(np.isnan(Fp13_n))
318/24: Fp13_n["Name"] = Fp13.Name
318/25: Fp13_n
318/26: Fp13_n = Fp13_n.set_index('Name')
318/27: Fp13_n.head(3)
318/28: raw13  = LogKp.merge(Fp13_n, on='Name', how='outer')
318/29: raw13
318/30: Fp13_n.shape
318/31: np.any(np.isnan(raw13))
318/32: raw13.shape
318/33: Fp13_n.shape
318/34: print (len(raw13))
318/35: print (len(raw13 .columns))
318/36:
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.logkp.tolist()
318/37: fingerprint_to_model.head(3)
318/38:
LogKp = df3[['logkp']]
LogKp
318/39:
Fp13 = Fp13.rename(columns={'name': 'Name'})
Fp13
318/40: Fp13.Name = name
318/41:
Fpp13 = Fp13.drop('Name', axis=1)
Fpp13
318/42: Fpp13 = Fpp13.fillna(0)
318/43:
#dont need to normalization becuase it only one class of finggerprint
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
318/44:
#Fp13_n = normalized (Fpp13)
Fp13_n = Fpp13
318/45: Fp13_n
318/46: np.any(np.isnan(Fp13_n))
318/47: Fp13_n["Name"] = Fp13.Name
318/48: Fp13_n
318/49: Fp13_n = Fp13_n.set_index('Name')
318/50: Fp13_n.head(3)
318/51: Fp13_n.shape
318/52: raw13  = LogKp.merge(Fp13_n, on='Name', how='outer')
318/53: raw13
318/54: np.any(np.isnan(raw13))
318/55: raw13.shape
318/56: raw13.to_csv('QSAR_Kp/Substructure_Kp_144.csv' , sep=',' ,index=True)
318/57: print (len(raw13))
318/58: print (len(raw13 .columns))
318/59:
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.logkp.tolist()
318/60:
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['logkp'], axis=1)
label_to_model = compound_df.logkp.tolist()
318/61:
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['logkp'], axis=1)
label_to_model = compound_df.logkp.tolist()
318/62: fingerprint_to_model.head(3)
318/63: indices = fingerprint_to_model.index
318/64:
train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
318/65: indices_train, indices_test
318/66:
import random
SEED = random.seed(10)
SEED
318/67:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=42)
318/68:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test

test_x_selected = sel.transform(test_x)

y_ext = SVR.predict(test_x_selected)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)


print('Results')
print('===================================')
print("%0.2f CV accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
318/69:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test

test_x = sel.transform(test_x)

y_ext = SVR.predict(test_x_selected)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)


print('Results')
print('===================================')
print("%0.2f CV accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
318/70:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test


y_ext = SVR.predict(test_x_selected)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)


print('Results')
print('===================================')
print("%0.2f CV accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
318/71:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test


y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)


print('Results')
print('===================================')
print("%0.2f CV accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
318/72:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)


#calculate R2
model_RF_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
318/73:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)


print('Results')
print('===================================')
print("%0.2f CV accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
318/74:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model
sel = VarianceThreshold(threshold=(0.05))
sel = sel.fit(train_x)
mask = sel.get_support()
318/75:
train_x_selected = train_x.loc[:, mask]
train_x_selected
318/76: train_x_selected.columns
318/77:
import random
SEED = random.seed(10)
SEED
318/78:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=42)
318/79: train_x_selected.shape
318/80: test_x_selected = sel.transform(test_x)
318/81:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x_selected, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x_selected, train_y)
y_pred = SVR.predict(train_x_selected)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x_selected, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test

test_x_selected = sel.transform(test_x)

y_ext = SVR.predict(test_x_selected)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)


print('Results')
print('===================================')
print("%0.2f CV accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
318/82:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='linear')
scores=cross_val_score(SVR, train_x_selected, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x_selected, train_y)
y_pred = SVR.predict(train_x_selected)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x_selected, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test

y_ext = SVR.predict(test_x_selected)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
318/83:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x_selected, train_y, cv=kf)


#calculate R2
model_RF_fit = model_RF.fit(train_x_selected, train_y)
y_pred = model_RF.predict(train_x_selected)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x_selected, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x_selected)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
318/84:
#Train features
from sklearn.inspection import permutation_importance
result = permutation_importance(model_RF, train_x_selected, train_y, n_repeats=10, random_state=42, n_jobs=2)
feature_names = train_x_selected.columns

forest_importances = pd.Series(result.importances_mean, index=feature_names)
fig, ax = plt.subplots()
forest_importances.plot.bar(yerr=result.importances_std, ax=ax)
ax.set_title("Feature importances using permutation on training model")
ax.set_ylabel("Mean accuracy decrease")
fig.tight_layout()
plt.show()
318/85:
FI_RF_train_mean = pd.DataFrame(forest_importances)
FI_RF_train_mean.to_csv('Result_RF/forest_importances_train_RF_Kp_Substructure44.csv')
FI_RF_train_mean
318/86:
from sklearn.decomposition import PCA
pca = PCA(n_components=7)
pca.fit(train_x_selected)
print(pca.explained_variance_ratio_)
318/87:
from sklearn.cross_decomposition import PLSRegression

model_PLS = PLSRegression(n_components=10)
scores=cross_val_score(model_PLS, train_x_selected, train_y, cv=kf)

#calculate R2
model_fit = model_PLS.fit(train_x_selected, train_y)
y_pred = model_PLS.predict(train_x_selected)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_PLS, train_x_selected, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_PLS.predict(test_x_selected)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
318/88: model_PLS.score(test_x_selected, test_y)
318/89:
model_PLS.score(test_x_selected, test_y)
pls = PLSRegression(n_components=7)
pls.fit(train_x_selected)
print(pca.explained_variance_ratio_)
318/90:
model_PLS.score(test_x_selected, test_y)
pls = PLSRegression(n_components=7)
pls.fit(train_x_selected, train_y)
print(pca.explained_variance_ratio_)
318/91:
model_PLS.score(test_x_selected, test_y)
pls = PLSRegression(n_components=7)
print(pls.fit(train_x_selected, train_y))
print(pls.explained_variance_ratio_)
318/92:
model_PLS.score(test_x_selected, test_y)
pls = PLSRegression(n_components=7)
print(pls.fit(train_x_selected, train_y))
print(pls.explained_variance)
318/93:
model_PLS.score(test_x_selected, test_y)
pls = PLSRegression(n_components=7)
print(pls.fit(train_x_selected, train_y))
318/94: model_PLS.score(test_x_selected, test_y)
318/95: model_PLS.score(train_x_selected, train_y)
318/96:
from sklearn.linear_model import Ridge
Ridge = Ridge(alpha=1.0)

#calculate R2
model_fit = Ridge.fit(train_x_selected, train_y)
y_pred = Ridge.predict(train_x_selected)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(Ridge, train_x_selected, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = Ridge.predict(test_x_selected)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
318/97:
from sklearn.neural_network import MLPRegressor
MLP = MLPRegressor(activation='relu', random_state=42, max_iter=1000, solver='lbfgs')


#calculate R2
model_fit = MLP.fit(train_x_selected, train_y)
y_pred = MLP.predict(train_x_selected)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(MLP, train_x_selected, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = MLP.predict(test_x_selected)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
308/77:
features_20 = fingerprint_to_model[['nFaRing', 'n9FaHRing', "n9FaRing", 'VSA_EState3', 'MAXaaaC', 'AXp-4dv', 
                                    'ATSC1m', 'SRW05', 'ATSC1Z', 'SlogP_VSA3', 'GGI5', 'PEOE_VSA6', 'C3SP2', 
                                    'MDEN-13', 'MINsCH3', 'EState_VSA9', 'GGI6', 'SaasN', 'VSA_EState8', 'EState_VSA4']]
308/78: features_20
308/79:
df = features_20.tail(3)
df
308/80:
train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(features_20, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
308/81:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)


#calculate R2
model_RF_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
308/82:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)


print('Results')
print('===================================')
print("%0.2f CV accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
308/83:
features_10 = fingerprint_to_model[['nFaRing', 'n9FaHRing', "n9FaRing", 'VSA_EState3', 'MAXaaaC', 'AXp-4dv', 
                                    'ATSC1m', 'SRW05', 'ATSC1Z', 'SlogP_VSA3']]
308/84:
df = features_10.tail(3)
df
308/85:
train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(features_20, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
308/86:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)


#calculate R2
model_RF_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
308/87: train_x.shape
308/88:
train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(features_10, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
308/89: train_x.shape
308/90:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)


#calculate R2
model_RF_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
308/91:
from sklearn.cross_decomposition import PLSRegression

model_PLS = PLSRegression(n_components=10)
scores=cross_val_score(model_PLS, train_x, train_y, cv=kf)

#calculate R2
model_PLS.fit(train_x_selected, train_y)
y_pred = model_PLS.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_PLS, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_PLS.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
308/92:
from sklearn.cross_decomposition import PLSRegression

model_PLS = PLSRegression(n_components=10)
scores=cross_val_score(model_PLS, train_x, train_y, cv=kf)

#calculate R2
model_PLS.fit(train_x, train_y)
y_pred = model_PLS.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_PLS, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_PLS.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
319/1:
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
319/2:
path = r'smiles/'

Fp13  = pd.read_csv(path + 'mordard_Kp.csv'               , header = 0)
319/3: Fp13.shape
319/4: Fp13
319/5:
df = pd.read_csv('df6_lipinski_Kpu_Kp_cells.csv')
df
319/6: name = df['compound']
319/7: name.shape
319/8:
df2 = df.rename({'compound': 'Name'}, axis=1)
df3 = df2.set_index('Name')
df3
319/9:
LogKp = df3[['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]
LogKp
319/10:
Fp13 = Fp13.rename(columns={'name': 'Name'})
Fp13
319/11:

Fp13.Name = name
319/12:

Fpp13 = Fp13.drop('Name', axis=1)
Fpp13
319/13: Fpp13 = Fpp13.fillna(0)
319/14:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
319/15:

Fp13_n = normalized (Fpp13)
319/16: Fp13_n
319/17: np.any(np.isnan(Fp13_n))
319/18: Fp13_n["Name"] = Fp13.Name
319/19: Fp13_n
319/20: Fp13_n = Fp13_n.set_index('Name')
319/21: Fp13_n.head(3)
319/22: raw13  = LogKp.merge(Fp13_n, on='Name', how='outer')
319/23: raw13
319/24: np.any(np.isnan(raw13))
319/25:
raw13 =raw13[raw13.logkp.notna()]
raw13.shape
319/26: print (len(raw13))
319/27: print (len(raw13 .columns))
319/28:
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.logkp.tolist()
319/29: fingerprint_to_model.head(3)
319/30: indices = fingerprint_to_model.index
319/31:
train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
319/32:
#not reduce
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['logkpu', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds'], axis=1)
label_to_model = compound_df.logkpu.tolist()
319/33:
train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
319/34:
train_x, test_x, train_y, test_y,indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
319/35:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

param = {
    "n_estimators": 100,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
319/36: indices_train, indices_test
319/37:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=42)
319/38:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

param = {
    "n_estimators": 100,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
319/39:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

param = {
    "n_estimators": 100,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
319/40:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

param = {
    "n_estimators": 100,  # number of trees to grows
}
model_RF = RandomForestRegressor(**param)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))



#calculate R2
SVR_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
320/1:
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
320/2:
path = r'smiles/'

Fp13  = pd.read_csv(path + 'mordard_Kp.csv'               , header = 0)
320/3: Fp13.shape
320/4: Fp13
320/5:
df = pd.read_csv('df6_lipinski_Kpu_Kp_cells.csv')
df
320/6: name = df['compound']
320/7: name.shape
320/8:
df2 = df.rename({'compound': 'Name'}, axis=1)
df3 = df2.set_index('Name')
df3
320/9:
LogKp = df3[['logkp', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds']]
LogKp
320/10:
Fp13 = Fp13.rename(columns={'name': 'Name'})
Fp13
320/11: Fp13.Name = name
320/12:
Fpp13 = Fp13.drop('Name', axis=1)
Fpp13
320/13: Fpp13 = Fpp13.fillna(0)
320/14:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
320/15: Fp13_n = normalized (Fpp13)
320/16: Fp13_n
320/17: np.any(np.isnan(Fp13_n))
320/18: Fp13_n["Name"] = Fp13.Name
320/19: Fp13_n
320/20: Fp13_n = Fp13_n.set_index('Name')
320/21: Fp13_n.head(3)
320/22: raw13  = LogKp.merge(Fp13_n, on='Name', how='outer')
320/23: raw13
320/24: np.any(np.isnan(raw13))
320/25:
path = r'smiles/'

Fp13  = pd.read_csv(path + 'mordard_Kp.csv'               , header = 0)
320/26: Fp13.shape
320/27: Fp13
320/28:
df = pd.read_csv('df6_lipinski_Kpu_Kp_cells.csv')
df
320/29: name = df['compound']
320/30: name.shape
320/31:
df2 = df.rename({'compound': 'Name'}, axis=1)
df3 = df2.set_index('Name')
df3
320/32:
LogKp = df3[['logkp']]
LogKp
320/33:
Fp13 = Fp13.rename(columns={'name': 'Name'})
Fp13
320/34: Fp13.Name = name
320/35:
Fpp13 = Fp13.drop('Name', axis=1)
Fpp13
320/36: Fpp13 = Fpp13.fillna(0)
320/37:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
320/38: Fp13_n = normalized (Fpp13)
320/39: Fp13_n
320/40: np.any(np.isnan(Fp13_n))
320/41: Fp13_n["Name"] = Fp13.Name
320/42: Fp13_n
320/43: Fp13_n = Fp13_n.set_index('Name')
320/44: Fp13_n.head(3)
320/45: raw13  = LogKp.merge(Fp13_n, on='Name', how='outer')
320/46:
LogKp = df3[['logkp']]
LogKp
320/47:
Fp13 = Fp13.rename(columns={'name': 'Name'})
Fp13
320/48: Fp13.Name = name
320/49:
Fpp13 = Fp13.drop('Name', axis=1)
Fpp13
320/50: Fpp13 = Fpp13.fillna(0)
320/51:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
320/52: Fp13_n = normalized (Fpp13)
320/53: Fp13_n
320/54: np.any(np.isnan(Fp13_n))
320/55: Fp13_n["Name"] = Fp13.Name
320/56: Fp13_n
320/57: Fp13_n = Fp13_n.set_index('Name')
320/58: Fp13_n.head(3)
320/59: raw13  = LogKp.merge(Fp13_n, on='Name', how='outer')
320/60: raw13
320/61: np.any(np.isnan(raw13))
320/62:
raw13 =raw13[raw13.logkp.notna()]
raw13.shape
320/63: print (len(raw13))
320/64: print (len(raw13 .columns))
320/65:
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['logkp'], axis=1)
label_to_model = compound_df.logkp.tolist()
320/66: fingerprint_to_model.head(3)
320/67: indices = fingerprint_to_model.index
320/68:
train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
320/69: indices_train, indices_test
320/70:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model

#sel = VarianceThreshold(threshold=(0.05))
#sel = sel.fit(train_x)
#mask = sel.get_support()
320/71:
#train_x_selected = train_x.loc[:, mask]
#train_x_selected
320/72: train_x_selected.columns
320/73: #train_x_selected.columns
320/74:
import random
SEED = random.seed(10)
SEED
320/75:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=42)
320/76: #train_x_selected.shape
320/77: #test_x_selected = sel.transform(test_x)
320/78:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test

test_x = sel.transform(test_x)

y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)


print('Results')
print('===================================')
print("%0.2f CV accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
320/79:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


SVR = SVR(kernel='rbf')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)


print('Results')
print('===================================')
print("%0.2f CV accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
320/80:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='linear')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x_selected)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test

y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
320/81:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='linear')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test

y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
320/82:
from sklearn.svm import SVR
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error

SVR = SVR(kernel='linear')
scores=cross_val_score(SVR, train_x, train_y, cv=kf)



#calculate R2
SVR_fit = SVR.fit(train_x, train_y)
y_pred = SVR.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(SVR, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test

y_ext = SVR.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
320/83:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x_selected, train_y, cv=kf)


#calculate R2
model_RF_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
320/84:

from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)


#calculate R2
model_RF_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
320/85:
#Train features
from sklearn.inspection import permutation_importance
result = permutation_importance(model_RF, train_x, train_y, n_repeats=10, random_state=42, n_jobs=2)
feature_names = train_x.columns

forest_importances = pd.Series(result.importances_mean, index=feature_names)
fig, ax = plt.subplots()
forest_importances.plot.bar(yerr=result.importances_std, ax=ax)
ax.set_title("Feature importances using permutation on full model")
ax.set_ylabel("Mean accuracy decrease")
fig.tight_layout()
plt.show()
320/86:
FI_RF_train_mean = pd.DataFrame(forest_importances)
FI_RF_train_mean.to_csv('Result_RF/forest_importances_train_RF_Kp_1493.csv')
FI_RF_train_mean
320/87:
from sklearn.decomposition import PCA
pca = PCA(n_components=7)
pca.fit(train_x)
print(pca.explained_variance_ratio_)
320/88:
from sklearn.cross_decomposition import PLSRegression

model_PLS = PLSRegression(n_components=10)
scores=cross_val_score(model_PLS, train_x, train_y, cv=kf)

#calculate R2
model_PLS.fit(train_x, train_y)
y_pred = model_PLS.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_PLS, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_PLS.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
320/89: model_PLS.score(test_x, test_y)
320/90:
from sklearn.linear_model import Ridge
Ridge = Ridge(alpha=1.0)

#calculate R2
model_fit = Ridge.fit(train_x_selected, train_y)
y_pred = Ridge.predict(train_x_selected)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(Ridge, train_x_selected, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = Ridge.predict(test_x_selected)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
320/91:
from sklearn.neural_network import MLPRegressor
MLP = MLPRegressor(activation='relu', random_state=42, max_iter=1000, solver='lbfgs')


#calculate R2
model_fit = MLP.fit(train_x, train_y)
y_pred = MLP.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(MLP, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = MLP.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')

plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
320/92:
forest_importances_sd = pd.Series(result.importances_std, index=feature_names)
FI_RF_train_mean = pd.DataFrame(forest_importances)
FI_RF_train_sd = pd.DataFrame(forest_importances)
FI_RF_train_all = pd.concat(FI_RF_train_mean, FI_RF_train_sd, axis = 1)
FI_RF_train_mean.to_csv('Result_RF/forest_importances_train_RF_Kp_1493.csv')
FI_RF_train_mean
320/93:
forest_importances_sd = pd.Series(result.importances_std, index=feature_names)
FI_RF_train_mean = pd.DataFrame(forest_importances)
FI_RF_train_sd = pd.DataFrame(forest_importances)
FI_RF_train_all = pd.concat(FI_RF_train_mean, FI_RF_train_sd, axis=1)
FI_RF_train_mean.to_csv('Result_RF/forest_importances_train_RF_Kp_1493.csv')
FI_RF_train_mean
320/94:
forest_importances_sd = pd.Series(result.importances_std, index=feature_names)
FI_RF_train_mean = pd.DataFrame(forest_importances)
FI_RF_train_sd = pd.DataFrame(forest_importances)
FI_RF_train_all = pd.concat([FI_RF_train_mean, FI_RF_train_sd], axis=1)
FI_RF_train_mean.to_csv('Result_RF/forest_importances_train_RF_Kp_1493.csv')
FI_RF_train_mean
320/95:
forest_importances_sd = pd.Series(result.importances_std, index=feature_names)
FI_RF_train_mean = pd.DataFrame(forest_importances)
FI_RF_train_sd = pd.DataFrame(forest_importances)
FI_RF_train_all = pd.concat([FI_RF_train_mean, FI_RF_train_sd], axis=1)
FI_RF_train_mean.to_csv('Result_RF/forest_importances_train_RF_Kp_1493.csv')
FI_RF_train_all
320/96:
forest_importances_sd = pd.Series(result.importances_std, index=feature_names, name=SD)
forest_importances = pd.Series(result.importances_mean, index=feature_names, name=Mean)
FI_RF_train_mean = pd.DataFrame(forest_importances)
FI_RF_train_sd = pd.DataFrame(forest_importances_sd)
FI_RF_train_all = pd.concat([FI_RF_train_mean, FI_RF_train_sd], axis=1)
FI_RF_train_all.to_csv('Result_RF/forest_importances_train_RF_Kp_1493.csv')
FI_RF_train_all
320/97:
forest_importances_sd = pd.Series(result.importances_std, index=feature_names, name='SD')
forest_importances = pd.Series(result.importances_mean, index=feature_names, name='Mean')
FI_RF_train_mean = pd.DataFrame(forest_importances)
FI_RF_train_sd = pd.DataFrame(forest_importances_sd)
FI_RF_train_all = pd.concat([FI_RF_train_mean, FI_RF_train_sd], axis=1)
FI_RF_train_all.to_csv('Result_RF/forest_importances_train_RF_Kp_1493.csv')
FI_RF_train_all
320/98: FI_RF_train_all.sort_values(by=['Mean'])
320/99: FI_20 = FI_RF_train_all.sort_values(by=['Mean'])
320/100:
FI_20 = FI_RF_train_all.sort_values(by=['Mean'])
FI_20
320/101:
FI_20 = FI_RF_train_all.sort_values(by=['Mean'])
FI_20.tail(20)
320/102:
FI = FI_RF_train_all.sort_values(by=['Mean'])
FI_20 = FI.tail(20)
FI_20
320/103:
features_20 = fingerprint_to_model[['PEOE_VSA10', 'AATS8p', "ATSC4are", 'AATSC4se', 'ATSC3s', 'SlogP_VSA10', 
                                    'VSA_EState4', 'nFaRing', 'ATSC4c', 'GATS3c']]
320/104:
features_10 = fingerprint_to_model[['PEOE_VSA10', 'AATS8p', "ATSC4are", 'AATSC4se', 'ATSC3s', 'SlogP_VSA10', 
                                    'VSA_EState4', 'nFaRing', 'ATSC4c', 'GATS3c']]
320/105:
FI = FI_RF_train_all.sort_values(by=['Mean'])
FI_10 = FI.tail(10)
FI_10
320/106: features_10
320/107:
df = features_10.tail(3)
df
320/108:
train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(features_10, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
320/109:
train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(features_10, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
320/110: train_x.shape
320/111:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)


#calculate R2
model_RF_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
320/112:
FI_20 = FI.tail(20)
FI_20
320/113:
features_20 = fingerprint_to_model[['PEOE_VSA10', 'AATS8p', "ATSC4are", 'AATSC4se', 'ATSC3s', 'SlogP_VSA10', 
                                    'VSA_EState4', 'nFaRing', 'ATSC4c', 'GATS3c', 'MATS1c', 'AATSC4pe', 
                                    'AATSC4are', 'ETA_dAlpha_B', 'AATSC1c', 'MAXaaaC', 'MATS4c', 'GATS3se',
                                   'MATS2Z', 'ATSC1m']]
320/114:
df = features_20.tail(3)
df
320/115:
train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(features_20, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
320/116: train_x.shape
320/117:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)


#calculate R2
model_RF_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
320/118: indices = fingerprint_to_model.index
320/119:
train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
320/120: indices_train, indices_test
320/121:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=42)
320/122:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)


#calculate R2
model_RF_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
320/123:
train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(features_20, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
320/124: train_x.shape
320/125:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)


#calculate R2
model_RF_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
320/126:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)


#calculate R2
model_RF_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
320/127:
#Train features
from sklearn.inspection import permutation_importance
result = permutation_importance(model_RF, train_x, train_y, n_repeats=10, random_state=42, n_jobs=-1)
feature_names = train_x.columns
forest_importances_sd = pd.Series(result.importances_std, index=feature_names, name='SD')
forest_importances = pd.Series(result.importances_mean, index=feature_names, name='Mean')
fig, ax = plt.subplots()
forest_importances.plot.bar(yerr=result.importances_std, ax=ax)
ax.set_title("Feature importances using permutation on 20 features model")
ax.set_ylabel("Mean accuracy decrease")
fig.tight_layout()
plt.show()
FI_RF_train_mean = pd.DataFrame(forest_importances)
FI_RF_train_sd = pd.DataFrame(forest_importances_sd)
FI_RF_train_all = pd.concat([FI_RF_train_mean, FI_RF_train_sd], axis=1)
FI_RF_train_all.to_csv('Result_RF/forest_importances_train_RF_Kp_20.csv')
FI_RF_train_all
320/128: FI_RF_train_all.sort_values(by=['Mean'])
320/129: model_RF.feature_importances_
321/1:
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole
from rdkit.Chem import Descriptors
from rdkit.Chem import AllChem
from rdkit import DataStructs
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from collections import defaultdict
321/2:
path = r'smiles/'

Fp13  = pd.read_csv(path + 'mordard_Kp.csv'               , header = 0)
321/3: Fp13.shape
321/4: Fp13
321/5: Fp13
321/6:
df = pd.read_csv('df6_lipinski_Kpu_Kp_cells.csv')
df
321/7: name = df['compound']
321/8: name.shape
321/9:
df2 = df.rename({'compound': 'Name'}, axis=1)
df3 = df2.set_index('Name')
df3
321/10:
LogKp = df3[['logkp']]
LogKp
321/11:
Fp13 = Fp13.rename(columns={'name': 'Name'})
Fp13
321/12: Fp13.Name = name
321/13:
Fpp13 = Fp13.drop('Name', axis=1)
Fpp13
321/14: Fpp13 = Fpp13.fillna(0)
321/15:
from sklearn import preprocessing

def normalized (Fp):
    
    min_max_scaler = preprocessing.MinMaxScaler()
    np_scaled = min_max_scaler.fit_transform(Fp)
    Fp_normalized = pd.DataFrame(np_scaled, columns=Fp.columns)
    Fp_normalized
    
    return Fp_normalized
321/16: Fp13_n = normalized (Fpp13)
321/17: Fp13_n
321/18: np.any(np.isnan(Fp13_n))
321/19: Fp13_n["Name"] = Fp13.Name
321/20: Fp13_n
321/21: Fp13_n = Fp13_n.set_index('Name')
321/22: Fp13_n.head(3)
321/23: Fp13_n.to_csv('Fp_normalized/Morderd_LogKp.csv' , sep=',' ,index=True)
321/24: raw13  = LogKp.merge(Fp13_n, on='Name', how='outer')
321/25: raw13
321/26: np.any(np.isnan(raw13))
321/27:
raw13 =raw13[raw13.logkp.notna()]
raw13.shape
321/28: raw13.to_csv('QSAR_Kp/Mordard_Kp_144_full.csv' , sep=',' ,index=True)
321/29: print (len(raw13))
321/30: print (len(raw13 .columns))
321/31:
compound_df = raw13.copy()
fingerprint_to_model = compound_df.drop(['logkp'], axis=1)
label_to_model = compound_df.logkp.tolist()
321/32: fingerprint_to_model.head(3)
321/33: indices = fingerprint_to_model.index
321/34:
train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
321/35: indices_train, indices_test
321/36:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model

#sel = VarianceThreshold(threshold=(0.05))
#sel = sel.fit(train_x)
#mask = sel.get_support()
321/37:
#train_x_selected = train_x.loc[:, mask]
#train_x_selected
321/38: #train_x_selected.columns
321/39:
import random
SEED = random.seed(10)
SEED
321/40:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=42)
321/41: #train_x_selected.shape
321/42: #test_x_selected = sel.transform(test_x)
321/43:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)


#calculate R2
model_RF_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
321/44:
pd.DataFrame(train_x, indices_train).to_csv("Result_RF/1493_features/x_train_kp.csv")
pd.DataFrame(test_x, indices_test).to_csv("Result_RF/1493_features/x_test_kp.csv")
pd.DataFrame(y_cv, indices_train).to_csv("Result_RF/1493_features/y_cv_kp.csv")
pd.DataFrame(train_y, indices_train).to_csv("Result_RF/1493_features/train_y_kp.csv")
pd.DataFrame(test_y, indices_test).to_csv("Result_RF/1493_features/test_y_kp.csv")
pd.DataFrame(y_pred, indices_train).to_csv("Result_RF/1493_features/y_pred_kp.csv")
pd.DataFrame(y_ext, indices_test).to_csv("Result_RF/1493_features/y_SVR_ext_kp.csv")
321/45: plt.barh(feature_names , model_RF.feature_importances_)
321/46:
#Train features
from sklearn.inspection import permutation_importance
result = permutation_importance(model_RF, train_x, train_y, n_repeats=10, random_state=42, n_jobs=-1)
feature_names = train_x.columns
forest_importances_sd = pd.Series(result.importances_std, index=feature_names, name='SD')
forest_importances = pd.Series(result.importances_mean, index=feature_names, name='Mean')
fig, ax = plt.subplots()
forest_importances.plot.bar(yerr=result.importances_std, ax=ax)
ax.set_title("Feature importances using permutation on 20 features model")
ax.set_ylabel("Mean accuracy decrease")
fig.tight_layout()
plt.show()
FI_RF_train_mean = pd.DataFrame(forest_importances)
FI_RF_train_sd = pd.DataFrame(forest_importances_sd)
FI_RF_train_all = pd.concat([FI_RF_train_mean, FI_RF_train_sd], axis=1)
FI_RF_train_all.to_csv('Result_RF/forest_importances_train_RF_Kp_20.csv')
FI_RF_train_all
321/47:
FI = FI_RF_train_all.sort_values(by=['Mean'])
FI_10 = FI.tail(10)
FI_10
321/48:
features_10 = fingerprint_to_model[['PEOE_VSA10', 'AATS8p', "ATSC4are", 'AATSC4se', 'ATSC3s', 'SlogP_VSA10', 
                                    'VSA_EState4', 'nFaRing', 'ATSC4c', 'GATS3c']]
321/49: features_10
321/50:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)


#calculate R2
model_RF_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
321/51: plt.barh(feature_names , model_RF.feature_importances_)
321/52:
forest_importances_sd = pd.Series(result.importances_std, index=feature_names, name='SD')
forest_importances = pd.Series(result.importances_mean, index=feature_names, name='Mean')
FI_RF_train_mean = pd.DataFrame(forest_importances)
FI_RF_train_sd = pd.DataFrame(forest_importances_sd)
FI_RF_train_all = pd.concat([FI_RF_train_mean, FI_RF_train_sd], axis=1)
FI_RF_train_all.to_csv('Result_RF/forest_importances_train_RF_Kp_1493.csv')
FI_RF_train_all
321/53:
FI = FI_RF_train_all.sort_values(by=['Mean'])
FI_10 = FI.tail(10)
FI_10
321/54:
features_10 = fingerprint_to_model[['PEOE_VSA10', 'AATS8p', "ATSC4are", 'AATSC4se', 'ATSC3s', 'SlogP_VSA10', 
                                    'VSA_EState4', 'nFaRing', 'ATSC4c', 'GATS3c']]
321/55: features_10
321/56:
df = features_10.tail(3)
df
321/57: df.to_csv('Result_RF/Features_10_SM.csv')
321/58:
train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(features_10, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
321/59: train_x.shape
321/60:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)


#calculate R2
model_RF_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
321/61: plt.barh(feature_names , model_RF.feature_importances_)
321/62: model_RF.feature_importances_
321/63: plt.barh(feature_names, model_RF.feature_importances_)
321/64:
feature_names = train_x.columns
model_RF.feature_importances_
321/65: plt.barh(feature_names, model_RF.feature_importances_)
321/66:
sorted_idx = model_RF.feature_importances_.argsort()
plt.barh(feature_names[sorted_idx], model_RF.feature_importances_[sorted_idx])
plt.xlabel("Random Forest Feature Importance")
321/67:
pd.DataFrame(train_x, indices_train).to_csv("Result_RF/10_features/x_train_kp.csv")
pd.DataFrame(test_x, indices_test).to_csv("Result_RF/10_features/x_test_kp.csv")
pd.DataFrame(y_cv, indices_train).to_csv("Result_RF/10_features/y_cv_kp.csv")
pd.DataFrame(train_y, indices_train).to_csv("Result_RF/10_features/train_y_kp.csv")
pd.DataFrame(test_y, indices_test).to_csv("Result_RF/10_features/test_y_kp.csv")
pd.DataFrame(y_pred, indices_train).to_csv("Result_RF/10_features/y_pred_kp.csv")
pd.DataFrame(y_ext, indices_test).to_csv("Result_RF/10_features/y_SVR_ext_kp.csv")
321/68:
features_20 = fingerprint_to_model[['PEOE_VSA10', 'AATS8p', "ATSC4are", 'AATSC4se', 'ATSC3s', 'SlogP_VSA10', 
                                    'VSA_EState4', 'nFaRing', 'ATSC4c', 'GATS3c', 'MATS1c', 'AATSC4pe', 
                                    'AATSC4are', 'ETA_dAlpha_B', 'AATSC1c', 'MAXaaaC', 'MATS4c', 'GATS3se',
                                   'MATS2Z', 'ATSC1m']]
321/69:
df = features_20.tail(3)
df
321/70: df.to_csv('Result_RF/Features_20_SM.csv')
321/71:
train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(features_20, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
321/72: train_x.shape
321/73:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)


#calculate R2
model_RF_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
321/74:
pd.DataFrame(train_x, indices_train).to_csv("Result_RF/20_features/x_train_kp.csv")
pd.DataFrame(test_x, indices_test).to_csv("Result_RF/20_features/x_test_kp.csv")
pd.DataFrame(y_cv, indices_train).to_csv("Result_RF/20_features/y_cv_kp.csv")
pd.DataFrame(train_y, indices_train).to_csv("Result_RF/20_features/train_y_kp.csv")
pd.DataFrame(test_y, indices_test).to_csv("Result_RF/20_features/test_y_kp.csv")
pd.DataFrame(y_pred, indices_train).to_csv("Result_RF/20_features/y_pred_kp.csv")
pd.DataFrame(y_ext, indices_test).to_csv("Result_RF/20_features/y_SVR_ext_kp.csv")
321/75:
#Train features
from sklearn.inspection import permutation_importance
result = permutation_importance(model_RF, train_x, train_y, n_repeats=10, random_state=42, n_jobs=-1)
feature_names = train_x.columns
forest_importances_sd = pd.Series(result.importances_std, index=feature_names, name='SD')
forest_importances = pd.Series(result.importances_mean, index=feature_names, name='Mean')
fig, ax = plt.subplots()
forest_importances.plot.bar(yerr=result.importances_std, ax=ax)
ax.set_title("Feature importances using permutation on 20 features model")
ax.set_ylabel("Mean accuracy decrease")
fig.tight_layout()
plt.show()
FI_RF_train_mean = pd.DataFrame(forest_importances)
FI_RF_train_sd = pd.DataFrame(forest_importances_sd)
FI_RF_train_all = pd.concat([FI_RF_train_mean, FI_RF_train_sd], axis=1)
FI_RF_train_all.to_csv('Result_RF/forest_importances_train_RF_Kp_20.csv')
FI_RF_train_all
321/76:
#Train features
from sklearn.inspection import permutation_importance
result = permutation_importance(model_RF, train_x, train_y, n_repeats=10, random_state=42, n_jobs=-1)
feature_names = train_x.columns
forest_importances_sd = pd.Series(result.importances_std, index=feature_names, name='SD')
forest_importances = pd.Series(result.importances_mean, index=feature_names, name='Mean')
fig, ax = plt.subplots()
forest_importances.plot.bar(yerr=result.importances_std, ax=ax)
ax.set_title("Feature importances using permutation on 20 features model")
ax.set_ylabel("Mean accuracy decrease")
fig.tight_layout()
plt.show()
FI_RF_train_mean = pd.DataFrame(forest_importances)
FI_RF_train_sd = pd.DataFrame(forest_importances_sd)
FI_RF_train_all = pd.concat([FI_RF_train_mean, FI_RF_train_sd], axis=1)
FI_RF_train_all.to_csv('Result_RF/20_features/forest_importances_train_RF_Kp_20.csv')
FI_RF_train_all
321/77: FI_RF_train_all.sort_values(by=['Mean'])
321/78: model_RF.feature_importances
321/79: model_RF.feature_importances_
321/80:
sorted_idx = model_RF.feature_importances_.argsort()
plt.barh(feature_names[sorted_idx], model_RF.feature_importances_[sorted_idx])
plt.xlabel("Random Forest Feature Importance")
321/81:
sorted_idx = result.importances_mean.argsort()
plt.barh(feature_names[sorted_idx], result.importances_mean[sorted_idx])
plt.xlabel("Random Forest Feature Importance")
321/82:
explainer = shap.TreeExplainer(model_RF)
shap_values = explainer.shap_values(test_x)
321/83:
import shap
explainer = shap.TreeExplainer(model_RF)
shap_values = explainer.shap_values(test_x)
321/84: shap.summary_plot(shap_values, test_x, plot_type="bar")
321/85: shap.summary_plot(shap_values, test_x)
321/86:
import shap
explainer = shap.TreeExplainer(model_RF)
shap_values = explainer.shap_values(train_x)
321/87: shap.summary_plot(shap_values, train_x, plot_type="bar")
321/88:
train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
321/89: indices_train, indices_test
321/90:
from sklearn.feature_selection import VarianceThreshold
#sel = VarianceThreshold(threshold=(0.05)).get_support(indices=True)
#fingerprint_to_model = sel.fit_transform(fingerprint_to_model)
#fingerprint_to_model

#sel = VarianceThreshold(threshold=(0.05))
#sel = sel.fit(train_x)
#mask = sel.get_support()
321/91:
#train_x_selected = train_x.loc[:, mask]
#train_x_selected
321/92: #train_x_selected.columns
321/93:
import random
SEED = random.seed(10)
SEED
321/94:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=42)
321/95: #train_x_selected.shape
321/96: #test_x_selected = sel.transform(test_x)
321/97:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)


#calculate R2
model_RF_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
321/98:
pd.DataFrame(train_x, indices_train).to_csv("Result_RF/1493_features/x_train_kp.csv")
pd.DataFrame(test_x, indices_test).to_csv("Result_RF/1493_features/x_test_kp.csv")
pd.DataFrame(y_cv, indices_train).to_csv("Result_RF/1493_features/y_cv_kp.csv")
pd.DataFrame(train_y, indices_train).to_csv("Result_RF/1493_features/train_y_kp.csv")
pd.DataFrame(test_y, indices_test).to_csv("Result_RF/1493_features/test_y_kp.csv")
pd.DataFrame(y_pred, indices_train).to_csv("Result_RF/1493_features/y_pred_kp.csv")
pd.DataFrame(y_ext, indices_test).to_csv("Result_RF/1493_features/y_SVR_ext_kp.csv")
321/99:
train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(features_10, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
321/100: train_x.shape
321/101:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)


#calculate R2
model_RF_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
321/102:
#Train features
from sklearn.inspection import permutation_importance
result = permutation_importance(model_RF, train_x, train_y, n_repeats=10, random_state=42, n_jobs=-1)
feature_names = train_x.columns
forest_importances_sd = pd.Series(result.importances_std, index=feature_names, name='SD')
forest_importances = pd.Series(result.importances_mean, index=feature_names, name='Mean')
fig, ax = plt.subplots()
forest_importances.plot.bar(yerr=result.importances_std, ax=ax)
ax.set_title("Feature importances using permutation on 20 features model")
ax.set_ylabel("Mean accuracy decrease")
fig.tight_layout()
plt.show()
FI_RF_train_mean = pd.DataFrame(forest_importances)
FI_RF_train_sd = pd.DataFrame(forest_importances_sd)
FI_RF_train_all = pd.concat([FI_RF_train_mean, FI_RF_train_sd], axis=1)
FI_RF_train_all.to_csv('Result_RF/20_features/forest_importances_train_RF_Kp_10.csv')
FI_RF_train_all
321/103:
sorted_idx = result.importances_mean.argsort()
plt.barh(feature_names[sorted_idx], result.importances_mean[sorted_idx])
plt.xlabel("Permutation Feature Importance")
321/104:
import shap
explainer = shap.TreeExplainer(model_RF)
shap_values = explainer.shap_values(train_x)
321/105: shap.summary_plot(shap_values, train_x, plot_type="bar")
321/106: shap.summary_plot(shap_values, test_x)
321/107: shap.summary_plot(shap_values, train_x)
321/108:
#Train features
from sklearn.inspection import permutation_importance
result = permutation_importance(model_RF, train_x, train_y, n_repeats=10, random_state=42, n_jobs=-1)
feature_names = train_x.columns
forest_importances_sd = pd.Series(result.importances_std, index=feature_names, name='SD')
forest_importances = pd.Series(result.importances_mean, index=feature_names, name='Mean')
fig, ax = plt.subplots()
forest_importances.plot.bar(yerr=result.importances_std, ax=ax)
ax.set_title("Feature importances using permutation on 20 features model")
ax.set_ylabel("Mean accuracy decrease")
fig.tight_layout()
plt.show()
FI_RF_train_mean = pd.DataFrame(forest_importances)
FI_RF_train_sd = pd.DataFrame(forest_importances_sd)
FI_RF_train_all = pd.concat([FI_RF_train_mean, FI_RF_train_sd], axis=1)
FI_RF_train_all.to_csv('Result_RF/10_features/forest_importances_train_RF_Kp_10.csv')
FI_RF_train_all
321/109:
sorted_idx = result.importances_mean.argsort()
plt.barh(feature_names[sorted_idx], result.importances_mean[sorted_idx])
plt.xlabel("Permutation Feature Importance")
321/110: -0.6375/-1.9
321/111: -0.6375/-1.9*100
321/112:
FI = FI_RF_train_all.sort_values(by=['Mean'])
FI_20 = FI.tail(20)
FI_20
321/113:
FI = FI_RF_train_all.sort_values(by=['Mean'])
FI_10 = FI.tail(10)
FI_10
321/114:
train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(features_10, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
321/115: train_x.shape
321/116:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestRegressor(n_estimators=200, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)


#calculate R2
model_RF_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
321/117:
pd.DataFrame(train_x, indices_train).to_csv("Result_RF/10_features/x_train_kp.csv", name='x_train')
pd.DataFrame(test_x, indices_test).to_csv("Result_RF/10_features/x_test_kp.csv", name='x_test')
pd.DataFrame(y_cv, indices_train).to_csv("Result_RF/10_features/y_cv_kp.csv", name='y_cv')
pd.DataFrame(train_y, indices_train).to_csv("Result_RF/10_features/train_y_kp.csv", name='train_y')
pd.DataFrame(test_y, indices_test).to_csv("Result_RF/10_features/test_y_kp.csv", name='test_y')
pd.DataFrame(y_pred, indices_train).to_csv("Result_RF/10_features/y_pred_kp.csv", name='y_pred')
pd.DataFrame(y_ext, indices_test).to_csv("Result_RF/10_features/y_ext_kp.csv", name='y_ext')
321/118:
pd.DataFrame(train_x, indices_train, colume='x_train').to_csv("Result_RF/10_features/x_train_kp.csv")
pd.DataFrame(test_x, indices_test, colume='x_test').to_csv("Result_RF/10_features/x_test_kp.csv")
pd.DataFrame(y_cv, indices_train, colume='y_cv').to_csv("Result_RF/10_features/y_cv_kp.csv")
pd.DataFrame(train_y, indices_train, colume='train_y').to_csv("Result_RF/10_features/train_y_kp.csv")
pd.DataFrame(test_y, indices_test, colume='test_y').to_csv("Result_RF/10_features/test_y_kp.csv")
pd.DataFrame(y_pred, indices_train, colume='y_pred').to_csv("Result_RF/10_features/y_pred_kp.csv")
pd.DataFrame(y_ext, indices_test, colume='y_ext').to_csv("Result_RF/10_features/y_ext_kp.csv")
321/119:
pd.DataFrame(train_x, indices_train, columes='x_train').to_csv("Result_RF/10_features/x_train_kp.csv")
pd.DataFrame(test_x, indices_test, columes='x_test').to_csv("Result_RF/10_features/x_test_kp.csv")
pd.DataFrame(y_cv, indices_train, columes='y_cv').to_csv("Result_RF/10_features/y_cv_kp.csv")
pd.DataFrame(train_y, indices_train, columes='train_y').to_csv("Result_RF/10_features/train_y_kp.csv")
pd.DataFrame(test_y, indices_test, columes='test_y').to_csv("Result_RF/10_features/test_y_kp.csv")
pd.DataFrame(y_pred, indices_train, columes='y_pred').to_csv("Result_RF/10_features/y_pred_kp.csv")
pd.DataFrame(y_ext, indices_test, columes='y_ext').to_csv("Result_RF/10_features/y_ext_kp.csv")
321/120: pd.DataFrame(train_x, indices_train, columns=c['Name', 'x_train']).to_csv("Result_RF/10_features/x_train_kp.csv")
321/121: pd.DataFrame(train_x, indices_train, columns=['Name', 'x_train']).to_csv("Result_RF/10_features/x_train_kp.csv")
321/122:
pd.DataFrame(train_x, indices_train).to_csv("Result_RF/10_features/x_train_kp.csv")
pd.DataFrame(test_x, indices_test).to_csv("Result_RF/10_features/x_test_kp.csv")
pd.DataFrame(y_cv, indices_train).to_csv("Result_RF/10_features/y_cv_kp.csv")
pd.DataFrame(train_y, indices_train).to_csv("Result_RF/10_features/train_y_kp.csv")
pd.DataFrame(test_y, indices_test).to_csv("Result_RF/10_features/test_y_kp.csv")
pd.DataFrame(y_pred, indices_train).to_csv("Result_RF/10_features/y_pred_kp.csv")
pd.DataFrame(y_ext, indices_test).to_csv("Result_RF/10_features/y_ext_kp.csv")
321/123:
#Train features
from sklearn.inspection import permutation_importance
result = permutation_importance(model_RF, train_x, train_y, n_repeats=10, random_state=42, n_jobs=-1)
feature_names = train_x.columns
forest_importances_sd = pd.Series(result.importances_std, index=feature_names, name='SD')
forest_importances = pd.Series(result.importances_mean, index=feature_names, name='Mean')
fig, ax = plt.subplots()
forest_importances.plot.bar(yerr=result.importances_std, ax=ax)
ax.set_title("Feature importances using permutation on 10 features model")
ax.set_ylabel("Mean accuracy decrease")
fig.tight_layout()
plt.show()
FI_RF_train_mean = pd.DataFrame(forest_importances)
FI_RF_train_sd = pd.DataFrame(forest_importances_sd)
FI_RF_train_all = pd.concat([FI_RF_train_mean, FI_RF_train_sd], axis=1)
FI_RF_train_all.to_csv('Result_RF/10_features/forest_importances_train_RF_Kp_10.csv')
FI_RF_train_all
321/124:
df = pd.read_csv('df6_lipinski_Kpu_Kp_cells.csv')
df_logkp = df[['compound', 'smiles', 'kp', 'logkp']]
321/125:
df = pd.read_csv('df6_lipinski_Kpu_Kp_cells.csv')
df_logkp = df[['compound', 'smiles', 'kp', 'logkp']]
df_logkp
321/126:
df_logkp = df[['compound', 'smiles', 'kp', 'logkp']]
df_logkp.to_csv('Supplementary_File_2.csv')
df_logkp
321/127: raw13.to_csv('QSAR_Kp/Mordard_Kp_144_1493_features.csv' , sep=',' ,index=True)
321/128: from rdkit import Chem
321/129: Chem.MolFromSmiles('Oc1ccc2c(c1)OCO2')
321/130: Chem.MolFromSmiles('C1Oc2c(O1)cc(cc2)C1OCC2C1COC2c1ccc2c(c1)OCO2')
321/131:
m = Chem.MolFromSmiles('C1Oc2c(O1)cc(cc2)C1OCC2C1COC2c1ccc2c(c1)OCO2')
substructure = Chem.MolFromSmarts('Oc1ccc2c(c1)OC')
print(m.GetSubstructMatches(substructure))
321/132:
m = Chem.MolFromSmiles('C1Oc2c(O1)cc(cc2)C1OCC2C1COC2c1ccc2c(c1)OCO2')
substructure = Chem.MolFromSmarts('Oc1ccc2c(c1)O')
print(m.GetSubstructMatches(substructure))
321/133:
m = Chem.MolFromSmiles('C1Oc2c(O1)cc(cc2)C1OCC2C1COC2c1ccc2c(c1)OCO2')
substructure = Chem.MolFromSmarts('Oc1ccc2c(c1)OCO2')
print(m.GetSubstructMatches(substructure))
321/134:
m = Chem.MolFromSmiles('C1Oc2c(O1)cc(cc2)C1OCC2C1COC2c1ccc2c(c1)OCO2')
substructure = Chem.MolFromSmarts('Oc1ccc2c(c1)OCO')
print(m.GetSubstructMatches(substructure))
321/135:
m = Chem.MolFromSmiles('C1Oc2c(O1)cc(cc2)C1OCC2C1COC2c1ccc2c(c1)OCO2')
substructure = Chem.MolFromSmarts('c1ccc2c(c1)OCO2')
print(m.GetSubstructMatches(substructure))
321/136: m
321/137:
SMO = Chem.MolFromSmiles('C1Oc2c(O1)cc(cc2)OC1OCC2C1COC2c1ccc2c(c1)OCO2')
substructure = Chem.MolFromSmarts('c1ccc2c(c1)OCO2')
print(m.GetSubstructMatches(substructure))
321/138: SMO
321/139:
SMO = Chem.MolFromSmiles('C1Oc2c(O1)cc(cc2)OC1OCC2C1COC2c1ccc2c(c1)OCO2')
substructure = Chem.MolFromSmarts('c1ccc2c(c1)OCO2')
print(m.GetSubstructMatches(substructure))
321/140: SMO
321/141:
SMO = Chem.MolFromSmiles('C1Oc2c(O1)cc(cc2)OC1OCC2C1COC2c1ccc2c(c1)OCO2')
substructure = Chem.MolFromSmarts('c1ccc2c(c1)OCO2')
print(SMO.GetSubstructMatches(substructure))
321/142: SMO
321/143:
SMO = Chem.MolFromSmiles('C1Oc2c(O1)cc(cc2)OC1OCC2C1COC2c1ccc2c(c1)OCO2')
substructure = Chem.MolFromSmarts('c1ccc2c(c1)OCO2')
SMO.GetSubstructMatches(substructure)
321/144:
SM = Chem.MolFromSmiles('Oc1ccc2c(c1)OCO')
substructure = Chem.MolFromSmarts('Oc1ccc2c(c1)OCO')
print(SM.GetSubstructMatches(substructure))
SM
321/145: from rdkit import Chem
321/146:
SM = Chem.MolFromSmiles('Oc1ccc2c(c1)OCO')
substructure = Chem.MolFromSmarts('c1ccc2c(c1)OCO')
print(SM.GetSubstructMatches(substructure))
SM
321/147:
SM = Chem.MolFromSmiles('Oc1ccc2c(c1)OCO')
substructure = Chem.MolFromSmarts('c1ccc2c(c1)OCO')
print(SM.GetSubstructMatches(substructure))
321/148:
SM = Chem.MolFromSmiles('Oc1ccc2c(c1)OCO2')
substructure = Chem.MolFromSmarts('c1ccc2c(c1)OCO2')
print(SM.GetSubstructMatches(substructure))
321/149:
SM = Chem.MolFromSmiles('Oc1ccc2c(c1)OCO2')
substructure = Chem.MolFromSmarts('c1ccc2c(c1)OCO2')
print(SM.GetSubstructMatches(substructure))
SM
321/150:
SM = Chem.MolFromSmiles('Oc1ccc2c(c1)OCO2')
substructure = Chem.MolFromSmarts('c1ccc2c(c1)OCO2')
substructure = Chem.MolFromSmarts('O')
print(SM.GetSubstructMatches(substructure))
SM
321/151:
SM = Chem.MolFromSmiles('Oc1ccc2c(c1)OCO2')
substructure = Chem.MolFromSmarts('c1ccc2c(c1)OCO2')
substructure = Chem.MolFromSmarts('Oc1')
print(SM.GetSubstructMatches(substructure))
SM
321/152:
SM = Chem.MolFromSmiles('Oc1ccc2c(c1)OCO2')
substructure = Chem.MolFromSmarts('c1ccc2c(c1)OCO2')
print(SM.GetSubstructMatches(substructure))
SM
321/153:
SM = Chem.MolFromSmiles('Oc1ccc2c(c1)OCO2')
substructure = Chem.MolFromSmarts('c1ccc2c(c1)OCO2')
print(SM.GetSubstructMatches(substructure))
SM
SM.Chem.Draw.MolToFile
321/154:
SM = Chem.MolFromSmiles('Oc1ccc2c(c1)OCO2')
substructure = Chem.MolFromSmarts('c1ccc2c(c1)OCO2')
print(SM.GetSubstructMatches(substructure))
SM
Chem.Draw.MolToFile(SM)
321/155:
SM = Chem.MolFromSmiles('Oc1ccc2c(c1)OCO2')
substructure = Chem.MolFromSmarts('c1ccc2c(c1)OCO2')
print(SM.GetSubstructMatches(substructure))
SM
Chem.Draw.MolToFile(SM, 'sesamol.png')
321/156:
SM = Chem.MolFromSmiles('Oc1ccc2c(c1)OCO2')
substructure = Chem.MolFromSmarts('c1ccc2c(c1)OCO2')
print(SM.GetSubstructMatches(substructure))
SM
321/157:
SM = Chem.MolFromSmiles('Oc1ccc2c(c1)OCO2')
substructure = Chem.MolFromSmarts('c1ccc2c(c1)OCO2')
SM.GetSubstructMatches(substructure)
SM
321/158: Chem.MolFromSmiles('Oc1ccc2c(c1)OCO2')
341/1: print('hello world')
341/2:
from rdkit import DataStructs
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
341/3:
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
341/4: df = pd.read_csv('antihyperglycemia.csv')
341/5: df
341/6: df = pd.read_csv('antihyperglycemia.csv')
341/7: df
341/8: df = df.drop['Name_Thai']
341/9: df
341/10: df = df.drop[['Name_Thai']]
341/11: df = df.drop(['Name_Thai'])
341/12: df = df.drop('Name_Thai')
341/13: df = df.drop(['Name_Thai'], axis=1)
341/14:
df = df.drop(['Name_Thai'], axis=1)
fd
341/15:
df = df.drop(['Name_Thai'], axis=1)
df
341/16: df = pd.read_csv('antihyperglycemia.csv')
341/17: df
341/18:
df = df.drop(['Number', 'Name_Thai'], axis=1)
df
341/19:
from scipy.stats import pearsonr
rho = df.corr()
pval = df.corr(method=lambda x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)
p = pval.applymap(lambda x: ''.join(['*' for t in [0.001,0.01,0.05] if x<=t]))
Res_pearson_sig = rho.round(3).astype(str) + p
Res_pearson_sig
341/20:
from scipy.stats import pearsonr
rho = df.corr()
pval = df.corr(method='pearson' x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)
p = pval.applymap('pearson' x: ''.join(['*' for t in [0.001,0.01,0.05] if x<=t]))
Res_pearson_sig = rho.round(3).astype(str) + p
Res_pearson_sig
341/21:
from scipy.stats import pearsonr
rho = df.corr()
pval = df.corr(method='pearson', x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)
p = pval.applymap('pearson', x: ''.join(['*' for t in [0.001,0.01,0.05] if x<=t]))
Res_pearson_sig = rho.round(3).astype(str) + p
Res_pearson_sig
341/22:
from scipy.stats import pearsonr
rho = df.corr()
pval = df.corr(method=lambda x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)
p = pval.applymap(lambda x: ''.join(['*' for t in [0.001,0.01,0.05] if x<=t]))
Res_pearson_sig = rho.round(3).astype(str) + p
Res_pearson_sig
341/23:
Res_pearson = df.corr(method='pearson').round(3)
Res_pearson
341/24:
fig, ax = plt.subplots(figsize=(10,10)) 
ax = sns.heatmap(Res_pearson, annot=True, cmap='coolwarm', fmt='.2f')
plt.tight_layout()
341/25:
fig, ax = plt.subplots(figsize=(10,10)) 
ax = sns.heatmap(Res_pearson, annot=True, cmap='coolwarm', fmt='.2f')
plt.tight_layout()
plt.savefig('corr_pearson_number.pdf', dpi=300)
341/26:
fig, ax = plt.subplots(figsize=(10,10)) 
ax = sns.heatmap(Res_pearson, annot=False, cmap='coolwarm', fmt='.2f')
plt.tight_layout()
plt.savefig('corr_pearson_number.pdf', dpi=300)
341/27:
fig, ax = plt.subplots(figsize=(10,10)) 
ax = sns.heatmap(Res_pearson, annot=False, cmap='coolwarm', fmt='.2f')
plt.tight_layout()
plt.savefig('corr_pearson_no_number.pdf', dpi=300)
341/28:
fig, ax = plt.subplots(figsize=(10,10)) 
ax = sns.heatmap(Res_pearson, annot=False, cmap='Spectral', fmt='.2f')
plt.tight_layout()
plt.savefig('corr_pearson_no_number.pdf', dpi=300)
341/29:
fig, ax = plt.subplots(figsize=(10,10)) 
ax = sns.heatmap(Res_pearson, annot=True, cmap='Spectral', fmt='.2f')
plt.tight_layout()
plt.savefig('corr_pearson_number.pdf', dpi=300)
341/30: df.to_csv('df_anthyperglycemia_clean.csv')
342/1:
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
342/2: df = pd.read_csv('df_antihyperglycemia_clean.csv')
342/3: df
341/31: df.to_csv('df_anthyperglycemia_clean.csv', index=True)
341/32: df.to_csv('df_antihyperglycemia_clean.csv', index=True)
342/4: df = pd.read_csv('df_antihyperglycemia_clean.csv')
342/5: df
341/33:
df.set_index(['Name_Short'])
.to_csv('df_antihyperglycemia_clean.csv', index=True)
341/34: df1 = df.set_index(['Name_Short'])
341/35:
df1 = df.set_index(['Name_Short'])
df1
341/36:
df1 = df.set_index(['Name_Short'])
df2 = df1.drop(['Name_Sci'])
341/37:
df1 = df.set_index(['Name_Short'])
df2 = df1.drop(['Name_Sci'], axis=1)
341/38: df2
341/39: df2.to_csv('df_clean.csv')
342/6: df = pd.read_csv('df_clean.csv')
342/7: df
342/8: df_nostring = df.drop(['Bioactivity_class_glucosidase', 'Bioactivity_class_glucosidase'], axis=1)
342/9: df_nona_nostr = df_nostring.notna()
342/10: df_nona_nostr
342/11: df_PCA = df.drop(['IC50_amylase','IC50_glucosidase','Bioactivity_class_glucosidase', 'Bioactivity_class_glucosidase'], axis=1)
342/12: df_PCA
342/13: df_PCA = df.drop(['IC50_amylase','IC50_glucosidase','Bioactivity_class_amylase', 'Bioactivity_class_glucosidase'], axis=1)
342/14: df_PCA
342/15: df.head(2)
342/16: features = ['Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Sapoonins', 'Tannins', 'Xanthones']
342/17: x = df.loc[:, features].values
342/18: features = ['Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']
342/19: x = df.loc[:, features].values
342/20: y = df.loc[:,['Bioactivity_class_amylase']].values
342/21: pd.DataFrame(data = x, columns = features).head()
342/22: pca = PCA(n_components=2)
342/23:
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
342/24: pca = PCA(n_components=2)
342/25: principalComponents = pca.fit_transform(x)
342/26:
principalDf = pd.DataFrame(data = principalComponents
             , columns = ['principal component 1', 'principal component 2'])
342/27: principalDf.head(5)
342/28: df[['Bioactivity_class_amylase']].head()
342/29: pd.DataFrame(data = x, columns = features, index=True).head()
342/30: indices = df.index
342/31: pd.DataFrame(data = x, columns = features, index=indices).head()
342/32: pd.DataFrame(data = x, columns = features, index=indices).head()
342/33: indices = df.index
342/34: df.head(2)
342/35: df = pd.read_csv('df_clean.csv', indices)
342/36: df = pd.read_csv('df_clean.csv', index_col=Name_Short)
342/37: df = pd.read_csv('df_clean.csv', index_col='Name_Short')
342/38: df.head(2)
342/39: indices = df.index
342/40: pd.DataFrame(data = x, columns = features, index=indices).head()
342/41: principalDf
342/42:
principalDf = pd.DataFrame(data = principalComponents
             , columns = ['principal component 1', 'principal component 2'], index=indices)
342/43: principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'], index=indices)
342/44: principalDf
342/45: principalDf.head(3)
342/46: df[['Bioactivity_class_amylase'], index=indices].head()
342/47: df[['Bioactivity_class_amylase']].head()
342/48: Bioactivity_class_amylase = df[['Bioactivity_class_amylase']].head()
342/49: finalDf = pd.concat([principalDf, Bioactivity_class_amylase], axis = 1)
342/50:
finalDf = pd.concat([principalDf, Bioactivity_class_amylase], axis = 1)
finalDf.head(5)
342/51: principalComponents
342/52: principalComponents.explained_variance_ratio_
342/53: pca.explained_variance_ratio_
342/54:
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 Component PCA', fontsize = 20)


targets = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']
colors = ['r', 'g', 'b']
for target, color in zip(targets,colors):
    indicesToKeep = finalDf['target'] == target
    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']
               , finalDf.loc[indicesToKeep, 'principal component 2']
               , c = color
               , s = 50)
ax.legend(targets)
ax.grid()
342/55:
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 Component PCA', fontsize = 20)


Bioactivity_class_amylase = ['active', 'inactive']
colors = ['r', 'g']
for target, color in zip(targets,colors):
    indicesToKeep = finalDf['target'] == target
    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']
               , finalDf.loc[indicesToKeep, 'principal component 2']
               , c = color
               , s = 50)
ax.legend(targets)
ax.grid()
342/56:
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 Component PCA', fontsize = 20)


Bioactivity_class_amylase = ['active', 'inactive']
colors = ['r', 'g']
for target, color in zip(targets,colors):
    indicesToKeep = finalDf['Bioactivity_class_amylase'] == Bioactivity_class_amylase
    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']
               , finalDf.loc[indicesToKeep, 'principal component 2']
               , c = color
               , s = 50)
ax.legend(targets)
ax.grid()
342/57:
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 Component PCA', fontsize = 20)


Bioactivity_class_amylase = ['active', 'inactive']
colors = ['r', 'g']
for Bioactivity_class_amylase, color in zip(Bioactivity_class_amylase,colors):
    indicesToKeep = finalDf['Bioactivity_class_amylase'] == Bioactivity_class_amylase
    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']
               , finalDf.loc[indicesToKeep, 'principal component 2']
               , c = color
               , s = 50)
ax.legend(targets)
ax.grid()
342/58:
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 Component PCA', fontsize = 20)


Bioactivity_class_amylase = ['active', 'inactive']
colors = ['r', 'g']
for Bioactivity_class_amylase, color in zip(Bioactivity_class_amylase,colors):
    indicesToKeep = finalDf['Bioactivity_class_amylase'] == Bioactivity_class_amylase
    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']
               , finalDf.loc[indicesToKeep, 'principal component 2']
               , c = color
               , s = 50)
ax.legend(Bioactivity_class_amylase)
ax.grid()
342/59: pca = PCA(n_components=5)
342/60: principalComponents = pca.fit_transform(x)
342/61: pca.explained_variance_ratio_
342/62: pca = PCA(n_components=7)
342/63: principalComponents = pca.fit_transform(x)
342/64: pca.explained_variance_ratio_
342/65: from mpl_toolkits import mplot3d
342/66:
fig = plt.figure()
ax = plt.axes(projection='3d')
342/67: pca = PCA(n_components=5)
342/68: principalComponents = pca.fit_transform(x)
342/69: pca.explained_variance_ratio_
342/70: principalComponents
342/71: principalComponents.argmax
342/72: principalComponents.cumsum
342/73: principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2', 'principal component 3', 'principal component 4', 'principal component 5'], index=indices)
342/74: principalDf.head(3)
342/75:
finalDf = pd.concat([principalDf, Bioactivity_class_amylase], axis = 1)
finalDf.head(5)
342/76: Bioactivity_class_amylase = df[['Bioactivity_class_amylase']].head()
342/77:
finalDf = pd.concat([principalDf, Bioactivity_class_amylase], axis = 1)
finalDf.head(5)
342/78: sns.pairplot(principalDf)
342/79: sns.pairplot(finalDf, hue="Bioactivity_class_amylase")
342/80:
finalDf = pd.concat([principalDf, Bioactivity_class_amylase], axis = 1)
finalDf
342/81: Bioactivity_class_amylase = df[['Bioactivity_class_amylase']]
342/82:
finalDf = pd.concat([principalDf, Bioactivity_class_amylase], axis = 1)
finalDf
342/83: sns.pairplot(finalDf, hue="Bioactivity_class_amylase")
342/84: sns.pairplot(finalDf, hue="Bioactivity_class_amylase", height=1.5)
342/85: sns.pairplot(finalDf, hue="Bioactivity_class_amylase", height=2)
342/86:
sns.scatterplot(x="principal component 1", y="principal component 2",
                hue="Bioactivity_class_amylase", 
                sizes=(1, 8), linewidth=0,
                data=finalDf, ax=ax)
342/87: sns.scatterplot(finalDf, x="principal component 1", y="principal component 2", hue="Bioactivity_class_amylase")
342/88: sns.scatterplot(finalDf, x="principal component 1", y="principal component 3", hue="Bioactivity_class_amylase")
342/89: sns.pairplot(finalDf, x_vars=['principal component 1'], y_vars=['principal component 2'] hue="Bioactivity_class_amylase", height=2)
342/90: sns.pairplot(finalDf, x_vars=['principal component 1'], y_vars=['principal component 2'], hue="Bioactivity_class_amylase", height=2)
342/91: sns.pairplot(finalDf, x_vars=['principal component 1'], y_vars=['principal component 2'], hue="Bioactivity_class_amylase")
342/92:
# Set a / KMeans clustering
kmeans = KMeans(n_clusters=2)
# Compute cluster centers and predict cluster indices
X_clustered = kmeans.fit_predict(principalComponents)

# Define our own color map
LABEL_COLOR_MAP = {active : 'r',inactive : 'g',2 : 'b'}
label_color = [LABEL_COLOR_MAP[l] for l in X_clustered]

# Plot the scatter digram
plt.figure(figsize = (7,7))
plt.scatter(principalComponents[:,0],principalComponents[:,2], c= label_color, alpha=0.5) 
plt.show()
342/93:
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
342/94:
# Set a / KMeans clustering
kmeans = KMeans(n_clusters=2)
# Compute cluster centers and predict cluster indices
X_clustered = kmeans.fit_predict(principalComponents)

# Define our own color map
LABEL_COLOR_MAP = {active : 'r',inactive : 'g',2 : 'b'}
label_color = [LABEL_COLOR_MAP[l] for l in X_clustered]

# Plot the scatter digram
plt.figure(figsize = (7,7))
plt.scatter(principalComponents[:,0],principalComponents[:,2], c= label_color, alpha=0.5) 
plt.show()
342/95:
# Set a / KMeans clustering
kmeans = KMeans(n_clusters=2)
# Compute cluster centers and predict cluster indices
X_clustered = kmeans.fit_predict(principalComponents)


# Plot the scatter digram
plt.figure(figsize = (7,7))
plt.scatter(principalComponents[:,0],principalComponents[:,2], c= Bioactivity_class_amylase, alpha=0.5) 
plt.show()
341/40:
fig, ax = plt.subplots(figsize=(10,10)) 
ax = sns.heatmap(Res_pearson, annot=True, cmap='Spectral', fmt='.2f', linecolor='black')
plt.tight_layout()
plt.savefig('corr_pearson_number.pdf', dpi=300)
342/96:
fig, ax = plt.subplots(figsize=(10,10)) 
ax = sns.pairplot(finalDf, hue="Bioactivity_class_amylase", height=2)
plt.tight_layout()
plt.savefig('PCA_pairplot.pdf', dpi=300)
342/97:
ax = sns.pairplot(finalDf, hue="Bioactivity_class_amylase", height=2)
plt.tight_layout()
plt.savefig('PCA_pairplot.pdf', dpi=300)
342/98: sns.pairplot(finalDf, hue="Bioactivity_class_amylase", height=2)
342/99:
sns.pairplot(finalDf, hue="Bioactivity_class_amylase", height=2)
plt.savefig('PCA_pairplot.pdf', dpi=300)
342/100:
# Set a / KMeans clustering
kmeans = KMeans(n_clusters=2)
# Compute cluster centers and predict cluster indices
X_clustered = kmeans.fit_predict(principalComponents)

# Plot the scatter digram
plt.figure(figsize = (7,7))
plt.scatter(principalComponents[:,0],principalComponents[:,2], alpha=0.5) 
plt.show()
342/101:
# Set a / KMeans clustering
kmeans = KMeans(n_clusters=2)
# Compute cluster centers and predict cluster indices
X_clustered = kmeans.fit_predict(principalComponents)

# Plot the scatter digram
plt.figure(figsize = (7,7))
sns.scatter(principalComponents[:,0],principalComponents[:,2], hue="Bioactivity_class_amylase", alpha=0.5) 
plt.show()
342/102:
# Set a / KMeans clustering
kmeans = KMeans(n_clusters=2)
# Compute cluster centers and predict cluster indices
X_clustered = kmeans.fit_predict(principalComponents)

# Plot the scatter digram
plt.figure(figsize = (7,7))
sns.scatterplot(principalComponents[:,0],principalComponents[:,2], hue=finalDf["Bioactivity_class_amylase"], alpha=0.5) 
plt.show()
342/103:
# Set a / KMeans clustering
kmeans = KMeans(n_clusters=2)
# Compute cluster centers and predict cluster indices
X_clustered = kmeans.fit_predict(principalComponents)

# Plot the scatter digram
plt.figure(figsize = (7,7))
sns.scatterplot(principalComponents[:,0],principalComponents[:,2], hue=finalDf["Bioactivity_class_amylase"], alpha=1) 
plt.show()
342/104:
plt.plot(range(0,5), pca.explained_variance_ratio_)
plt.ylabel('Explained Variance')
plt.xlabel('Principal Components')
plt.xticks(range(0,3),
           ["1st comp", "2nd comp", "3rd comp"], rotation=60)
plt.title('Explained Variance Ratio')
plt.show()
342/105:
plt.plot(range(0,5), pca.explained_variance_ratio_)
plt.ylabel('Explained Variance')
plt.xlabel('Principal Components')
plt.xticks(range(0,3),
           ['principal component 1', 'principal component 2', 'principal component 3', 'principal component 4', 'principal component 5'], rotation=60)
plt.title('Explained Variance Ratio')
plt.show()
342/106:
plt.plot(range(0,5), pca.explained_variance_ratio_)
plt.ylabel('Explained Variance')
plt.xlabel('Principal Components')
plt.xticks(range(0,5),
           ['principal component 1', 'principal component 2', 'principal component 3', 'principal component 4', 'principal component 5'], rotation=60)
plt.title('Explained Variance Ratio')
plt.show()
342/107:
plt.plot(range(0,5), pca.explained_variance_ratio_)
plt.ylabel('Explained Variance')
plt.xlabel('Principal Components')
plt.xticks(range(0,5),
           ['principal component 1', 'principal component 2', 'principal component 3', 'principal component 4', 'principal component 5'], rotation=90)
plt.title('Explained Variance Ratio')
plt.show()
342/108:
plt.plot(range(0,5), pca.explained_variance_ratio_)
plt.ylabel('Explained Variance')
plt.xlabel('Principal Components')
plt.xticks(range(0,5),
           ['PC 1', 'PC 2', 'PC 3', 'PC 4', 'PC 5'], rotation=90)
plt.title('Explained Variance Ratio')
plt.show()
342/109:
plt.plot(range(0,5), pca.explained_variance_ratio_)
plt.ylabel('Explained Variance')
plt.xlabel('Principal Components')
plt.xticks(range(0,5),
           ['PC 1', 'PC 2', 'PC 3', 'PC 4', 'PC 5'], rotation=0)
plt.title('Explained Variance Ratio')
plt.show()
342/110:
# Calculation of Explained Variance from the eigenvalues
tot = sum(pca.explained_variance_ratio_)
var_exp = [(i/tot)*100 for i in sorted(eig_vals, reverse=True)] # Individual explained variance
cum_var_exp = np.cumsum(var_exp) # Cumulative explained variance
342/111:
# Calculation of Explained Variance from the eigenvalues
tot = sum(pca.explained_variance_ratio_)
var_exp = [(i/tot)*100 for i in sorted(pca.explained_variance_ratio_, reverse=True)] # Individual explained variance
cum_var_exp = np.cumsum(var_exp) # Cumulative explained variance
342/112:
plt.figure(figsize=(8, 5))
plt.bar(range(16), var_exp, alpha=0.3333, align='center', label='individual explained variance', color = 'g')
plt.step(range(16), cum_var_exp, where='mid',label='cumulative explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal components')
plt.legend(loc='best')
plt.show()
342/113:
plt.figure(figsize=(8, 5))
plt.bar(range(5), var_exp, alpha=0.3333, align='center', label='individual explained variance', color = 'g')
plt.step(range(5), cum_var_exp, where='mid',label='cumulative explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal components')
plt.legend(loc='best')
plt.show()
342/114:
plt.figure(figsize=(8, 5))
plt.bar(range(5), var_exp, alpha=0.3333, align='center', label='individual explained variance', color = 'g')
plt.step(range(5), cum_var_exp, where='mid',label='cumulative explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal components')
plt.xticks(range(0,5),
           ['PC 1', 'PC 2', 'PC 3', 'PC 4', 'PC 5'], rotation=0)
plt.legend(loc='best')
plt.show()
342/115:
plt.figure(figsize=(8, 5))
plt.bar(range(5), var_exp, alpha=0.3333, align='center', label='individual explained variance', color = 'g')
plt(range(5), cum_var_exp, where='mid',label='cumulative explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal components')
plt.xticks(range(0,5),
           ['PC 1', 'PC 2', 'PC 3', 'PC 4', 'PC 5'], rotation=0)
plt.legend(loc='best')
plt.show()
342/116:
plt.figure(figsize=(8, 5))
plt.bar(range(5), var_exp, alpha=0.3333, align='center', label='individual explained variance', color = 'g')
plt.plot(range(5), cum_var_exp, where='mid',label='cumulative explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal components')
plt.xticks(range(0,5),
           ['PC 1', 'PC 2', 'PC 3', 'PC 4', 'PC 5'], rotation=0)
plt.legend(loc='best')
plt.show()
342/117:
plt.figure(figsize=(8, 5))
plt.bar(range(5), var_exp, alpha=0.3333, align='center', label='individual explained variance', color = 'g')
plt.plot(cum_var_exp, where='mid',label='cumulative explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal components')
plt.xticks(range(0,5),
           ['PC 1', 'PC 2', 'PC 3', 'PC 4', 'PC 5'], rotation=0)
plt.legend(loc='best')
plt.show()
342/118:
plt.figure(figsize=(8, 5))
plt.bar(range(5), var_exp, alpha=0.3333, align='center', label='individual explained variance', color = 'g')
plt.step(range(5), cum_var_exp, where='mid',label='cumulative explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal components')
plt.xticks(range(0,5),
           ['PC 1', 'PC 2', 'PC 3', 'PC 4', 'PC 5'], rotation=0)
plt.legend(loc='best')
plt.show()
342/119:
plt.figure(figsize=(8, 5))
plt.bar(range(5), var_exp, alpha=0.3333, align='center', label='individual explained variance', color = 'g')
ax = sns.plot(cum_var_exp, where='mid',label='cumulative explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal components')
plt.xticks(range(0,5),
           ['PC 1', 'PC 2', 'PC 3', 'PC 4', 'PC 5'], rotation=0)
plt.legend(loc='best')
plt.show()
342/120:
plt.figure(figsize=(8, 5))
plt.bar(range(5), var_exp, alpha=0.3333, align='center', label='individual explained variance', color = 'g')
ax = sns.lineplot(cum_var_exp, where='mid',label='cumulative explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal components')
plt.xticks(range(0,5),
           ['PC 1', 'PC 2', 'PC 3', 'PC 4', 'PC 5'], rotation=0)
plt.legend(loc='best')
plt.show()
342/121:
plt.figure(figsize=(8, 5))
plt.bar(range(5), var_exp, alpha=0.3333, align='center', label='individual explained variance', color = 'g')
ax = sns.lineplot(cum_var_exp,label='cumulative explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal components')
plt.xticks(range(0,5),
           ['PC 1', 'PC 2', 'PC 3', 'PC 4', 'PC 5'], rotation=0)
plt.legend(loc='best')
plt.show()
342/122:
plt.figure(figsize=(8, 5))
plt.bar(range(5), var_exp, alpha=0.3333, align='center', label='individual explained variance', color = 'g')
ax = sns.lineplot(range(5), cum_var_exp,label='cumulative explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal components')
plt.xticks(range(0,5),
           ['PC 1', 'PC 2', 'PC 3', 'PC 4', 'PC 5'], rotation=0)
plt.legend(loc='best')
plt.show()
342/123:
plt.figure(figsize=(8, 5))
plt.bar(range(5), var_exp, alpha=0.3333, align='center', label='individual explained variance', color = 'g')
plt.plot(range(5), cum_var_exp,label='cumulative explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal components')
plt.xticks(range(0,5),
           ['PC 1', 'PC 2', 'PC 3', 'PC 4', 'PC 5'], rotation=0)
plt.legend(loc='best')
plt.show()
342/124:
plt.figure(figsize=(8, 5))
plt.bar(range(5), var_exp, alpha=0.5, align='center', label='individual explained variance', color = 'g')
plt.plot(range(5), cum_var_exp,label='cumulative explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal components')
plt.xticks(range(0,5),
           ['PC 1', 'PC 2', 'PC 3', 'PC 4', 'PC 5'], rotation=0)
plt.legend(loc='best')
plt.show()
342/125:
# Calculation of Explained Variance from the eigenvalues
tot = sum(pca.explained_variance_ratio_)
var_exp = [(i/tot)*100 for i in sorted(pca.explained_variance_ratio_, reverse=True)] # Individual explained variance
cum_var_exp = np.cumsum(var_exp) # Cumulative explained variance
cum_var_exp
342/126:
# Calculation of Explained Variance from the eigenvalues
tot = sum(pca.explained_variance_ratio_)
var_exp = [(i/tot)*100 for i in sorted(pca.explained_variance_ratio_, reverse=True)] # Individual explained variance
cum_var_exp = np.cumsum(var_exp) # Cumulative explained variance
cum_var_exp
342/127:
# Calculation of Explained Variance from the eigenvalues
tot = sum(pca.explained_variance_ratio_)
var_exp = [(i/tot)*100 for i in sorted(pca.explained_variance_ratio_, reverse=True)] # Individual explained variance
cum_var_exp = np.cumsum(var_exp) # Cumulative explained variance
cum_var_exp.T
342/128: cum_var_exp.T
342/129: cum_var_exp.transpose()
342/130: pd.DataFrame(cum_var_exp, columns=['PC'])
342/131: pd.DataFrame(cum_var_exp, columns=['PC'], index=['PC 1', 'PC 2', 'PC 3', 'PC 4', 'PC 5'])
342/132:
SumExplained = pd.DataFrame(cum_var_exp, columns=['PC'], index=['PC 1', 'PC 2', 'PC 3', 'PC 4', 'PC 5'])
SumExplained
342/133:
finalDf = pd.concat([principalDf, Bioactivity_class_amylase], axis = 1)
finalDf.head(3)
342/134: Bioactivity_class_glucosidase = df[['Bioactivity_class_glucosidase']]
342/135:
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans 
from mpl_toolkits import mplot3d
342/136:
plt.figure(figsize=(8, 5))
plt.bar(range(5), var_exp, alpha=0.5, align='center', label='individual explained variance', color = 'g')
plt.plot(range(5), cum_var_exp,label='cumulative explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal components')
plt.xticks(range(0,5),
           ['PC 1', 'PC 2', 'PC 3', 'PC 4', 'PC 5'], rotation=0)
plt.legend(loc='best')
plt.savefig('PCA_explained_variance.pdf', dpi=300)
plt.show()
342/137: sns.pairplot(finalDf, x_vars=['principal component 1'], y_vars=['principal component 2'], hue="Index")
342/138: sns.pairplot(finalDf, x_vars=['principal component 1'], y_vars=['principal component 2'], hue=index)
342/139: sns.pairplot(finalDf, x_vars=['principal component 1'], y_vars=['principal component 2'], hue=indices)
342/140: sns.pairplot(finalDf, x_vars=['principal component 1'], y_vars=['principal component 2'], hue=indices)
342/141:
# Calculation of Explained Variance from the eigenvalues
tot = sum(pca.explained_variance_ratio_)
var_exp = [(i/tot)*100 for i in sorted(pca.explained_variance_ratio_, reverse=True)] # Individual explained variance
cum_var_exp = np.cumsum(var_exp) # Cumulative explained variance
342/142: sns.pairplot(finalDf, x_vars=['principal component 1'], y_vars=['principal component 2'], hue='indices')
342/143:
fig = plt.figure()
ax = fig.add_subplot(projection='3d')
ax.scatter(finalDf['principal component 1', 'principal component 2', 'principal component 3', marker=m)
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')

plt.show()
342/144:
fig = plt.figure()
ax = fig.add_subplot(projection='3d')
ax.scatter(finalDf['principal component 1', 'principal component 2', 'principal component 3')
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')

plt.show()
342/145:
fig = plt.figure()
ax = fig.add_subplot(projection='3d')
ax.scatter(finalDf['principal component 1', 'principal component 2', 'principal component 3'])
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')

plt.show()
342/146:
fig = plt.figure()
ax = fig.add_subplot(projection='3d')
ax.scatter(finalDf[['principal component 1', 'principal component 2', 'principal component 3']])
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')

plt.show()
342/147:
fig = plt.figure()
ax = fig.add_subplot(projection='3d')
ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'])
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')

plt.show()
342/148:
fig = plt.figure()
ax = fig.add_subplot(projection='3d')
ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=finalDf['Bioactivity_class_amylase'])
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')

plt.show()
342/149:
LABEL_COLOR_MAP = {active : 'blue',inactive : 'orange'}
label_color = [LABEL_COLOR_MAP[l] for l in finalDf['Bioactivity_class_amylase']]
342/150:
LABEL_COLOR_MAP = {'active' : 'blue','inactive' : 'orange'}
label_color = [LABEL_COLOR_MAP[l] for l in finalDf['Bioactivity_class_amylase']]
342/151:
fig = plt.figure()
ax = fig.add_subplot(projection='3d')
ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color)
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')

plt.show()
342/152:
fig = plt.figure()
ax = fig.add_subplot(projection='3d')
ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color)
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')
plt.figure(figsize=(8, 5))
plt.show()
342/153:
fig = plt.figure(figsize=(8, 8))
ax = fig.add_subplot(projection='3d')
ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color)
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')

plt.show()
342/154:
fig = plt.figure(figsize=(8, 8))
ax = fig.add_subplot(projection='3d')
ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s='100')
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')

plt.show()
342/155:
fig = plt.figure(figsize=(8, 8))
ax = fig.add_subplot(projection='3d')
ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s='2')
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')

plt.show()
342/156:
fig = plt.figure(figsize=(8, 8))
ax = fig.add_subplot(projection='3d')
ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=3)
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')

plt.show()
342/157:
fig = plt.figure(figsize=(8, 8))
ax = fig.add_subplot(projection='3d')
ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')

plt.show()
342/158:
fig = plt.figure(figsize=(4, 4))
ax = fig.add_subplot(projection='3d')
ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')
ax.set_title('PCA')
plt.show()
342/159:
fig = plt.figure(figsize=(6, 6))
ax = fig.add_subplot(projection='3d')
ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')
ax.set_title('PCA')
plt.show()
342/160:
fig = plt.figure(figsize=(6, 6))
ax = fig.add_subplot(projection='3d')
ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')
ax.legend(LABEL_COLOR_MAP)
ax.set_title('PCA')
plt.show()
342/161:
fig = plt.figure(figsize=(6, 6))
ax = fig.add_subplot(projection='3d')
ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')
ax.legend('active' : 'blue','inactive' : 'orange')
ax.set_title('PCA')
plt.show()
342/162:
fig = plt.figure(figsize=(6, 6))
ax = fig.add_subplot(projection='3d')
ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')
ax.legend({'active': 'blue','inactive' : 'orange'})
ax.set_title('PCA')
plt.show()
342/163:
fig = plt.figure(figsize=(6, 6))
ax = fig.add_subplot(projection='3d')
ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')
ax.legend()
ax.set_title('PCA')
plt.show()
342/164:
fig = plt.figure(figsize=(6, 6))
ax = fig.add_subplot(projection='3d')
ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100, )
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')
ax.legend(label_color)
ax.set_title('PCA')
plt.show()
342/165:
fig = plt.figure(figsize=(6, 6))
ax = fig.add_subplot(projection='3d')
ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')
ax.legend(['active' : 'blue','inactive' : 'orange'])
ax.legend()
ax.set_title('PCA')
plt.show()
342/166:
fig = plt.figure(figsize=(6, 6))
ax = fig.add_subplot(projection='3d')
ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')
ax.legend(['active': 'blue','inactive': 'orange'])
ax.set_title('PCA')
plt.show()
342/167:
fig = plt.figure(figsize=(6, 6))
ax = fig.add_subplot(projection='3d')
ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')
ax.legend()
ax.set_title('PCA')
plt.show()
342/168:
fig = plt.figure(figsize=(6, 6))
ax = fig.add_subplot(projection='3d')
ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')
ax.legend(active, blue
         )
ax.set_title('PCA')
plt.show()
342/169:
fig = plt.figure(figsize=(6, 6))
ax = fig.add_subplot(projection='3d')
ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')
ax.legend('active', 'inactive'
         )
ax.set_title('PCA')
plt.show()
342/170:
fig = plt.figure(figsize=(6, 6))
ax = fig.add_subplot(projection='3d')
ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')
ax.legend()
ax.set_title('PCA')
plt.show()
342/171:
fig = plt.figure(figsize=(6, 6))
ax = fig.add_subplot(projection='3d')
ax = ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')
ax.legend(ax.legend_elements)
ax.set_title('PCA')
plt.show()
342/172:
fig = plt.figure(figsize=(6, 6))
ax = fig.add_subplot(projection='3d')
ax = ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')
ax.legend(ax.legend_elements{})
ax.set_title('PCA')
plt.show()
342/173:
fig = plt.figure(figsize=(6, 6))
ax = fig.add_subplot(projection='3d')
ax = ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')
ax.legend(ax.legend_elements())
ax.set_title('PCA')
plt.show()
342/174:
fig = plt.figure(figsize=(6, 6))
ax = fig.add_subplot(projection='3d')
ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')
ax.legend(ax.legend_elements())
ax.set_title('PCA')
plt.show()
342/175:
fig = plt.figure(figsize=(6, 6))
ax = fig.add_subplot(projection='3d')
ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')
plt.legend(ax.legend_elements())
ax.set_title('PCA')
plt.show()
342/176: label_color
342/177: LABEL_COLOR_MAP
342/178:
fig = plt.figure(figsize=(6, 6))
ax = fig.add_subplot(projection='3d')
ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')
plt.legend(LABEL_COLOR_MAP )
ax.set_title('PCA')
plt.show()
342/179:
fig = plt.figure(figsize=(6, 6))
ax = fig.add_subplot(projection='3d')
ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')
plt.legend(LABEL_COLOR_MAP, nrow = 2)
ax.set_title('PCA')
plt.show()
342/180:
fig = plt.figure(figsize=(6, 6))
ax = fig.add_subplot(projection='3d')
ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')
plt.legend(LABEL_COLOR_MAP, ncol = 2)
ax.set_title('PCA')
plt.show()
342/181:
fig = plt.figure(figsize=(6, 6))
ax = fig.add_subplot(projection='3d')
ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')
plt.legend(LABEL_COLOR_MAP)
plt.legend(LABEL_COLOR_MAP)
ax.set_title('PCA')
plt.show()
342/182:
fig = plt.figure(figsize=(6, 6))
ax = fig.add_subplot(projection='3d')
ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')
plt.legend(LABEL_COLOR_MAP)
ax.set_title('PCA')
plt.show()
342/183:
fig = plt.figure(figsize=(6, 6))
ax = fig.add_subplot(projection='3d')
ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')
plt.legend(LABEL_COLOR_MAP, LABEL_COLOR_MAP)
ax.set_title('PCA')
plt.show()
342/184:
fig = plt.figure(figsize=(6, 6))
ax = fig.add_subplot(projection='3d')
ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')
plt.legend(LABEL_COLOR_MAP)
ax.set_title('PCA')
plt.show()
342/185:
fig = plt.figure(figsize=(6, 6))
ax = fig.add_subplot(projection='3d')
ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')
plt.legend(LABEL_COLOR_MAP)
ax.set_title('PCA')
plt.savefig('PCA_3D_amylase.pdf', dpi=300)
plt.show()
342/186:
sns.pairplot(finalDf, hue="Bioactivity_class_amylase", height=2)
plt.savefig('PCA_pairplot_amylase.pdf', dpi=300)
342/187:
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans 
from mpl_toolkits import mplot3d
from sklearn.preprocessing import StandardScaler
342/188:
scaler = StandardScaler()
scaler.fit(X)
X=scaler.transform(X)
342/189:
scaler = StandardScaler()
scaler.fit(x)
x=scaler.transform(x)
342/190: pd.DataFrame(data = x, columns = features, index=indices).head()
342/191: x = df.loc[:, features].values
342/192: pd.DataFrame(data = x, columns = features, index=indices).head()
344/1:
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans 
from mpl_toolkits import mplot3d
from sklearn.preprocessing import StandardScaler
344/2: df = pd.read_csv('df_clean.csv', index_col='Name_Short')
344/3: df.head(2)
344/4: indices = df.index
344/5: features = ['Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']
344/6: x = df.loc[:, features].values
344/7: y = df.loc[:,['Bioactivity_class_amylase']].values
344/8: pd.DataFrame(data = x, columns = features, index=indices).head()
344/9: pca = PCA(n_components=5)
344/10: principalComponents = pca.fit_transform(x)
344/11: pca.explained_variance_ratio_
344/12: principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2', 'principal component 3', 'principal component 4', 'principal component 5'], index=indices)
344/13: principalDf.head(3)
344/14: Bioactivity_class_amylase = df[['Bioactivity_class_amylase']]
344/15:
finalDf = pd.concat([principalDf, Bioactivity_class_amylase], axis = 1)
finalDf.head(3)
344/16:
sns.pairplot(finalDf, hue="Bioactivity_class_amylase", height=2)
plt.savefig('PCA_pairplot_amylase.pdf', dpi=300)
344/17: sns.pairplot(finalDf, x_vars=['principal component 1'], y_vars=['principal component 2'], hue="Bioactivity_class_amylase")
344/18:
# Calculation of Explained Variance from the eigenvalues
tot = sum(pca.explained_variance_ratio_)
var_exp = [(i/tot)*100 for i in sorted(pca.explained_variance_ratio_, reverse=True)] # Individual explained variance
cum_var_exp = np.cumsum(var_exp) # Cumulative explained variance
344/19:
plt.figure(figsize=(8, 5))
plt.bar(range(5), var_exp, alpha=0.5, align='center', label='individual explained variance', color = 'g')
plt.plot(range(5), cum_var_exp,label='cumulative explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal components')
plt.xticks(range(0,5),
           ['PC 1', 'PC 2', 'PC 3', 'PC 4', 'PC 5'], rotation=0)
plt.legend(loc='best')
plt.savefig('PCA_explained_variance.pdf', dpi=300)
plt.show()
344/20:
SumExplained = pd.DataFrame(cum_var_exp, columns=['PC'], index=['PC 1', 'PC 2', 'PC 3', 'PC 4', 'PC 5'])
SumExplained
344/21:
LABEL_COLOR_MAP = {'active' : 'blue','inactive' : 'orange'}
label_color = [LABEL_COLOR_MAP[l] for l in finalDf['Bioactivity_class_amylase']]
344/22:
fig = plt.figure(figsize=(6, 6))
ax = fig.add_subplot(projection='3d')
ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')
plt.legend(LABEL_COLOR_MAP)
ax.set_title('PCA')
plt.savefig('PCA_3D_amylase.pdf', dpi=300)
plt.show()
344/23: x = df.loc[:, features].values
344/24: y = y = df.loc[:,['Bioactivity_class_glucosidase']].values
344/25: y = df.loc[:,['Bioactivity_class_glucosidase']].values
344/26: pca = PCA(n_components=5)
344/27: principalComponents = pca.fit_transform(x)
344/28: pca.explained_variance_ratio_
344/29: principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2', 'principal component 3', 'principal component 4', 'principal component 5'], index=indices)
344/30: Bioactivity_class_glucosidase = df[['Bioactivity_class_glucosidase']]
344/31:
finalDf = pd.concat([principalDf, Bioactivity_class_glucosidase], axis = 1)
finalDf.head(3)
344/32:
sns.pairplot(finalDf, hue="Bioactivity_class_amylase", height=2, palette=pal)
plt.savefig('PCA_pairplot_amylase.pdf', dpi=300)
344/33:
sns.pairplot(finalDf, hue="Bioactivity_class_amylase", height=2, palette="deep")
plt.savefig('PCA_pairplot_glucosidase.pdf', dpi=300)
344/34:
sns.pairplot(finalDf, hue="Bioactivity_class_glucosidase", height=2, palette="deep")
plt.savefig('PCA_pairplot_glucosidase.pdf', dpi=300)
344/35:
sns.pairplot(finalDf, hue="Bioactivity_class_glucosidase", height=2)
plt.savefig('PCA_pairplot_glucosidase.pdf', dpi=300)
344/36:
LABEL_COLOR_MAP = {'active' : 'blue','inactive' : 'orange'}
label_color = [LABEL_COLOR_MAP[l] for l in finalDf['Bioactivity_class_glucosidase']]
344/37:
fig = plt.figure(figsize=(6, 6))
ax = fig.add_subplot(projection='3d')
ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')
plt.legend(LABEL_COLOR_MAP)
ax.set_title('PCA')
plt.savefig('PCA_3D_glucosidase.pdf', dpi=300)
plt.show()
344/38:
fig = plt.figure(figsize=(6, 6))
ax = fig.add_subplot(projection='3d')
ax.scatter(finalDf['principal component 1'], finalDf['principal component 2'], finalDf['principal component 3'], c=label_color, s=100)
ax.set_xlabel('PC 1')
ax.set_ylabel('PC 2')
ax.set_zlabel('PC 3')
plt.legend(LABEL_COLOR_MAP)
ax.set_title('PCA_glucosidase')
plt.savefig('PCA_3D_glucosidase.pdf', dpi=300)
plt.show()
344/39:
LABEL_COLOR_MAP = {'active' : 'blue','inactive' : 'orange'}
label_color = [LABEL_COLOR_MAP[l] for l in finalDf['Bioactivity_class_amylase']]
350/1:
import networkx as nx
import matplotlib.pyplot as plt

G = nx.Graph()
G.add_edge(1, 2)
G.add_edge(1, 3)
G.add_edge(1, 5)
G.add_edge(2, 3)
G.add_edge(3, 4)
G.add_edge(4, 5)

# explicitly set positions
pos = {1: (0, 0), 2: (-1, 0.3), 3: (2, 0.17), 4: (4, 0.255), 5: (5, 0.03)}

options = {
    "font_size": 36,
    "node_size": 3000,
    "node_color": "white",
    "edgecolors": "black",
    "linewidths": 5,
    "width": 5,
}
nx.draw_networkx(G, pos, **options)

# Set margins for the axes so that nodes aren't clipped
ax = plt.gca()
ax.margins(0.20)
plt.axis("off")
plt.show()
350/2:
import networkx as nx
import matplotlib.pyplot as plt

G = nx.Graph()
350/3:
import networkx as nx
import matplotlib.pyplot as plt

G = nx.Graph()
G.add_edge(1, 2)
350/4: G
350/5: G,nodes
350/6: G.nodes
350/7:
import networkx as nx
import matplotlib.pyplot as plt

G = nx.Graph()
G.add_edge(1, 2)
G.add_edge(1, 3)
G.add_edge(1, 5)
G.add_edge(2, 3)
G.add_edge(3, 4)
G.add_edge(4, 5)
350/8: nx.draw(G)
350/9:
import networkx as nx
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
350/10:
import networkx as nx
%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
350/11: df = pd.read_csv('df_clean.csv', index_col='Name_Short')
350/12: df.head(2)
350/13: df1 = df[['Name_Short', 'Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]
350/14: df = pd.read_csv('df_clean.csv')
350/15: df.head(2)
350/16: df1 = df[['Name_Short', 'Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]
350/17: df1
350/18: df1 = df[['Emax_amylase_2.75mcgml','Emax_glucosidase_0.67mcgml','Name_Short', 'Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]
350/19: df1
350/20: df1 = df[['Name_Short','Emax_amylase_2.75mcgml','Emax_glucosidase_0.67mcgml', 'Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]
350/21: df1
350/22: df1 = df[['Name_Short','Emax_amylase_2.75mcgml','Emax_glucosidase_0.67mcgml', 'Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]
350/23: df1
350/24: df1.head(3)
350/25: G = nx.Graph()
350/26: G = nx.from_pandas_edgelist(df1, 'Name_Short', 'Emax_amylase_2.75mcgml', edge_attr=True, edge_key='Emax_glucosidase_0.67mcgml')
350/27:
from matplotlib.pyplot import figurefigure(figsize=(10, 8))
nx.draw_shell(G, with_labels=True)
350/28:
from matplotlib.pyplot import figure
figure(figsize=(10, 8))
nx.draw_shell(G, with_labels=True)
350/29: G = nx.from_pandas_edgelist(df1, 'Name_Short', edge_attr='Emax_amylase_2.75mcgml', edge_key='Emax_glucosidase_0.67mcgml')
350/30: G = nx.from_pandas_edgelist(df1, 'Name_Short', 'Name_Short', edge_attr='Emax_amylase_2.75mcgml', edge_key='Emax_glucosidase_0.67mcgml')
350/31:
from matplotlib.pyplot import figure
figure(figsize=(10, 8))
nx.draw_shell(G, with_labels=True)
350/32: G = nx.from_pandas_edgelist(df1, 'Name_Short', 'Name_Short', ['Alkaloids', 'Antaquinones'])
350/33:
from matplotlib.pyplot import figure
figure(figsize=(10, 8))
nx.draw_shell(G, with_labels=True)
350/34: G = nx.from_pandas_edgelist(df1, 'Name_Short', 'Carotenoids', ['Alkaloids', 'Antaquinones'])
350/35:
from matplotlib.pyplot import figure
figure(figsize=(10, 8))
nx.draw_shell(G, with_labels=True)
350/36: df1 = df[['Name_Short','Bioactivity_class_amylase','Bioactivity_class_glucosidase', 'Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]
350/37: df1.head(3)
350/38: G = nx.Graph()
350/39: G = nx.from_pandas_edgelist(df1, 'Name_Short', 'Bioactivity_class_glucosidase', ['Alkaloids', 'Antaquinones'])
350/40:
from matplotlib.pyplot import figure
figure(figsize=(10, 8))
nx.draw_shell(G, with_labels=True)
350/41: G = nx.from_pandas_edgelist(df1, 'Name_Short', 'Bioactivity_class_glucosidase', 'Bioactivity_class_amylase', ['Alkaloids', 'Antaquinones'])
350/42: G = nx.from_pandas_edgelist(df1, 'Name_Short', 'Bioactivity_class_glucosidase', ['Alkaloids', 'Antaquinones'])
350/43: df1 = df[['Name_Short','Bioactivity_class_amylase','Bioactivity_class_glucosidase', 'Emax_glucosidase_0.67mcgml', 'Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]
350/44: df1.head(3)
350/45: G = nx.Graph()
350/46: G = nx.from_pandas_edgelist(df1, 'Name_Short', 'Bioactivity_class_glucosidase', edge_attr='Emax_glucosidase_0.67mcgml')
350/47:
from matplotlib.pyplot import figure
figure(figsize=(10, 8))
nx.draw_shell(G, with_labels=True)
350/48: G = nx.from_pandas_edgelist(df1, 'Name_Short', 'Bioactivity_class_glucosidase', edge_attr='flavonoids')
350/49:
from matplotlib.pyplot import figure
figure(figsize=(10, 8))
nx.draw_shell(G, with_labels=True)
348/1:
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
348/2: df = pd.read_csv('antihyperglycemia.csv')
348/3: df
348/4:
df = df.drop(['Number', 'Name_Thai'], axis=1)
df
348/5:
from scipy.stats import pearsonr
rho = df.corr()
pval = df.corr(method=lambda x, y: pearsonr(x, y)[1]) - np.eye(*rho.shape)
p = pval.applymap(lambda x: ''.join(['*' for t in [0.001,0.01,0.05] if x<=t]))
Res_pearson_sig = rho.round(3).astype(str) + p
Res_pearson_sig
348/6:
Res_pearson = df.corr(method='pearson').round(3)
Res_pearson
348/7:
Res_pearson = df.corr(method='pearson').round(3)
Res_pearson.to_csv('Res_pearson.csv')
Res_pearson
350/50: df = pd.read_csv('Res_pearson.csv')
350/51: df
350/52: df['Unnamed: 0'] = df['List']
350/53: df['Unnamed: 0': 'List']
350/54: df
350/55:
from matplotlib.pyplot import figure
figure(figsize=(10, 8))
nx.draw_networkx_nodes(G, with_labels=True)
350/56:
from matplotlib.pyplot import figure
figure(figsize=(10, 8))
nx.draw_networkx_nodes(G)
350/57:
from matplotlib.pyplot import figure
figure(figsize=(10, 8))
nx.draw_shell(G, with_labels=True)
350/58:
import networkx as nx
%matplotlib inline
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
import numpy as np
import pandas as pd
350/59: networkx.draw(G, with_labels=True, font_weight='bold')
350/60: nx.draw(G, with_labels=True, font_weight='bold')
350/61: df = pd.read_csv('Res_pearson.csv')
350/62: df
350/63: df.rename(colums={'Unnamed: 0' : Name})
350/64: df.rename(colums={'Unnamed: 0' : 'Name'})
350/65: df.rename(columns={'Unnamed: 0' : 'Name'})
350/66: G = nx.from_pandas_edgelist(df)
348/8: rho
348/9: pval
348/10: rho
348/11: val
348/12: pal
348/13: pval
348/14: rho
348/15: edges = rho.stack().reset_index()
348/16:
import numpy as np
import pandas as pd
import scipy.stats
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
import networkx as nx
348/17: edges.columns = ['Features_1','Features_2','correlation']
348/18:
#convert matrix to list of edges and rename the columns
edges = rho.stack().reset_index()
edges.columns = ['Features_1','Features_2','correlation']
348/19:
#remove self correlations
edges = edges.loc[edges['Features_1'] != edges['Features_2']].copy()
348/20: edges.head()
348/21:
#create undirected graph with weights corresponding to the correlation magnitude
G0 = nx.from_pandas_edgelist(edges, 'Features_1', 'Features_2', edge_attr=['correlation'])

#print out the graph info
#check number of nodes and degrees are as expected (all should have degree = 38, i.e. average degree = 38)
print(nx.info(G0))
348/22:
fig, ax = plt.subplots(nrows=2, ncols=2,figsize=(20,20))

nx.draw(G0, with_labels=True, node_size=700, node_color="#e1575c",
        edge_color='#363847',  pos=nx.circular_layout(G0),ax=ax[0,0])
ax[0,0].set_title("Circular layout")

nx.draw(G0, with_labels=True, node_size=700, node_color="#e1575c",
        edge_color='#363847',  pos=nx.random_layout(G0),ax=ax[0,1])
ax[0,1].set_title("Random layout")

nx.draw(G0, with_labels=True, node_size=700, node_color="#e1575c",
        edge_color='#363847',  pos=nx.spring_layout(G0),ax=ax[1,0])
ax[1,0].set_title("Spring layout")

nx.draw(G0, with_labels=True, node_size=700, node_color="#e1575c",
        edge_color='#363847',  pos=nx.spectral_layout(G0),ax=ax[1,1])
ax[1,1].set_title("Spectral layout")

plt.show()
348/23:
# 'winner takes all' method - set minium correlation threshold to remove some edges from the diagram
threshold = 0.5

# create a new graph from edge list
Gx = nx.from_pandas_edgelist(edges, 'Features_1', 'Features_2', edge_attr=['correlation'])

# list to store edges to remove
remove = []
# loop through edges in Gx and find correlations which are below the threshold
for asset_1, asset_2 in Gx.edges():
    corr = Gx[asset_1][asset_2]['correlation']
    #add to remove node list if abs(corr) < threshold
    if abs(corr) < threshold:
        remove.append((asset_1, asset_2))

# remove edges contained in the remove list
Gx.remove_edges_from(remove)

print(str(len(remove)) + " edges removed")
348/24:
ef assign_colour(correlation):
    if correlation <= 0:
        return "#ffa09b"  # red
    else:
        return "#9eccb7"  # green


def assign_thickness(correlation, benchmark_thickness=2, scaling_factor=3):
    return benchmark_thickness * abs(correlation)**scaling_factor


def assign_node_size(degree, scaling_factor=50):
    return degree * scaling_factor


# assign colours to edges depending on positive or negative correlation
# assign edge thickness depending on magnitude of correlation
edge_colours = []
edge_width = []
for key, value in nx.get_edge_attributes(Gx, 'correlation').items():
    edge_colours.append(assign_colour(value))
    edge_width.append(assign_thickness(value))

# assign node size depending on number of connections (degree)
node_size = []
for key, value in dict(Gx.degree).items():
    node_size.append(assign_node_size(value))
348/25:
def assign_colour(correlation):
    if correlation <= 0:
        return "#ffa09b"  # red
    else:
        return "#9eccb7"  # green


def assign_thickness(correlation, benchmark_thickness=2, scaling_factor=3):
    return benchmark_thickness * abs(correlation)**scaling_factor


def assign_node_size(degree, scaling_factor=50):
    return degree * scaling_factor


# assign colours to edges depending on positive or negative correlation
# assign edge thickness depending on magnitude of correlation
edge_colours = []
edge_width = []
for key, value in nx.get_edge_attributes(Gx, 'correlation').items():
    edge_colours.append(assign_colour(value))
    edge_width.append(assign_thickness(value))

# assign node size depending on number of connections (degree)
node_size = []
for key, value in dict(Gx.degree).items():
    node_size.append(assign_node_size(value))
348/26:
# draw improved graph
sns.set(rc={'figure.figsize': (9, 9)})
font_dict = {'fontsize': 18}

nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,
        node_size=node_size, node_color="#e1575c", edge_color=edge_colours,
        width=edge_width)
plt.title("Correlation_matrix", fontdict=font_dict)
plt.show()
348/27:
# draw improved graph
sns.set(rc={'figure.figsize': (9, 9)})
font_dict = {'fontsize': 18}

nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,
        node_size=node_size, node_color="#e1575c", edge_color=edge_colours,
        width=edge_width)
plt.title("Correlation R>0.5", fontdict=font_dict)
plt.show()
348/28:
# draw improved graph
sns.set(rc={'figure.figsize': (9, 9)})
font_dict = {'fontsize': 18}

nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,
        node_size=node_size, node_color="#e1575c", edge_color=edge_colours,
        width=edge_width)
plt.title("Correlation R > 0.5", fontdict=font_dict)
plt.show()
348/29:
# draw improved graph
sns.set(rc={'figure.figsize': (9, 9)})
font_dict = {'fontsize': 30}

nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,
        node_size=node_size, node_color="#e1575c", edge_color=edge_colours,
        width=edge_width)
plt.title("Correlation R > 0.5", fontdict=font_dict)
plt.show()
348/30:
# draw improved graph
sns.set(rc={'figure.figsize': (9, 9)})
font_dict = {'fontsize': 18}

nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,
        node_size=node_size, node_color="#e1575c", edge_color=edge_colours,
        width=edge_width)
plt.title("Correlation R > 0.5", fontdict=font_dict)
plt.show()
348/31:
# draw improved graph
sns.set(rc={'figure.figsize': (12, 12)})
font_dict = {'fontsize': 18}

nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,
        node_size=node_size, node_color="#e1575c", edge_color=edge_colours,
        width=edge_width)
plt.title("Correlation R > 0.5", fontdict=font_dict)
plt.show()
348/32:
# 'winner takes all' method - set minium correlation threshold to remove some edges from the diagram
threshold = 0.5

# create a new graph from edge list
Gx = nx.from_pandas_edgelist(edges, 'Features_1', 'Features_2', edge_attr=['correlation'])

# list to store edges to remove
remove = []
# loop through edges in Gx and find correlations which are below the threshold
for Features_1, Features_2 in Gx.edges():
    corr = Gx[asset_1][asset_2]['correlation']
    #add to remove node list if abs(corr) < threshold
    if abs(corr) < threshold:
        remove.append((Features_1, Features_2))

# remove edges contained in the remove list
Gx.remove_edges_from(remove)

print(str(len(remove)) + " edges removed")
348/33:
def assign_colour(correlation):
    if correlation <= 0:
        return "#ffa09b"  # red
    else:
        return "#9eccb7"  # green


def assign_thickness(correlation, benchmark_thickness=2, scaling_factor=3):
    return benchmark_thickness * abs(correlation)**scaling_factor


def assign_node_size(degree, scaling_factor=50):
    return degree * scaling_factor


# assign colours to edges depending on positive or negative correlation
# assign edge thickness depending on magnitude of correlation
edge_colours = []
edge_width = []
for key, value in nx.get_edge_attributes(Gx, 'correlation').items():
    edge_colours.append(assign_colour(value))
    edge_width.append(assign_thickness(value))

# assign node size depending on number of connections (degree)
node_size = []
for key, value in dict(Gx.degree).items():
    node_size.append(assign_node_size(value))
348/34:
# draw improved graph
sns.set(rc={'figure.figsize': (12, 12)})
font_dict = {'fontsize': 18}

nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,
        node_size=node_size, node_color="#e1575c", edge_color=edge_colours,
        width=edge_width)
plt.title("Correlation R > 0.5", fontdict=font_dict)
plt.show()
348/35:
# 'winner takes all' method - set minium correlation threshold to remove some edges from the diagram
threshold = 0.5

# create a new graph from edge list
Gx = nx.from_pandas_edgelist(edges, 'Features_1', 'Features_2', edge_attr=['correlation'])

# list to store edges to remove
remove = []
# loop through edges in Gx and find correlations which are below the threshold
for Features_1, Features_2 in Gx.edges():
    corr = Gx[Features_1][Features_2]['correlation']
    #add to remove node list if abs(corr) < threshold
    if abs(corr) < threshold:
        remove.append((Features_1, Features_2))

# remove edges contained in the remove list
Gx.remove_edges_from(remove)

print(str(len(remove)) + " edges removed")
348/36:
def assign_colour(correlation):
    if correlation <= 0:
        return "#ffa09b"  # red
    else:
        return "#9eccb7"  # green


def assign_thickness(correlation, benchmark_thickness=2, scaling_factor=3):
    return benchmark_thickness * abs(correlation)**scaling_factor


def assign_node_size(degree, scaling_factor=50):
    return degree * scaling_factor


# assign colours to edges depending on positive or negative correlation
# assign edge thickness depending on magnitude of correlation
edge_colours = []
edge_width = []
for key, value in nx.get_edge_attributes(Gx, 'correlation').items():
    edge_colours.append(assign_colour(value))
    edge_width.append(assign_thickness(value))

# assign node size depending on number of connections (degree)
node_size = []
for key, value in dict(Gx.degree).items():
    node_size.append(assign_node_size(value))
348/37:
# draw improved graph
sns.set(rc={'figure.figsize': (12, 12)})
font_dict = {'fontsize': 18}

nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,
        node_size=node_size, node_color="#e1575c", edge_color=edge_colours,
        width=edge_width)
plt.title("Correlation R > 0.5", fontdict=font_dict)
plt.show()
348/38:
# draw improved graph
sns.set(rc={'figure.figsize': (5, 5)})
font_dict = {'fontsize': 18}

nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,
        node_size=node_size, node_color="#e1575c", edge_color=edge_colours,
        width=edge_width)
plt.title("Correlation R > 0.5", fontdict=font_dict)
plt.show()
348/39:
# 'winner takes all' method - set minium correlation threshold to remove some edges from the diagram
threshold = 0.4

# create a new graph from edge list
Gx = nx.from_pandas_edgelist(edges, 'Features_1', 'Features_2', edge_attr=['correlation'])

# list to store edges to remove
remove = []
# loop through edges in Gx and find correlations which are below the threshold
for Features_1, Features_2 in Gx.edges():
    corr = Gx[Features_1][Features_2]['correlation']
    #add to remove node list if abs(corr) < threshold
    if abs(corr) < threshold:
        remove.append((Features_1, Features_2))

# remove edges contained in the remove list
Gx.remove_edges_from(remove)

print(str(len(remove)) + " edges removed")
348/40:
def assign_colour(correlation):
    if correlation <= 0:
        return "#ffa09b"  # red
    else:
        return "#9eccb7"  # green


def assign_thickness(correlation, benchmark_thickness=2, scaling_factor=3):
    return benchmark_thickness * abs(correlation)**scaling_factor


def assign_node_size(degree, scaling_factor=50):
    return degree * scaling_factor


# assign colours to edges depending on positive or negative correlation
# assign edge thickness depending on magnitude of correlation
edge_colours = []
edge_width = []
for key, value in nx.get_edge_attributes(Gx, 'correlation').items():
    edge_colours.append(assign_colour(value))
    edge_width.append(assign_thickness(value))

# assign node size depending on number of connections (degree)
node_size = []
for key, value in dict(Gx.degree).items():
    node_size.append(assign_node_size(value))
348/41:
# draw improved graph
sns.set(rc={'figure.figsize': (5, 5)})
font_dict = {'fontsize': 18}

nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,
        node_size=node_size, node_color="#e1575c", edge_color=edge_colours,
        width=edge_width)
plt.title("Correlation R > 0.5", fontdict=font_dict)
plt.show()
348/42:
# draw improved graph
sns.set(rc={'figure.figsize': (5, 5)})
font_dict = {'fontsize': 18}

nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,
        node_size=node_size, node_color="#e1575c", edge_color=edge_colours,
        width=edge_width)
plt.title("Correlation R > 0.4", fontdict=font_dict)
plt.show()
348/43:
def assign_colour(correlation):
    if correlation <= 0:
        return "#ffa09b"  # red
    else:
        return "#9eccb7"  # green


def assign_thickness(correlation, benchmark_thickness=2, scaling_factor=5):
    return benchmark_thickness * abs(correlation)**scaling_factor


def assign_node_size(degree, scaling_factor=100):
    return degree * scaling_factor


# assign colours to edges depending on positive or negative correlation
# assign edge thickness depending on magnitude of correlation
edge_colours = []
edge_width = []
for key, value in nx.get_edge_attributes(Gx, 'correlation').items():
    edge_colours.append(assign_colour(value))
    edge_width.append(assign_thickness(value))

# assign node size depending on number of connections (degree)
node_size = []
for key, value in dict(Gx.degree).items():
    node_size.append(assign_node_size(value))
348/44:
# draw improved graph
sns.set(rc={'figure.figsize': (5, 5)})
font_dict = {'fontsize': 18}

nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,
        node_size=node_size, node_color="#e1575c", edge_color=edge_colours,
        width=edge_width)
plt.title("Correlation R > 0.4", fontdict=font_dict)
plt.show()
348/45:
def assign_colour(correlation):
    if correlation <= 0:
        return "#ffa09b"  # red
    else:
        return "#9eccb7"  # green


def assign_thickness(correlation, benchmark_thickness=5, scaling_factor=5):
    return benchmark_thickness * abs(correlation)**scaling_factor


def assign_node_size(degree, scaling_factor=100):
    return degree * scaling_factor


# assign colours to edges depending on positive or negative correlation
# assign edge thickness depending on magnitude of correlation
edge_colours = []
edge_width = []
for key, value in nx.get_edge_attributes(Gx, 'correlation').items():
    edge_colours.append(assign_colour(value))
    edge_width.append(assign_thickness(value))

# assign node size depending on number of connections (degree)
node_size = []
for key, value in dict(Gx.degree).items():
    node_size.append(assign_node_size(value))
348/46:
# draw improved graph
sns.set(rc={'figure.figsize': (5, 5)})
font_dict = {'fontsize': 18}

nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,
        node_size=node_size, node_color="#e1575c", edge_color=edge_colours,
        width=edge_width)
plt.title("Correlation R > 0.4", fontdict=font_dict)
plt.show()
348/47:
def assign_colour(correlation):
    if correlation <= 0:
        return "#ffa09b"  # red
    else:
        return "#9eccb7"  # green


def assign_thickness(correlation, benchmark_thickness=5, scaling_factor=5):
    return benchmark_thickness * abs(correlation)**scaling_factor


def assign_node_size(degree, scaling_factor=100):
    return degree * scaling_factor


# assign colours to edges depending on positive or negative correlation
# assign edge thickness depending on magnitude of correlation
edge_colours = []
edge_width = []
for key, value in nx.get_edge_attributes(Gx, 'correlation').items():
    edge_colours.append(assign_colour(value))
    edge_width.append(assign_thickness(value))

# assign node size depending on number of connections (degree)
node_size = []
for key, value in dict(Gx.degree).items():
    node_size.append(assign_node_size(value))
348/48:
# draw improved graph
sns.set(rc={'figure.figsize': (5, 5)})
font_dict = {'fontsize': 18}

nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,
        node_size=node_size, node_color="#e1575c", edge_color=edge_colours,
        width=edge_width)
plt.title("Correlation R > 0.4", fontdict=font_dict)
plt.show()
348/49:
# draw improved graph
sns.set(rc={'figure.figsize': (5, 5)})
font_dict = {'fontsize': 18}

nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,
        node_size=node_size, node_color="#e1575c", edge_color=edge_colours,
        width=edge_width)
plt.title("Correlation R > 0.4", fontdict=font_dict)
plt.show()
348/50:
def assign_colour(correlation):
    if correlation <= 0:
        return "#ffa09b"  # red
    else:
        return "#9eccb7"  # green


def assign_thickness(correlation, benchmark_thickness=5, scaling_factor=5):
    return benchmark_thickness * abs(correlation)**scaling_factor


def assign_node_size(degree, scaling_factor=50):
    return degree * scaling_factor


# assign colours to edges depending on positive or negative correlation
# assign edge thickness depending on magnitude of correlation
edge_colours = []
edge_width = []
for key, value in nx.get_edge_attributes(Gx, 'correlation').items():
    edge_colours.append(assign_colour(value))
    edge_width.append(assign_thickness(value))

# assign node size depending on number of connections (degree)
node_size = []
for key, value in dict(Gx.degree).items():
    node_size.append(assign_node_size(value))
348/51:
# draw improved graph
sns.set(rc={'figure.figsize': (5, 5)})
font_dict = {'fontsize': 18}

nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,
        node_size=node_size, node_color="#e1575c", edge_color=edge_colours,
        width=edge_width)
plt.title("Correlation R > 0.4", fontdict=font_dict)
plt.show()
348/52:
# 'winner takes all' method - set minium correlation threshold to remove some edges from the diagram
threshold = 0.2

# create a new graph from edge list
Gx = nx.from_pandas_edgelist(edges, 'Features_1', 'Features_2', edge_attr=['correlation'])

# list to store edges to remove
remove = []
# loop through edges in Gx and find correlations which are below the threshold
for Features_1, Features_2 in Gx.edges():
    corr = Gx[Features_1][Features_2]['correlation']
    #add to remove node list if abs(corr) < threshold
    if abs(corr) < threshold:
        remove.append((Features_1, Features_2))

# remove edges contained in the remove list
Gx.remove_edges_from(remove)

print(str(len(remove)) + " edges removed")
348/53:
def assign_colour(correlation):
    if correlation <= 0:
        return "#ffa09b"  # red
    else:
        return "#9eccb7"  # green


def assign_thickness(correlation, benchmark_thickness=5, scaling_factor=5):
    return benchmark_thickness * abs(correlation)**scaling_factor


def assign_node_size(degree, scaling_factor=50):
    return degree * scaling_factor


# assign colours to edges depending on positive or negative correlation
# assign edge thickness depending on magnitude of correlation
edge_colours = []
edge_width = []
for key, value in nx.get_edge_attributes(Gx, 'correlation').items():
    edge_colours.append(assign_colour(value))
    edge_width.append(assign_thickness(value))

# assign node size depending on number of connections (degree)
node_size = []
for key, value in dict(Gx.degree).items():
    node_size.append(assign_node_size(value))
348/54:
# draw improved graph
sns.set(rc={'figure.figsize': (5, 5)})
font_dict = {'fontsize': 18}

nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,
        node_size=node_size, node_color="#e1575c", edge_color=edge_colours,
        width=edge_width)
plt.title("Correlation R > 0.4", fontdict=font_dict)
plt.show()
348/55:
# draw improved graph
sns.set(rc={'figure.figsize': (10, 10)})
font_dict = {'fontsize': 18}

nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,
        node_size=node_size, node_color="#e1575c", edge_color=edge_colours,
        width=edge_width)
plt.title("Correlation R > 0.4", fontdict=font_dict)
plt.show()
348/56:
def assign_colour(correlation):
    if correlation <= 0:
        return "#ffa09b"  # red
    else:
        return "#9eccb7"  # green


def assign_thickness(correlation, benchmark_thickness=100, scaling_factor=5):
    return benchmark_thickness * abs(correlation)**scaling_factor


def assign_node_size(degree, scaling_factor=50):
    return degree * scaling_factor


# assign colours to edges depending on positive or negative correlation
# assign edge thickness depending on magnitude of correlation
edge_colours = []
edge_width = []
for key, value in nx.get_edge_attributes(Gx, 'correlation').items():
    edge_colours.append(assign_colour(value))
    edge_width.append(assign_thickness(value))

# assign node size depending on number of connections (degree)
node_size = []
for key, value in dict(Gx.degree).items():
    node_size.append(assign_node_size(value))
348/57:
# draw improved graph
sns.set(rc={'figure.figsize': (10, 10)})
font_dict = {'fontsize': 18}

nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,
        node_size=node_size, node_color="#e1575c", edge_color=edge_colours,
        width=edge_width)
plt.title("Correlation R > 0.4", fontdict=font_dict)
plt.show()
348/58:
# draw improved graph
sns.set(rc={'figure.figsize': (5, 5)})
font_dict = {'fontsize': 18}

nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,
        node_size=node_size, node_color="#e1575c", edge_color=edge_colours,
        width=edge_width)
plt.title("Correlation R > 0.4", fontdict=font_dict)
plt.show()
348/59:
def assign_colour(correlation):
    if correlation <= 0:
        return "blue"  # red
    else:
        return "#9eccb7"  # green


def assign_thickness(correlation, benchmark_thickness=100, scaling_factor=5):
    return benchmark_thickness * abs(correlation)**scaling_factor


def assign_node_size(degree, scaling_factor=50):
    return degree * scaling_factor


# assign colours to edges depending on positive or negative correlation
# assign edge thickness depending on magnitude of correlation
edge_colours = []
edge_width = []
for key, value in nx.get_edge_attributes(Gx, 'correlation').items():
    edge_colours.append(assign_colour(value))
    edge_width.append(assign_thickness(value))

# assign node size depending on number of connections (degree)
node_size = []
for key, value in dict(Gx.degree).items():
    node_size.append(assign_node_size(value))
348/60:
# draw improved graph
sns.set(rc={'figure.figsize': (5, 5)})
font_dict = {'fontsize': 18}

nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,
        node_size=node_size, node_color="#e1575c", edge_color=edge_colours,
        width=edge_width)
plt.title("Correlation R > 0.4", fontdict=font_dict)
plt.show()
348/61:
def assign_colour(correlation):
    if correlation <= 0:
        return "blue"  # red
    else:
        return "#Green"  # green


def assign_thickness(correlation, benchmark_thickness=100, scaling_factor=5):
    return benchmark_thickness * abs(correlation)**scaling_factor


def assign_node_size(degree, scaling_factor=50):
    return degree * scaling_factor


# assign colours to edges depending on positive or negative correlation
# assign edge thickness depending on magnitude of correlation
edge_colours = []
edge_width = []
for key, value in nx.get_edge_attributes(Gx, 'correlation').items():
    edge_colours.append(assign_colour(value))
    edge_width.append(assign_thickness(value))

# assign node size depending on number of connections (degree)
node_size = []
for key, value in dict(Gx.degree).items():
    node_size.append(assign_node_size(value))
348/62:
def assign_colour(correlation):
    if correlation <= 0:
        return "blue"  # red
    else:
        return "Green"  # green


def assign_thickness(correlation, benchmark_thickness=100, scaling_factor=5):
    return benchmark_thickness * abs(correlation)**scaling_factor


def assign_node_size(degree, scaling_factor=50):
    return degree * scaling_factor


# assign colours to edges depending on positive or negative correlation
# assign edge thickness depending on magnitude of correlation
edge_colours = []
edge_width = []
for key, value in nx.get_edge_attributes(Gx, 'correlation').items():
    edge_colours.append(assign_colour(value))
    edge_width.append(assign_thickness(value))

# assign node size depending on number of connections (degree)
node_size = []
for key, value in dict(Gx.degree).items():
    node_size.append(assign_node_size(value))
348/63:
# draw improved graph
sns.set(rc={'figure.figsize': (5, 5)})
font_dict = {'fontsize': 18}

nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,
        node_size=node_size, node_color="#e1575c", edge_color=edge_colours,
        width=edge_width)
plt.title("Correlation R > 0.4", fontdict=font_dict)
plt.show()
348/64:
def assign_colour(correlation):
    if correlation <= 0:
        return "blue"  # red
    else:
        return "Green"  # green


def assign_thickness(correlation, benchmark_thickness=50, scaling_factor=5):
    return benchmark_thickness * abs(correlation)**scaling_factor


def assign_node_size(degree, scaling_factor=50):
    return degree * scaling_factor


# assign colours to edges depending on positive or negative correlation
# assign edge thickness depending on magnitude of correlation
edge_colours = []
edge_width = []
for key, value in nx.get_edge_attributes(Gx, 'correlation').items():
    edge_colours.append(assign_colour(value))
    edge_width.append(assign_thickness(value))

# assign node size depending on number of connections (degree)
node_size = []
for key, value in dict(Gx.degree).items():
    node_size.append(assign_node_size(value))
348/65:
# draw improved graph
sns.set(rc={'figure.figsize': (5, 5)})
font_dict = {'fontsize': 18}

nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,
        node_size=node_size, node_color="#e1575c", edge_color=edge_colours,
        width=edge_width)
plt.title("Correlation R > 0.4", fontdict=font_dict)
plt.show()
348/66:
def assign_colour(correlation):
    if correlation <= 0:
        return "blue"  # red
    else:
        return "Green"  # green


def assign_thickness(correlation, benchmark_thickness=20, scaling_factor=5):
    return benchmark_thickness * abs(correlation)**scaling_factor


def assign_node_size(degree, scaling_factor=50):
    return degree * scaling_factor


# assign colours to edges depending on positive or negative correlation
# assign edge thickness depending on magnitude of correlation
edge_colours = []
edge_width = []
for key, value in nx.get_edge_attributes(Gx, 'correlation').items():
    edge_colours.append(assign_colour(value))
    edge_width.append(assign_thickness(value))

# assign node size depending on number of connections (degree)
node_size = []
for key, value in dict(Gx.degree).items():
    node_size.append(assign_node_size(value))
348/67:
# draw improved graph
sns.set(rc={'figure.figsize': (5, 5)})
font_dict = {'fontsize': 18}

nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,
        node_size=node_size, node_color="#e1575c", edge_color=edge_colours,
        width=edge_width)
plt.title("Correlation R > 0.4", fontdict=font_dict)
plt.show()
348/68:
# 'winner takes all' method - set minium correlation threshold to remove some edges from the diagram
threshold = 0.4

# create a new graph from edge list
Gx = nx.from_pandas_edgelist(edges, 'Features_1', 'Features_2', edge_attr=['correlation'])

# list to store edges to remove
remove = []
# loop through edges in Gx and find correlations which are below the threshold
for Features_1, Features_2 in Gx.edges():
    corr = Gx[Features_1][Features_2]['correlation']
    #add to remove node list if abs(corr) < threshold
    if abs(corr) < threshold:
        remove.append((Features_1, Features_2))

# remove edges contained in the remove list
Gx.remove_edges_from(remove)

print(str(len(remove)) + " edges removed")
348/69:
def assign_colour(correlation):
    if correlation <= 0:
        return "blue"  # red
    else:
        return "Green"  # green


def assign_thickness(correlation, benchmark_thickness=20, scaling_factor=5):
    return benchmark_thickness * abs(correlation)**scaling_factor


def assign_node_size(degree, scaling_factor=50):
    return degree * scaling_factor


# assign colours to edges depending on positive or negative correlation
# assign edge thickness depending on magnitude of correlation
edge_colours = []
edge_width = []
for key, value in nx.get_edge_attributes(Gx, 'correlation').items():
    edge_colours.append(assign_colour(value))
    edge_width.append(assign_thickness(value))

# assign node size depending on number of connections (degree)
node_size = []
for key, value in dict(Gx.degree).items():
    node_size.append(assign_node_size(value))
348/70:
# draw improved graph
sns.set(rc={'figure.figsize': (5, 5)})
font_dict = {'fontsize': 18}

nx.draw(Gx, pos=nx.circular_layout(Gx), with_labels=True,
        node_size=node_size, node_color="#e1575c", edge_color=edge_colours,
        width=edge_width)
plt.title("Correlation R > 0.4", fontdict=font_dict)
plt.show()
352/1:
import networkx as nx
%matplotlib inline
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_predict, cross_validate
from sklearn.ensemble import RandomForestClassifier
352/2: df = pd.read_csv('df_clean.csv')
352/3: df
352/4:
bioactivity_class = []
for i in df.Bioactivity_class_amylase:
    if df['Bioactivity_class_amylase']='active':
        bioactivity_class.append("active")
352/5:
bioactivity_class = []
for i in df.Bioactivity_class_amylase:
    if i = 'active':
        bioactivity_class.append("active")
352/6:
bioactivity_class = []
for i in df.Bioactivity_class_amylase:
    if i=='active':
        bioactivity_class.append("active")
352/7: bioactivity_class
352/8:
bioactivity_class = []
for i in df.Bioactivity_class_amylase&df.Bioactivity_class_amylase:
    if i=='active':
        bioactivity_class.append("active")
    elif
352/9: df = pd.read_csv('df_clean.csv')
352/10: df
352/11: df = pd.read_csv('df_clean.csv')
352/12: df
352/13: df = pd.read_csv('df_clean_class.csv')
352/14: df
352/15:
bioactivity_class = []
for i in df.Bioactivity_class_amylase:
    if i=='active':
        bioactivity_class.append("active")
    elif:
        bioactivity_class.append("inactive")
352/16:
bioactivity_class = []
for i in df.Bioactivity_class_amylase:
    if i=='active':
        bioactivity_class.append("active")
    elif:
        bioactivity_class.append("inactive")
352/17:
bioactivity_class = []
for i in df.Bioactivity_class_amylase:
    if i=='active':
        bioactivity_class.append("active")
    elif
        bioactivity_class.append("inactive")
352/18:
bioactivity_class = []
for i in df.Bioactivity_class_amylase:
    if i=='active':
        bioactivity_class.append("active")
    elif :
        bioactivity_class.append("inactive")
352/19:
bioactivity_class = []
for i in df.Bioactivity_class_amylase:
    if i=='active':
        bioactivity_class.append("active")
    else:
        bioactivity_class.append("inactive")
352/20: bioactivity_class
352/21: df = pd.read_csv('df_clean_class.csv', index = 'Name_Short')
352/22: df = pd.read_csv('df_clean_class.csv', index_col = 'Name_Short')
352/23: df
352/24:
plant_df = df.copy()
fingerprint_to_model = plant_df.[['Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]
label_to_model = compound_df.Bioactivity_class.tolist()
352/25:
plant_df = df.copy()
fingerprint_to_model = plant_df[['Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]
label_to_model = compound_df.Bioactivity_class.tolist()
352/26:
plant_df = df.copy()
fingerprint_to_model = plant_df[['Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]
label_to_model = plant_df.Bioactivity_class.tolist()
352/27: indices = fingerprint_to_model.index
352/28: fingerprint_to_model.ehad()
352/29: fingerprint_to_model.head()
352/30: label_to_model
352/31: label_to_model.head()
352/32: label_to_model
352/33:
train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
352/34: indices_train, indices_test
352/35:
from sklearn.model_selection import cross_val_score, cross_val_predict, KFold
# Shuffle the indices for the k-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=42)
352/36:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestClassifier(n_estimators=100, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)


#calculate R2
model_RF_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
plt.scatter(test_y, y_ext, c='red', s)
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
352/37:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestClassifier(n_estimators=31, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)


#calculate R2
model_RF_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
plt.scatter(test_y, y_ext, c='red', s)
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
352/38:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestClassifier(n_estimators=31, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)


#calculate R2
model_RF_fit = model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
r2_train = r2_score(train_y, y_pred)
RMSE_train = mean_squared_error(train_y, y_pred)


#CV
y_cv = cross_val_predict(model_RF, train_x, train_y, cv=kf)

#Rsqure_CV
r2_cv = r2_score(train_y, y_cv)
RMSE_cv = mean_squared_error(train_y, y_cv)


#Test
y_ext = model_RF.predict(test_x)
#Rsqure_ext
r2_ext = r2_score(test_y, y_ext)
RMSE_test = mean_squared_error(test_y, y_ext)

print('Results')
print('===================================')
print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
print('===================================')
print(f'R2 model is {r2_train}')
print(f'RMSE model is {RMSE_train}')
print('===================================')
print(f'Q2 CV is {r2_cv}')
print(f'RMSE CV is {RMSE_cv}')
print('===================================')
print(f'Q2 test is {r2_ext}')
print(f'RMSE test is {RMSE_test}')
print('===================================')
plt.scatter(test_y, y_ext, c='red')
plt.scatter(train_y, y_cv, c='blue')
plt.scatter(train_y, y_pred, c='black')
352/39:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestClassifier(n_estimators=31, random_state=42)
352/40:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestClassifier(n_estimators=31, random_state=42)
model_RF
352/41:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestClassifier(n_estimators=31, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
352/42: scores
352/43:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestClassifier(n_estimators=100, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
352/44: scores
352/45:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestClassifier(n_estimators=100, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
avg_score = np.mean(score_array)
print(avg_score)
352/46:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestClassifier(n_estimators=100, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
avg_score = np.mean(scores)
print(avg_score)
352/47:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestClassifier(n_estimators=100, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
avg_score = np.mean(scores)
sd_score = np.sd(scores)
print(avg_score)
352/48:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestClassifier(n_estimators=100, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
avg_score = np.mean(scores)
sd_score = np.std(scores)
print(avg_score)
352/49:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestClassifier(n_estimators=100, random_state=42)
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
avg_score = np.mean(scores)
sd_score = np.std(scores)
print(avg_score)
print(sd_score)
352/50:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestClassifier(n_estimators=100, random_state=42)
model_RF.fit(train_x)

scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
avg_score = np.mean(scores)
sd_score = np.std(scores)
print(avg_score)
print(sd_score)
352/51:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestClassifier(n_estimators=100, random_state=42)
model_RF.fit(train_x, train_y)
352/52:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error


model_RF = RandomForestClassifier(n_estimators=100, random_state=42)
model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
y_pred
352/53:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import accuracy_score

model_RF = RandomForestClassifier(n_estimators=100, random_state=42)
model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
y_pred
accuracy_score(y_true, y_pred, normalize=False)
352/54:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import accuracy_score

model_RF = RandomForestClassifier(n_estimators=100, random_state=42)
model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
y_pred
accuracy_score(train_y, y_pred, normalize=False)
352/55:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import accuracy_score

model_RF = RandomForestClassifier(n_estimators=100, random_state=42)
model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(train_x)
y_pred
accuracy_score(train_y, y_pred, normalize=True)
352/56:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import accuracy_score
import scikitplot as scplot

model_RF = RandomForestClassifier(n_estimators=100, random_state=42)
model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(test_x)
y_pred
accuracy_score(train_y, y_pred, normalize=True)
352/57:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import accuracy_score
import scikitplot as scplot

model_RF = RandomForestClassifier(n_estimators=100, random_state=42)
model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(test_x)
y_pred
accuracy_score(test_y, y_pred, normalize=True)
352/58:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import accuracy_score
import scikitplot as skplot

model_RF = RandomForestClassifier(n_estimators=100, random_state=42)
model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(test_x)
y_pred
accuracy_score(test_y, y_pred, normalize=True)
352/59:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import accuracy_score
import scikitplot as skplot

model_RF = RandomForestClassifier(n_estimators=100, random_state=42)
model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(test_x)
y_pred
accuracy_score(test_y, y_pred, normalize=True)
skplot.metrics.confusion_matrix(test_y, y_pred)
352/60:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import accuracy_score
import scikitplot as skplot

model_RF = RandomForestClassifier(n_estimators=100, random_state=42)
model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(test_x)
y_pred
accuracy_score(test_y, y_pred, normalize=True)
skplot.metrics.confusion_matrix(test_y, y_pred)
plot.show
352/61:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import accuracy_score
import scikitplot as skplot

model_RF = RandomForestClassifier(n_estimators=100, random_state=42)
model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(test_x)
y_pred
accuracy_score(test_y, y_pred, normalize=True)
skplot.metrics.confusion_matrix(test_y, y_pred)
plt.show
352/62:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import accuracy_score
import scikitplot as skplot

model_RF = RandomForestClassifier(n_estimators=100, random_state=42)
model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(test_x)
y_pred
accuracy_score(test_y, y_pred, normalize=True)
skplot.metrics.plot_confusion_matrixconfusion_matrix(test_y, y_pred)
plt.show
352/63:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import accuracy_score
import scikitplot as skplot

model_RF = RandomForestClassifier(n_estimators=100, random_state=42)
model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(test_x)
y_pred
accuracy_score(test_y, y_pred, normalize=True)
skplot.metrics.plot_confusion_matrix(test_y, y_pred)
plt.show
352/64:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import accuracy_score
import scikitplot as skplot

model_RF = RandomForestClassifier(n_estimators=100, random_state=42)
model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(test_x)
y_pred
accuracy_score = accuracy_score(test_y, y_pred, normalize=True)
print(f'accuracy score = {accuracy_score}')
skplot.metrics.plot_confusion_matrix(test_y, y_pred)
plt.show
352/65:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import accuracy_score
import scikitplot as skplot

model_RF = RandomForestClassifier(n_estimators=100, random_state=42)
model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(test_x)
y_pred
accuracy_score = accuracy_score(test_y, y_pred, normalize=True)
print(f'accuracy score = {accuracy_score}')
skplot.metrics.plot_confusion_matrix(test_y, y_pred)
352/66:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import accuracy_score
import scikitplot as skplot

model_RF = RandomForestClassifier(n_estimators=100, random_state=42)
model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(test_x)
y_pred
accuracy_score = accuracy_score(test_y, y_pred, normalize=True)
print(f'accuracy score = {accuracy_score}')
skplot.metrics.plot_confusion_matrix(test_y, y_pred)
plt.show()
352/67:
Bioactivity_class = []
for i in df.Bioactivity_class:
    if i =='active_both' or 'active_glucosidase' or 'active_amylase':
        bioactivity_class.append("active")
    else:
        bioactivity_class.append("inactive")
352/68: Bioactivity_class
352/69:
Bioactivity_class = []
for i in df.Bioactivity_class:
    if i =='active_both' or 'active_glucosidase' or 'active_amylase':
        Bioactivity_class.append("active")
    else:
        Bioactivity_class.append("inactive")
352/70: Bioactivity_class
352/71: len(Bioactivity_class)
352/72:
Bioactivity_class = []
for i in df.Bioactivity_class:
    if i = 'active_both' or 'active_glucosidase' or 'active_amylase':
        Bioactivity_class.append("active")
    else:
        Bioactivity_class.append("inactive")
352/73: len(Bioactivity_class)
352/74:
Bioactivity_class = []
for i in df.Bioactivity_class:
    if i =='active_both' or 'active_glucosidase' or 'active_amylase':
        Bioactivity_class.append("active")
    else:
        Bioactivity_class.append("inactive")
352/75: Bioactivity_class
352/76:
Bioactivity_class = []
for i in df.Bioactivity_class:
    if i =='active_both' or 'active_glucosidase' or 'active_amylase':
        Bioactivity_class.append("active")
    else i =='inactive':
        Bioactivity_class.append("inactive")
352/77:
Bioactivity_class = []
for i in df.Bioactivity_class:
    if i =='active_both' or 'active_glucosidase' or 'active_amylase':
        Bioactivity_class.append("active")
    else i =='inactive':
        Bioactivity_class.append("inactive")
352/78:
Bioactivity_class = []
for i in df.Bioactivity_class:
    if i =='active_both' or 'active_glucosidase' or 'active_amylase':
        Bioactivity_class.append("active")
        else i =='inactive':
        Bioactivity_class.append("inactive")
352/79:
Bioactivity_class = []
for i in df.Bioactivity_class:
    if i =='active_both' or 'active_glucosidase' or 'active_amylase':
        Bioactivity_class.append("active")
    else i =='inactive':
        Bioactivity_class.append("inactive")
352/80:
Bioactivity_class = []
for i in df.Bioactivity_class:
    if i =='active_both' or 'active_glucosidase' or 'active_amylase':
        Bioactivity_class.append("active")
    else i == 'inactive':
        Bioactivity_class.append("inactive")
352/81:
Bioactivity_class = []
for i in df.Bioactivity_class:
    if i =='active_both' or 'active_glucosidase' or 'active_amylase':
        Bioactivity_class.append("active")
    else == 'inactive':
        Bioactivity_class.append("inactive")
352/82:
Bioactivity_class = []
for i in df.Bioactivity_class:
    if i =='active_both' or 'active_glucosidase' or 'active_amylase':
        Bioactivity_class.append("active")
    else i== 'inactive':
        Bioactivity_class.append("inactive")
352/83:
Bioactivity_class = []
for i in df.Bioactivity_class:
    if i in {'active_both' or 'active_glucosidase' or 'active_amylase'}:
        Bioactivity_class.append("active")
    else i== 'inactive':
        Bioactivity_class.append("inactive")
352/84:
Bioactivity_class = []
for i in df.Bioactivity_class:
    if i in {'active_both' or 'active_glucosidase' or 'active_amylase'}:
        Bioactivity_class.append("active")
    else:
        Bioactivity_class.append("inactive")
352/85: len(Bioactivity_class)
352/86: Bioactivity_class
352/87:
Bioactivity_class = []
for i in df.Bioactivity_class:
    if i in {'active_both', 'active_glucosidase', 'active_amylase'}:
        Bioactivity_class.append("active")
    else:
        Bioactivity_class.append("inactive")
352/88: len(Bioactivity_class)
352/89: Bioactivity_class
352/90: label_to_model = Bioactivity_class
352/91:
train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
352/92: indices_train, indices_test
352/93:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import accuracy_score
import scikitplot as skplot

model_RF = RandomForestClassifier(n_estimators=100, random_state=42)
model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(test_x)
y_pred
accuracy_score = accuracy_score(test_y, y_pred, normalize=True)
print(f'accuracy score = {accuracy_score}')
skplot.metrics.plot_confusion_matrix(test_y, y_pred)
plt.show()
352/94:
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
avg_score = np.mean(scores)
sd_score = np.std(scores)
print(avg_score)
print(sd_score)
352/95:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import accuracy_score
import scikitplot as skplot

model_RF = RandomForestClassifier(n_estimators=200, random_state=42)
model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(test_x)
y_pred
accuracy_score = accuracy_score(test_y, y_pred, normalize=True)
print(f'accuracy score = {accuracy_score}')
skplot.metrics.plot_confusion_matrix(test_y, y_pred)
plt.show()
352/96:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import accuracy_score
import scikitplot as skplot

model_RF = RandomForestClassifier(n_estimators=100, random_state=42)
model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(test_x)
y_pred
accuracy_score = accuracy_score(test_y, y_pred, normalize=True)
print(f'accuracy score = {accuracy_score}')
skplot.metrics.plot_confusion_matrix(test_y, y_pred)
plt.show()
352/97:
plant_df = df.copy()
fingerprint_to_model = plant_df[['Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]
label_to_model = plant_df.Bioactivity_class.tolist()
352/98:
plant_df = df.copy()
fingerprint_to_model = plant_df[['Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]
label_to_model = plant_df.Bioactivity_class.tolist()
352/99:
from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors=4)
352/100:
scores=cross_val_score(model, train_x, train_y, cv=kf)
avg_score = np.mean(scores)
sd_score = np.std(scores)
print(avg_score)
print(sd_score)
352/101:
train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
352/102:
from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors=4)
352/103:
scores=cross_val_score(model, train_x, train_y, cv=kf)
avg_score = np.mean(scores)
sd_score = np.std(scores)
print(avg_score)
print(sd_score)
352/104:
from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors=2)
352/105:
scores=cross_val_score(model, train_x, train_y, cv=kf)
avg_score = np.mean(scores)
sd_score = np.std(scores)
print(avg_score)
print(sd_score)
352/106:
from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors=3)
352/107:
scores=cross_val_score(model, train_x, train_y, cv=kf)
avg_score = np.mean(scores)
sd_score = np.std(scores)
print(avg_score)
print(sd_score)
352/108:
from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors=5)
352/109:
scores=cross_val_score(model, train_x, train_y, cv=kf)
avg_score = np.mean(scores)
sd_score = np.std(scores)
print(avg_score)
print(sd_score)
352/110:
from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors=10)
352/111:
scores=cross_val_score(model, train_x, train_y, cv=kf)
avg_score = np.mean(scores)
sd_score = np.std(scores)
print(avg_score)
print(sd_score)
352/112:
from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors=4)
352/113:
scores=cross_val_score(model, train_x, train_y, cv=kf)
avg_score = np.mean(scores)
sd_score = np.std(scores)
print(avg_score)
print(sd_score)
352/114:
scores=cross_val_score(model, train_x, train_y, cv=kf)
avg_score = np.mean(scores)
sd_score = np.std(scores)
print(scores)
print(avg_score)
print(sd_score)
352/115: print(label_to_model.unique())
352/116: print(label_to_model)
352/117:
from sklearn.cluster import KMeans
model = KMeans(n_clusters=4)
y_pred = model.fit_predict(fingerprint_to_model)
352/118: df['cluster'] = y_pred
352/119: df
352/120: df[df.cluster==0]
352/121: df[df.cluster==1]a
352/122: df[df.cluster==1]
352/123: df[df.cluster==2]
352/124: df[df.cluster==3]
352/125: fingerprint_to_model_KMean = plant_df[['Emax_amylase_2.75mcgml','Emax_glucosidase_0.67mcgml','Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]
352/126:
from sklearn.cluster import KMeans
model = KMeans(n_clusters=4)
y_pred = model.fit_predict(fingerprint_to_model_KMean)
352/127: df['cluster'] = y_pred
352/128: df
352/129: pvt = df.pivot_table(index='Name_Short', columns=['cluster'], values='Bioactivity_class', fill_value=0, aggfunc='count')
352/130: pvt
352/131: pvt = df.pivot_table(index='Bioactivity_class', columns=['cluster'], values='Bioactivity_class', fill_value=0, aggfunc='count')
352/132: pvt = df.pivot_table(index='Bioactivity_class', columns=['cluster'], values='', fill_value=0, aggfunc='count')
352/133: pvt = df.pivot_table(index='Bioactivity_class', columns=['cluster'], fill_value=0, aggfunc='count')
352/134: pvt
352/135: pvt = df.pivot_table(index='Bioactivity_class', columns=['cluster'], values='Name_short', fill_value=0, aggfunc='count')
352/136: pvt = df.pivot_table(index='Bioactivity_class', columns=['cluster'], values='Name_Short', fill_value=0, aggfunc='count')
352/137: pvt = df.pivot_table(index='Bioactivity_class', columns=['cluster'], values='cluster', fill_value=0, aggfunc='count')
352/138: pvt = df.pivot_table(index='Name_Short', columns=['cluster'], values='Bioactivity_class', fill_value=0, aggfuc='cont')
352/139: pvt = df.pivot_table(index='Name_Short', columns=['cluster'], values='Bioactivity_class', fill_value=0, aggfunc='cont')
352/140: pvt = df.pivot_table(index='Name_Short', columns=['cluster'], values='Bioactivity_class', fill_value=0, aggfunc='cout')
352/141: pvt = df.pivot_table(index='Name_Short', columns=['cluster'], values='Bioactivity_class', fill_value=0, aggfunc='count')
352/142: pvt
352/143: df['Bioactivity_class_number'] = df['Bioactivity_class'].map({'active_both': 3, 'active_glucosidase': 2, 'active_amylase': 1, 'inactive': 0})
352/144:
df['Bioactivity_class_number'] = df['Bioactivity_class'].map({'active_both': 3, 'active_glucosidase': 2, 'active_amylase': 1, 'inactive': 0})
df
352/145:
df['Bioactivity_class_number'] = df['Bioactivity_class'].map({'active_both': 3, 'active_glucosidase': 2, 'active_amylase': 1, 'inactive': 0})
df.head()
352/146:
from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors=4)
352/147:
plant_df = df.copy()
fingerprint_to_model = plant_df[['Emax_amylase_2.75mcgml','Emax_glucosidase_0.67mcgml', 'Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]
label_to_model = plant_df.Bioactivity_class_number.tolist()
352/148:
train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
352/149:
from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors=4)
scores=cross_val_score(model, train_x, train_y, cv=kf)
avg_score = np.mean(scores)
sd_score = np.std(scores)
print(scores)
print(avg_score)
print(sd_score)
352/150:
from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors=4)
scores=cross_val_score(model, train_x, train_y, cv=kf)
avg_score = np.mean(scores)
sd_score = np.std(scores)
print(scores)
print('mean (%) = {:.3f}'.format(scores.mean(*100))
print(sd_score)
352/151:
from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors=4)
scores=cross_val_score(model, train_x, train_y, cv=kf)
avg_score = np.mean(scores)
sd_score = np.std(scores)
print(scores)
print('mean (%) = {:.3f}'.format(scores.mean(*100))
352/152:
from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors=4)
scores=cross_val_score(model, train_x, train_y, cv=kf)
avg_score = np.mean(scores)
sd_score = np.std(scores)
print(scores)
print('mean (%) = {:.3f}'.format(scores.mean(*100))
352/153:
from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors=4)
scores=cross_val_score(model, train_x, train_y, cv=kf)
avg_score = np.mean(scores)
sd_score = np.std(scores)
print(scores)
print('mean (%) = {:.3f}'.format(scores.mean()*100))
print(sd_score)
352/154:
from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors=4)
scores=cross_val_score(model, train_x, train_y, cv=kf)
avg_score = np.mean(scores)
sd_score = np.std(scores)
print(scores)
print('mean (%) = {:.3f}'.format(avg_score*100))
print(sd_score)
352/155:
from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors=4)
scores=cross_val_score(model, train_x, train_y, cv=kf)
avg_score = np.mean(scores)
sd_score = np.std(scores)
print(scores)
print('mean (%) = {:.3f}'.format(avg_score*100))
print('sd (%) = {:.3f}'.format(sd_score*100))
352/156:
from sklearn.cluster import KMeans
model = KMeans(n_clusters=4)
y_pred = model.fit_predict(fingerprint_to_model_KMean)
352/157: df['cluster'] = y_pred
352/158: df
352/159: df.head()
352/160: print(label_to_model)
352/161: fingerprint_to_model_KMean = plant_df[['Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]
352/162:
from sklearn.cluster import KMeans
model = KMeans(n_clusters=4)
y_pred = model.fit_predict(fingerprint_to_model_KMean)
352/163: df['cluster'] = y_pred
352/164: df.head()
352/165:
plant_df = df.copy()
fingerprint_to_model = plant_df[['Emax_amylase_2.75mcgml','Emax_glucosidase_0.67mcgml', 'Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]
label_to_model = plant_df.Bioactivity_class_number.tolist()
352/166:
train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
352/167:
from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors=4)
scores=cross_val_score(model, train_x, train_y, cv=kf)
avg_score = np.mean(scores)
sd_score = np.std(scores)
print(scores)
print('mean (%) = {:.3f}'.format(avg_score*100))
print('sd (%) = {:.3f}'.format(sd_score*100))
352/168:
plant_df = df.copy()
fingerprint_to_model = plant_df[['Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]
label_to_model = plant_df.Bioactivity_class_number.tolist()
352/169:
train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
352/170:
from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors=4)
scores=cross_val_score(model, train_x, train_y, cv=kf)
avg_score = np.mean(scores)
sd_score = np.std(scores)
print(scores)
print('mean (%) = {:.3f}'.format(avg_score*100))
print('sd (%) = {:.3f}'.format(sd_score*100))
352/171:
y_pred = model.predict(test_x)
y_pred
accuracy_score = accuracy_score(test_y, y_pred, normalize=True)
print(f'accuracy score = {accuracy_score}')
skplot.metrics.plot_confusion_matrix(test_y, y_pred)
plt.show()
352/172:
model.fit(train_x)
y_pred = model.predict(test_x)
y_pred
accuracy_score = accuracy_score(test_y, y_pred, normalize=True)
print(f'accuracy score = {accuracy_score}')
skplot.metrics.plot_confusion_matrix(test_y, y_pred)
plt.show()
352/173:
model.fit(train_x, train_y)
y_pred = model.predict(test_x)
y_pred
accuracy_score = accuracy_score(test_y, y_pred, normalize=True)
print(f'accuracy score = {accuracy_score}')
skplot.metrics.plot_confusion_matrix(test_y, y_pred)
plt.show()
352/174:
model.fit(train_x, train_y)
y_pred = model.predict(test_x)
y_pred
skplot.metrics.plot_confusion_matrix(test_y, y_pred)
plt.show()
352/175:
model.fit(train_x, train_y)
y_pred = model.predict(test_x)
y_pred
accuracy_score = accuracy_score(test_y, y_pred)
print(f'accuracy score = {accuracy_score}')
skplot.metrics.plot_confusion_matrix(test_y, y_pred)
plt.show()
352/176:
model.fit(train_x, train_y)
y_pred = model.predict(test_x)
y_pred
352/177:
model.fit(train_x, train_y)
y_pred = model.predict(test_x)
y_pred
skplot.metrics.plot_confusion_matrix(test_y, y_pred)
plt.show()
352/178: pvt.head()
352/179: accuracy_score(y_true, y_pred)
352/180: accuracy_score(train_y, y_pred)
352/181:
plant_df = df.copy()
fingerprint_to_model = plant_df[['Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]
label_to_model = plant_df.Bioactivity_class.tolist()
352/182:
train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
352/183:
from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors=4)
scores=cross_val_score(model, train_x, train_y, cv=kf)
avg_score = np.mean(scores)
sd_score = np.std(scores)
print(scores)
print('mean (%) = {:.3f}'.format(avg_score*100))
print('sd (%) = {:.3f}'.format(sd_score*100))
352/184:
model.fit(train_x, train_y)
y_pred = model.predict(test_x)
y_pred
skplot.metrics.plot_confusion_matrix(test_y, y_pred)
plt.show()ef
352/185:
model.fit(train_x, train_y)
y_pred = model.predict(test_x)
y_pred
skplot.metrics.plot_confusion_matrix(test_y, y_pred)
plt.show()
352/186:
model.fit(train_x, train_y)
y_pred = model.predict(test_x)
accuracy_score = accuracy_score(test_y, y_pred, normalize=True)
print(f'accuracy score = {accuracy_score}')
skplot.metrics.plot_confusion_matrix(test_y, y_pred)
plt.show()
352/187:
model.fit(train_x, train_y)
y_pred = model.predict(test_x)
accuracy_score = accuracy_score(test_y, y_pred, normalize=True)
print(f'accuracy score = {accuracy_score}')
skplot.metrics.plot_confusion_matrix(test_y, y_pred)
plt.show()
352/188:
from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors=4)
scores=cross_val_score(model, train_x, train_y, cv=kf)
avg_score = np.mean(scores)
sd_score = np.std(scores)
print(scores)
print('mean (%) = {:.3f}'.format(avg_score*100))
print('sd (%) = {:.3f}'.format(sd_score*100))
352/189:
model.fit(train_x, train_y)
y_pred = model.predict(test_x)
accuracy_score = accuracy_score(test_y, y_pred, normalize=True)
print(f'accuracy score = {accuracy_score}')
skplot.metrics.plot_confusion_matrix(test_y, y_pred)
plt.show()
352/190:
model.fit(train_x, train_y)
y_pred = model.predict(test_x)
accuracy_score = accuracy_score(test_y, y_pred, normalize=True)
print(f'accuracy score = {accuracy_score}')
skplot.metrics.plot_confusion_matrix(test_y, y_pred)
plt.show()
352/191:
model.fit(train_x, train_y)
y_pred = model.predict(test_x)
y_pred
352/192:
model.fit(train_x, train_y)
y_pred = model.predict(test_x)
y_pred
text_y
352/193:
model.fit(train_x, train_y)
y_pred = model.predict(test_x)
y_pred
test_y
352/194:
model.fit(train_x, train_y)
y_pred = model.predict(test_x)

test_y
352/195:
model.fit(train_x, train_y)
y_pred = model.predict(test_x)
accuracy_score = accuracy_score(test_y, y_pred, normalize=True)
352/196:
model.fit(train_x, train_y)
y_pred = model.predict(test_x)
accuracy_score = accuracy_score(test_y, y_pred, normalize=False)
352/197:
model.fit(train_x, train_y)
y_pred = model.predict(test_x)
accuracy_score = accuracy_score(np.array(test_y), y_pred, normalize=False)
352/198:
model.fit(train_x, train_y)
y_pred = model.predict(test_x)
accuracy_score = accuracy_score(np.array(test_y), np.array(y_pred), normalize=False)
352/199:
model.fit(train_x, train_y)
y_pred = model.predict(test_x)
accuracy_score(test_y, y_pred)
352/200:
plant_df = df.copy()
fingerprint_to_model = plant_df[['Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]
label_to_model = plant_df.Bioactivity_class.tolist()
352/201:
train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
352/202:
from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors=4)
scores=cross_val_score(model, train_x, train_y, cv=kf)
avg_score = np.mean(scores)
sd_score = np.std(scores)
print(scores)
print('mean (%) = {:.3f}'.format(avg_score*100))
print('sd (%) = {:.3f}'.format(sd_score*100))
352/203:
model.fit(train_x, train_y)
y_pred = model.predict(test_x)
accuracy_score(test_y, y_pred)
352/204:
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import accuracy_score
import scikitplot as skplot

model_RF = RandomForestClassifier(n_estimators=100, random_state=42)
model_RF.fit(train_x, train_y)
y_pred = model_RF.predict(test_x)
y_pred
accuracy_score = accuracy_score(test_y, y_pred, normalize=True)
print(f'accuracy score = {accuracy_score}')
skplot.metrics.plot_confusion_matrix(test_y, y_pred)
plt.show()
352/205:
scores=cross_val_score(model_RF, train_x, train_y, cv=kf)
avg_score = np.mean(scores)
sd_score = np.std(scores)
print(avg_score)
print(sd_score)
352/206:
plant_df = df.copy()
fingerprint_to_model = plant_df[['Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]
label_to_model = plant_df.Bioactivity_class.tolist()
352/207:
df['Bioactivity_class_number'] = df['Bioactivity_class'].map({'active_both': 3, 'active_glucosidase': 2, 'active_amylase': 1, 'inactive': 0})
df.head()
352/208:
plant_df = df.copy()
fingerprint_to_model = plant_df[['Alkaloids', 'Antaquinones', 'Carotenoids', 'flavonoids', 'Reducing_sugars', 'Saponins', 'Tannins', 'Xanthones']]
label_to_model = plant_df.Bioactivity_class.tolist()
352/209:
train_x, test_x, train_y, test_y, indices_train, indices_test = train_test_split(fingerprint_to_model, label_to_model, indices, test_size=0.2, random_state=42)
splits = [train_x, test_x, train_y, test_y]
# NBVAL_CHECK_OUTPUT
print("Training data size:", len(train_x))
print("Test data size:", len(test_x))
352/210:
from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors=4)
scores=cross_val_score(model, train_x, train_y, cv=kf)
avg_score = np.mean(scores)
sd_score = np.std(scores)
print(scores)
print('mean (%) = {:.3f}'.format(avg_score*100))
print('sd (%) = {:.3f}'.format(sd_score*100))
352/211:
model.fit(train_x, train_y)
y_pred = model.predict(test_x)
accuracy_score(test_y, y_pred)
352/212:
model.fit(train_x, train_y)
y_pred = model.predict(test_x)
model.score(test_x, test_y)
352/213:
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
print(classification_report(test_y, y_pred))
print(confusion_matrix(test_y, y_pred))
352/214:
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
print(classification_report(test_y, y_pred))
skplot.metrics.plot_confusion_matrix(test_y, y_pred)
plt.show()
352/215:
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
print(classification_report(test_y, y_pred))
skplot.metrics.plot_confusion_matrix(test_y, y_pred)
skplt.metrics.plot_roc(test_y, y_preds)
plt.show()
352/216:
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
print(classification_report(test_y, y_pred))
skplot.metrics.plot_confusion_matrix(test_y, y_pred)
skplot.metrics.plot_roc(test_y, y_preds)
plt.show()
352/217:
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
print(classification_report(test_y, y_pred))
skplot.metrics.plot_confusion_matrix(test_y, y_pred)
skplot.metrics.plot_roc(test_y, y_pred)
plt.show()
352/218:
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
print(classification_report(test_y, y_pred))
skplot.metrics.plot_confusion_matrix(test_y, y_pred)
y_prob = model.predict_proba(test_x)
skplot.metrics.plot_roc(test_y, y_prob)
plt.show()
352/219:
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
print(classification_report(test_y, y_pred))
skplot.metrics.plot_confusion_matrix(test_y, y_pred)
y_prob = model.predict_proba(test_x)
skplot.metrics.plot_precision_recall(test_y, y_prob)
plt.show()
352/220:
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
print(classification_report(test_y, y_pred))
skplot.metrics.plot_confusion_matrix(test_y, y_pred)
y_prob = model.predict_proba(test_x)
skplot.metrics.plot_roc(test_y, y_prob)
plt.show()
352/221:
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
print(classification_report(test_y, y_pred))
skplot.metrics.plot_confusion_matrix(test_y, y_pred)
y_prob = model.predict_proba(test_x)
plt.show()
352/222:
test_prob = model.predict_proba(test_x)[:, 1]
# Prediction class on test set
test_pred = ml_model.predict(test_x)
# Compute False postive rate and True positive rate
fpr, tpr, thresholds = metrics.roc_curve(test_y, test_prob)
# Calculate Area under the curve to display on the plot
auc = roc_auc_score(test_y, test_prob)
# Plot the computed values
ax.plot(fpr, tpr, label=(f"{model['label']} AUC area = {auc:.2f}"))
ax.plot([0, 1], [0, 1], "r--")
ax.set_xlabel("False Positive Rate")
ax.set_ylabel("True Positive Rate")
ax.set_title("Receiver Operating Characteristic")
ax.legend(loc="lower right")
352/223:
test_prob = model.predict_proba(test_x)[:, 1]
# Prediction class on test set
test_pred = model.predict(test_x)
# Compute False postive rate and True positive rate
fpr, tpr, thresholds = metrics.roc_curve(test_y, test_prob)
# Calculate Area under the curve to display on the plot
auc = roc_auc_score(test_y, test_prob)
# Plot the computed values
ax.plot(fpr, tpr, label=(f"{model['label']} AUC area = {auc:.2f}"))
ax.plot([0, 1], [0, 1], "r--")
ax.set_xlabel("False Positive Rate")
ax.set_ylabel("True Positive Rate")
ax.set_title("Receiver Operating Characteristic")
ax.legend(loc="lower right")
352/224:
from sklearn.metrics import auc, accuracy_score, recall_score
from sklearn.metrics import roc_curve, roc_auc_score

test_prob = model.predict_proba(test_x)[:, 1]
# Prediction class on test set
test_pred = model.predict(test_x)
# Compute False postive rate and True positive rate
fpr, tpr, thresholds = metrics.roc_curve(test_y, test_prob)
# Calculate Area under the curve to display on the plot
auc = roc_auc_score(test_y, test_prob)
# Plot the computed values
ax.plot(fpr, tpr, label=(f"{model['label']} AUC area = {auc:.2f}"))
ax.plot([0, 1], [0, 1], "r--")
ax.set_xlabel("False Positive Rate")
ax.set_ylabel("True Positive Rate")
ax.set_title("Receiver Operating Characteristic")
ax.legend(loc="lower right")
352/225:
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
352/226:
models = []
model.append(('kNN', KNeighborsClassier()))
model.append(('NB', Gaussian()))
model.append(('SVC', SVC()))
model.append(('RF', RandomForestClassifier()))
model.append(('DT', DecisionTreeClassifier()))

results = []
names = []
for names, model in models
    cvs = cross_val_score(model, train_x, train_y, cv=kf)
    results.append(cvs)
    names.append(name)
    print('{:4} {:3f} {:3f}').format(name, cvs.mean(), cvs.std())
352/227:
models = []
model.append(('kNN', KNeighborsClassier()))
model.append(('NB', Gaussian()))
model.append(('SVC', SVC()))
model.append(('RF', RandomForestClassifier()))
model.append(('DT', DecisionTreeClassifier()))

results = []
names = []
for names, model in models:
    cvs = cross_val_score(model, train_x, train_y, cv=kf)
    results.append(cvs)
    names.append(name)
    print('{:4} {:3f} {:3f}').format(name, cvs.mean(), cvs.std())
352/228:
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
352/229:
models = []
model.append(('kNN', KNeighborsClassier()))
model.append(('NB', Gaussian()))
model.append(('SVC', SVC()))
model.append(('RF', RandomForestClassifier()))
model.append(('DT', DecisionTreeClassifier()))

results = []
names = []
for names, model in models:
    cvs = cross_val_score(model, train_x, train_y, cv=kf)
    results.append(cvs)
    names.append(name)
    print('{:4} {:3f} {:3f}').format(name, cvs.mean(), cvs.std())
352/230:
models = []
model.append(('kNN', KNeighborsClassifier()))
model.append(('NB', Gaussian()))
model.append(('SVC', SVC()))
model.append(('RF', RandomForestClassifier()))
model.append(('DT', DecisionTreeClassifier()))

results = []
names = []
for names, model in models:
    cvs = cross_val_score(model, train_x, train_y, cv=kf)
    results.append(cvs)
    names.append(name)
    print('{:4} {:3f} {:3f}').format(name, cvs.mean(), cvs.std())
352/231:
models = []
models.append(('kNN', KNeighborsClassifier()))
models.append(('NB', Gaussian()))
models.append(('SVC', SVC()))
models.append(('RF', RandomForestClassifier()))
models.append(('DT', DecisionTreeClassifier()))

results = []
names = []
for names, model in models:
    cvs = cross_val_score(model, train_x, train_y, cv=kf)
    results.append(cvs)
    names.append(name)
    print('{:4} {:3f} {:3f}').format(name, cvs.mean(), cvs.std())
352/232:
models = []
models.append(('kNN', KNeighborsClassifier()))
models.append(('NB', GaussianNB()))
models.append(('SVC', SVC()))
models.append(('RF', RandomForestClassifier()))
models.append(('DT', DecisionTreeClassifier()))

results = []
names = []
for names, model in models:
    cvs = cross_val_score(model, train_x, train_y, cv=kf)
    results.append(cvs)
    names.append(name)
    print('{:4} {:3f} {:3f}').format(name, cvs.mean(), cvs.std())
352/233:
models = []
models.append(('kNN', KNeighborsClassifier()))
models.append(('NB', GaussianNB()))
models.append(('SVC', SVC()))
models.append(('RF', RandomForestClassifier()))
models.append(('DT', DecisionTreeClassifier()))

results = []
names = []
for names, model in models:
    cvs = cross_val_score(model, train_x, train_y, cv=kf)
    results.append(cvs)
    names.append(name)
    print('{:4} {:3f} {:3f}').format(name, cvs.mean(), cvs.std())
352/234:
models = []
models.append(('kNN', KNeighborsClassifier()))
models.append(('NB', GaussianNB()))
models.append(('SVC', SVC()))
models.append(('RF', RandomForestClassifier()))
models.append(('DT', DecisionTreeClassifier()))

results = []
names = []
for name, model in models:
    cvs = cross_val_score(model, train_x, train_y, cv=kf)
    results.append(cvs)
    names.append(name)
    print('{:4} {:3f} {:3f}').format(name, cvs.mean(), cvs.std())
352/235:
models = []
models.append(('kNN', KNeighborsClassifier()))
models.append(('NB', GaussianNB()))
models.append(('SVC', SVC()))
models.append(('RF', RandomForestClassifier()))
models.append(('DT', DecisionTreeClassifier()))

results = []
names = []
for name, model in models:
    cvs = cross_val_score(model, train_x, train_y, cv=kf)
    results.append(cvs)
    names.append(name)
    print('{:4} {:.3f} {:.3f}').format(name, cvs.mean(), cvs.std())
352/236:
models = []
models.append(('kNN', KNeighborsClassifier()))
models.append(('NB', GaussianNB()))
models.append(('SVC', SVC()))
models.append(('RF', RandomForestClassifier()))
models.append(('DT', DecisionTreeClassifier()))

results = []
names = []
for name, model in models:
    cvs = cross_val_score(model, train_x, train_y, cv=kf)
    results.append(cvs)
    names.append(name)
    print('{:4} {:.3f} ({:.3f})'.format(name, cvs.mean(), cvs.std()))
352/237:
fig = ply.figure()
fig.suptitle('Algorithm Comparison')
ax = fig.add_subplot(111)
plt.boxplot(results)
352/238:
fig = plt.figure()
fig.suptitle('Algorithm Comparison')
ax = fig.add_subplot(111)
plt.boxplot(results)
ax.set_xticklabels(names)
plt.show()
352/239:
models = []
models.append(('kNN', KNeighborsClassifier()))
models.append(('NB', GaussianNB()))
models.append(('SVC', SVC()))
models.append(('RF', RandomForestClassifier()))
models.append(('DT', DecisionTreeClassifier()))

results = []
names = []
for name, model in models:
    cvs = cross_val_score(model, train_x, train_y, cv=kf)
    results.append(cvs)
    names.append(name)
    print('{:4} {:.3f} ({:.3f})'.format(name, cvs.median(), cvs.std()))
352/240:
models = []
models.append(('kNN', KNeighborsClassifier()))
models.append(('NB', GaussianNB()))
models.append(('SVC', SVC()))
models.append(('RF', RandomForestClassifier()))
models.append(('DT', DecisionTreeClassifier()))

results = []
names = []
for name, model in models:
    cvs = cross_val_score(model, train_x, train_y, cv=kf)
    results.append(cvs)
    names.append(name)
    print('{:4} {:.3f} ({:.3f})'.format(name, cvs.mean(), cvs.std()))
352/241:
fig = plt.figure()
fig.suptitle('Algorithm Comparison')
ax = fig.add_subplot(111)
plt.boxplot(results)
ax.set_xticklabels(names)
ax.set_xticklabels('Accuracy score')
plt.show()
352/242:
fig = plt.figure()
fig.suptitle('Algorithm Comparison')
ax = fig.add_subplot(111)
plt.boxplot(results)
ax.set_xticklabels(names)
ax.set_ylabels('Accuracy score')
plt.show()
352/243:
fig = plt.figure()
fig.suptitle('Algorithm Comparison')
ax = fig.add_subplot(111)
plt.boxplot(results)
ax.set_xticklabels(names)
ax.set_ylabel('Accuracy score')
plt.show()
352/244:
fig = plt.figure()
fig.suptitle('Algorithm Comparison by 10-fold Cross validation')
ax = fig.add_subplot(111)
plt.boxplot(results)
ax.set_xticklabels(names)
ax.set_ylabel('Accuracy score')
plt.show()
352/245:
models = []
models.append(('kNN', KNeighborsClassifier(n_neighbors=4)))
models.append(('NB', GaussianNB()))
models.append(('SVC', SVC()))
models.append(('RF', RandomForestClassifier()))
models.append(('DT', DecisionTreeClassifier()))

results = []
names = []
for name, model in models:
    cvs = cross_val_score(model, train_x, train_y, cv=kf)
    results.append(cvs)
    names.append(name)
    print('{:4} {:.3f} ({:.3f})'.format(name, cvs.mean(), cvs.std()))
352/246:
models = []
models.append(('kNN', KNeighborsClassifier()))
models.append(('NB', GaussianNB()))
models.append(('SVC', SVC()))
models.append(('RF', RandomForestClassifier()))
models.append(('DT', DecisionTreeClassifier()))

results = []
names = []
for name, model in models:
    cvs = cross_val_score(model, train_x, train_y, cv=kf)
    results.append(cvs)
    names.append(name)
    print('{:4} {:.3f} ({:.3f})'.format(name, cvs.mean(), cvs.std()))
352/247:
models = []
models.append(('kNN', KNeighborsClassifier()))
models.append(('NB', GaussianNB()))
models.append(('SVC', SVC()))
models.append(('RF', RandomForestClassifier(random_state=42)))
models.append(('DT', DecisionTreeClassifier()))

results = []
names = []
for name, model in models:
    cvs = cross_val_score(model, train_x, train_y, cv=kf)
    results.append(cvs)
    names.append(name)
    print('{:4} {:.3f} ({:.3f})'.format(name, cvs.mean(), cvs.std()))
352/248:
fig = plt.figure()
fig.suptitle('Algorithm Comparison by 10-fold Cross validation')
ax = fig.add_subplot(111)
plt.boxplot(results)
ax.set_xticklabels(names)
ax.set_ylabel('Accuracy score')
plt.show()
352/249:
models = []
models.append(('kNN', KNeighborsClassifier()))
models.append(('NB', GaussianNB()))
models.append(('SVC', SVC()))
models.append(('RF', RandomForestClassifier(n_estimators=200, random_state=42)))
models.append(('DT', DecisionTreeClassifier()))

results = []
names = []
for name, model in models:
    cvs = cross_val_score(model, train_x, train_y, cv=kf)
    results.append(cvs)
    names.append(name)
    print('{:4} {:.3f} ({:.3f})'.format(name, cvs.mean(), cvs.std()))
352/250:
fig = plt.figure()
fig.suptitle('Algorithm Comparison by 10-fold Cross validation')
ax = fig.add_subplot(111)
plt.boxplot(results)
ax.set_xticklabels(names)
ax.set_ylabel('Accuracy score')
plt.show()
352/251:
models = []
models.append(('kNN', KNeighborsClassifier()))
models.append(('NB', GaussianNB()))
models.append(('SVC', SVC()))
models.append(('RF', RandomForestClassifier(random_state=42)))
models.append(('DT', DecisionTreeClassifier()))

results = []
names = []
for name, model in models:
    cvs = cross_val_score(model, train_x, train_y, cv=kf)
    results.append(cvs)
    names.append(name)
    print('{:4} {:.3f} ({:.3f})'.format(name, cvs.mean(), cvs.std()))
352/252:
fig = plt.figure()
fig.suptitle('Algorithm Comparison by 10-fold Cross validation')
ax = fig.add_subplot(111)
plt.boxplot(results)
ax.set_xticklabels(names)
ax.set_ylabel('Accuracy score')
plt.show()
352/253: model = SVC()
352/254: model_SV = SVC()
352/255: svc = SVC()
352/256: SVR = SVR(kernel='rbf')
352/257: SVC = SVC(kernel='rbf')
352/258:
SVC = SVC(kernel='rbf')
SVR_fit = SVR.fit(train_x_selected, train_y)
352/259:
SVC = SVC(kernel='rbf')
SVC.fit(train_x, train_y)
352/260:
SVC = SVC(kernel='rbf')
SVC.fit(train_x, train_y)
352/261:
svc = SVC(kernel='rbf')
Svc.fit(train_x, train_y)
352/262:
svc = SVC(kernel='rbf')
svc.fit(train_x, train_y)
352/263:
svc = SVC()
svc.fit(train_x, train_y)
352/264:
model_svc = SVC()
svc.fit(train_x, train_y)
352/265:
model_svc = SVC()
svc.fit(train_x, train_y)
   1: %history -g
   2: _ih[-15:]
   3: %history -g -f notebook_file.ipynb
